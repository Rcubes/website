<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>datacubeR on datacubeR</title>
    <link>/</link>
    <description>Recent content in datacubeR on datacubeR</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>This is just mine. Use it giving proper credit.</copyright>
    <lastBuildDate>Sat, 01 Jun 2030 13:00:00 -0400</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Example Talk</title>
      <link>/talk/example/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 -0400</pubDate>
      
      <guid>/talk/example/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.&lt;/p&gt;

  &lt;/div&gt;
&lt;/div&gt;


&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Academic&amp;rsquo;s &lt;em&gt;Slides&lt;/em&gt; feature and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Further talk details can easily be added to this page using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Latin R</title>
      <link>/post/latin-r/</link>
      <pubDate>Tue, 24 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/latin-r/</guid>
      <description>


&lt;div id=&#34;latin-r&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Latin R&lt;/h2&gt;
&lt;p&gt;Latin R is a conference organize by RLadies Latam and is the oportunity we have here to know the last breakthrough and how is being using in Research, companies, etc.
They also provide a full day of Tutorials and of course important keynotes are invited to come over. This year I think is huge 3 Main R users are coming:&lt;/p&gt;
&lt;div id=&#34;mine-cetinkaya-rundel&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Mine Çetinkaya-Rundel&lt;/h3&gt;
&lt;p&gt;She is one of the most important persons involved into R Teaching. Statistics Professor from Duke University. I personally had the chance to take some Coursera and Datacamp Courses with her and I learned a lot, really good teacher and really knowledgable. Having her now here in Chile to meet her in person is such a great Honor. I just can´t wait to hear about what she has to say.
She will be conducting a Tutorial on how to Teach R and of Course a main Speech.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;erin-ledell&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Erin Ledell&lt;/h3&gt;
&lt;p&gt;I have to say I don’t know a lot about her but she is the Chief Machine Learning Scientist at H2O and that is more than enough. She will be conducting a Tutorial on Machine Learning and Deep Learning that of course I´ll be taking so I´ll give more details on a later post.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hadley-wickham&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Hadley Wickham&lt;/h3&gt;
&lt;p&gt;This Guy needs no Introduction. He is the Chief Scientist at RStudio and he will be giving the Main Speech of the Conference plus a Package Development Tutorial. Of ourse I will be in first row of this Tutorial and I expect to learn a lot and have a lot of questions prepared beforehand.&lt;/p&gt;
&lt;p&gt;This guys is by far my most important inspiration when it comes to program in R. Definitely this guys has made my life easier because of all of the packages that he has created and I use almost every day.&lt;/p&gt;
&lt;p&gt;I’ll be the next 3 days in the Conference and I expect to share some pictures and experiences about the things I will learn.&lt;/p&gt;
&lt;p&gt;Stay tuned!!!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>My Final Project at the ML Diploma (Part I)</title>
      <link>/post/machine-learning-diploma/</link>
      <pubDate>Tue, 24 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/machine-learning-diploma/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/pagedtable/css/pagedtable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/pagedtable/js/pagedtable.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#importing-data&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Importing Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#redefining-categorical-variables&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2&lt;/span&gt; Redefining Categorical Variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#discovering-missing-values&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3&lt;/span&gt; Discovering Missing Values&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4&lt;/span&gt; Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;During my Machine Learning Diploma I had the chance to work on a very interesting project that was actually created in SAS. Of course I absolutely refused to use that old fashioned tool and I move everything to R.&lt;/p&gt;
&lt;p&gt;I will try to demonstrate as much of the packages I used to perform this analysis.&lt;/p&gt;
&lt;div id=&#34;importing-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Importing Data&lt;/h1&gt;
&lt;p&gt;Since the data is coming from a SAS format, it was absolutely necessary to use this incredibly tidyverse package called &lt;code&gt;haven&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The code to import the data is super simple and goes like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Loading silent tidyverse to make normal utility functions available
suppressPackageStartupMessages(library(tidyverse))
library(haven)
data &amp;lt;- read_sas(&amp;quot;develop.sas7bdat&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will import the data into an R object:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Showing 10 first Obs
data %&amp;gt;%
  head&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
&lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;AcctAge&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;DDA&#34;],&#34;name&#34;:[2],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;DDABal&#34;],&#34;name&#34;:[3],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;CashBk&#34;],&#34;name&#34;:[4],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;Checks&#34;],&#34;name&#34;:[5],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;DirDep&#34;],&#34;name&#34;:[6],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;NSF&#34;],&#34;name&#34;:[7],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;NSFAmt&#34;],&#34;name&#34;:[8],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;Phone&#34;],&#34;name&#34;:[9],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;Teller&#34;],&#34;name&#34;:[10],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;Sav&#34;],&#34;name&#34;:[11],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;SavBal&#34;],&#34;name&#34;:[12],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;ATM&#34;],&#34;name&#34;:[13],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;ATMAmt&#34;],&#34;name&#34;:[14],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;POS&#34;],&#34;name&#34;:[15],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;POSAmt&#34;],&#34;name&#34;:[16],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;CD&#34;],&#34;name&#34;:[17],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;CDBal&#34;],&#34;name&#34;:[18],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;IRA&#34;],&#34;name&#34;:[19],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;IRABal&#34;],&#34;name&#34;:[20],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;LOC&#34;],&#34;name&#34;:[21],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;LOCBal&#34;],&#34;name&#34;:[22],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;ILS&#34;],&#34;name&#34;:[23],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;ILSBal&#34;],&#34;name&#34;:[24],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;MM&#34;],&#34;name&#34;:[25],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;MMBal&#34;],&#34;name&#34;:[26],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;MMCred&#34;],&#34;name&#34;:[27],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;MTG&#34;],&#34;name&#34;:[28],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;MTGBal&#34;],&#34;name&#34;:[29],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;CC&#34;],&#34;name&#34;:[30],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;CCBal&#34;],&#34;name&#34;:[31],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;CCPurc&#34;],&#34;name&#34;:[32],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;SDB&#34;],&#34;name&#34;:[33],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;Income&#34;],&#34;name&#34;:[34],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;HMOwn&#34;],&#34;name&#34;:[35],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;LORes&#34;],&#34;name&#34;:[36],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;HMVal&#34;],&#34;name&#34;:[37],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;Age&#34;],&#34;name&#34;:[38],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;CRScore&#34;],&#34;name&#34;:[39],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;Moved&#34;],&#34;name&#34;:[40],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;InArea&#34;],&#34;name&#34;:[41],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;Ins&#34;],&#34;name&#34;:[42],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;Branch&#34;],&#34;name&#34;:[43],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;Res&#34;],&#34;name&#34;:[44],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;Dep&#34;],&#34;name&#34;:[45],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;DepAmt&#34;],&#34;name&#34;:[46],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;Inv&#34;],&#34;name&#34;:[47],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;InvBal&#34;],&#34;name&#34;:[48],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]}],&#34;data&#34;:[{&#34;1&#34;:&#34;0.3&#34;,&#34;2&#34;:&#34;1&#34;,&#34;3&#34;:&#34;419.27&#34;,&#34;4&#34;:&#34;0&#34;,&#34;5&#34;:&#34;0&#34;,&#34;6&#34;:&#34;0&#34;,&#34;7&#34;:&#34;0&#34;,&#34;8&#34;:&#34;0&#34;,&#34;9&#34;:&#34;0&#34;,&#34;10&#34;:&#34;0&#34;,&#34;11&#34;:&#34;1&#34;,&#34;12&#34;:&#34;10233.72&#34;,&#34;13&#34;:&#34;1&#34;,&#34;14&#34;:&#34;106.74&#34;,&#34;15&#34;:&#34;0&#34;,&#34;16&#34;:&#34;0&#34;,&#34;17&#34;:&#34;0&#34;,&#34;18&#34;:&#34;0&#34;,&#34;19&#34;:&#34;0&#34;,&#34;20&#34;:&#34;0&#34;,&#34;21&#34;:&#34;0&#34;,&#34;22&#34;:&#34;0&#34;,&#34;23&#34;:&#34;0&#34;,&#34;24&#34;:&#34;0&#34;,&#34;25&#34;:&#34;0&#34;,&#34;26&#34;:&#34;0&#34;,&#34;27&#34;:&#34;0&#34;,&#34;28&#34;:&#34;0&#34;,&#34;29&#34;:&#34;0&#34;,&#34;30&#34;:&#34;1&#34;,&#34;31&#34;:&#34;483.65&#34;,&#34;32&#34;:&#34;0&#34;,&#34;33&#34;:&#34;0&#34;,&#34;34&#34;:&#34;16&#34;,&#34;35&#34;:&#34;1&#34;,&#34;36&#34;:&#34;11.0&#34;,&#34;37&#34;:&#34;89&#34;,&#34;38&#34;:&#34;63&#34;,&#34;39&#34;:&#34;696&#34;,&#34;40&#34;:&#34;0&#34;,&#34;41&#34;:&#34;1&#34;,&#34;42&#34;:&#34;1&#34;,&#34;43&#34;:&#34;B17&#34;,&#34;44&#34;:&#34;R&#34;,&#34;45&#34;:&#34;2&#34;,&#34;46&#34;:&#34;1170.06&#34;,&#34;47&#34;:&#34;0&#34;,&#34;48&#34;:&#34;0&#34;},{&#34;1&#34;:&#34;0.7&#34;,&#34;2&#34;:&#34;1&#34;,&#34;3&#34;:&#34;1986.81&#34;,&#34;4&#34;:&#34;0&#34;,&#34;5&#34;:&#34;1&#34;,&#34;6&#34;:&#34;1&#34;,&#34;7&#34;:&#34;0&#34;,&#34;8&#34;:&#34;0&#34;,&#34;9&#34;:&#34;0&#34;,&#34;10&#34;:&#34;0&#34;,&#34;11&#34;:&#34;0&#34;,&#34;12&#34;:&#34;0.00&#34;,&#34;13&#34;:&#34;1&#34;,&#34;14&#34;:&#34;268.88&#34;,&#34;15&#34;:&#34;0&#34;,&#34;16&#34;:&#34;0&#34;,&#34;17&#34;:&#34;0&#34;,&#34;18&#34;:&#34;0&#34;,&#34;19&#34;:&#34;0&#34;,&#34;20&#34;:&#34;0&#34;,&#34;21&#34;:&#34;0&#34;,&#34;22&#34;:&#34;0&#34;,&#34;23&#34;:&#34;0&#34;,&#34;24&#34;:&#34;0&#34;,&#34;25&#34;:&#34;0&#34;,&#34;26&#34;:&#34;0&#34;,&#34;27&#34;:&#34;0&#34;,&#34;28&#34;:&#34;0&#34;,&#34;29&#34;:&#34;0&#34;,&#34;30&#34;:&#34;1&#34;,&#34;31&#34;:&#34;0.00&#34;,&#34;32&#34;:&#34;1&#34;,&#34;33&#34;:&#34;0&#34;,&#34;34&#34;:&#34;4&#34;,&#34;35&#34;:&#34;1&#34;,&#34;36&#34;:&#34;7.0&#34;,&#34;37&#34;:&#34;87&#34;,&#34;38&#34;:&#34;51&#34;,&#34;39&#34;:&#34;674&#34;,&#34;40&#34;:&#34;0&#34;,&#34;41&#34;:&#34;1&#34;,&#34;42&#34;:&#34;0&#34;,&#34;43&#34;:&#34;B2&#34;,&#34;44&#34;:&#34;R&#34;,&#34;45&#34;:&#34;1&#34;,&#34;46&#34;:&#34;446.93&#34;,&#34;47&#34;:&#34;0&#34;,&#34;48&#34;:&#34;0&#34;},{&#34;1&#34;:&#34;4.1&#34;,&#34;2&#34;:&#34;0&#34;,&#34;3&#34;:&#34;0.00&#34;,&#34;4&#34;:&#34;0&#34;,&#34;5&#34;:&#34;0&#34;,&#34;6&#34;:&#34;0&#34;,&#34;7&#34;:&#34;0&#34;,&#34;8&#34;:&#34;0&#34;,&#34;9&#34;:&#34;0&#34;,&#34;10&#34;:&#34;0&#34;,&#34;11&#34;:&#34;0&#34;,&#34;12&#34;:&#34;0.00&#34;,&#34;13&#34;:&#34;0&#34;,&#34;14&#34;:&#34;0.00&#34;,&#34;15&#34;:&#34;0&#34;,&#34;16&#34;:&#34;0&#34;,&#34;17&#34;:&#34;0&#34;,&#34;18&#34;:&#34;0&#34;,&#34;19&#34;:&#34;0&#34;,&#34;20&#34;:&#34;0&#34;,&#34;21&#34;:&#34;0&#34;,&#34;22&#34;:&#34;0&#34;,&#34;23&#34;:&#34;0&#34;,&#34;24&#34;:&#34;0&#34;,&#34;25&#34;:&#34;0&#34;,&#34;26&#34;:&#34;0&#34;,&#34;27&#34;:&#34;0&#34;,&#34;28&#34;:&#34;0&#34;,&#34;29&#34;:&#34;0&#34;,&#34;30&#34;:&#34;1&#34;,&#34;31&#34;:&#34;0.00&#34;,&#34;32&#34;:&#34;0&#34;,&#34;33&#34;:&#34;0&#34;,&#34;34&#34;:&#34;30&#34;,&#34;35&#34;:&#34;1&#34;,&#34;36&#34;:&#34;8.5&#34;,&#34;37&#34;:&#34;97&#34;,&#34;38&#34;:&#34;60&#34;,&#34;39&#34;:&#34;640&#34;,&#34;40&#34;:&#34;0&#34;,&#34;41&#34;:&#34;1&#34;,&#34;42&#34;:&#34;1&#34;,&#34;43&#34;:&#34;B3&#34;,&#34;44&#34;:&#34;S&#34;,&#34;45&#34;:&#34;0&#34;,&#34;46&#34;:&#34;0.00&#34;,&#34;47&#34;:&#34;0&#34;,&#34;48&#34;:&#34;0&#34;},{&#34;1&#34;:&#34;0.5&#34;,&#34;2&#34;:&#34;1&#34;,&#34;3&#34;:&#34;1594.84&#34;,&#34;4&#34;:&#34;0&#34;,&#34;5&#34;:&#34;1&#34;,&#34;6&#34;:&#34;0&#34;,&#34;7&#34;:&#34;0&#34;,&#34;8&#34;:&#34;0&#34;,&#34;9&#34;:&#34;0&#34;,&#34;10&#34;:&#34;1&#34;,&#34;11&#34;:&#34;1&#34;,&#34;12&#34;:&#34;425.06&#34;,&#34;13&#34;:&#34;1&#34;,&#34;14&#34;:&#34;278.07&#34;,&#34;15&#34;:&#34;0&#34;,&#34;16&#34;:&#34;0&#34;,&#34;17&#34;:&#34;0&#34;,&#34;18&#34;:&#34;0&#34;,&#34;19&#34;:&#34;0&#34;,&#34;20&#34;:&#34;0&#34;,&#34;21&#34;:&#34;0&#34;,&#34;22&#34;:&#34;0&#34;,&#34;23&#34;:&#34;0&#34;,&#34;24&#34;:&#34;0&#34;,&#34;25&#34;:&#34;0&#34;,&#34;26&#34;:&#34;0&#34;,&#34;27&#34;:&#34;0&#34;,&#34;28&#34;:&#34;0&#34;,&#34;29&#34;:&#34;0&#34;,&#34;30&#34;:&#34;1&#34;,&#34;31&#34;:&#34;65.76&#34;,&#34;32&#34;:&#34;0&#34;,&#34;33&#34;:&#34;0&#34;,&#34;34&#34;:&#34;125&#34;,&#34;35&#34;:&#34;1&#34;,&#34;36&#34;:&#34;7.5&#34;,&#34;37&#34;:&#34;145&#34;,&#34;38&#34;:&#34;44&#34;,&#34;39&#34;:&#34;672&#34;,&#34;40&#34;:&#34;0&#34;,&#34;41&#34;:&#34;1&#34;,&#34;42&#34;:&#34;0&#34;,&#34;43&#34;:&#34;B1&#34;,&#34;44&#34;:&#34;S&#34;,&#34;45&#34;:&#34;1&#34;,&#34;46&#34;:&#34;1144.24&#34;,&#34;47&#34;:&#34;0&#34;,&#34;48&#34;:&#34;0&#34;},{&#34;1&#34;:&#34;6.7&#34;,&#34;2&#34;:&#34;1&#34;,&#34;3&#34;:&#34;2813.45&#34;,&#34;4&#34;:&#34;0&#34;,&#34;5&#34;:&#34;2&#34;,&#34;6&#34;:&#34;0&#34;,&#34;7&#34;:&#34;0&#34;,&#34;8&#34;:&#34;0&#34;,&#34;9&#34;:&#34;0&#34;,&#34;10&#34;:&#34;5&#34;,&#34;11&#34;:&#34;1&#34;,&#34;12&#34;:&#34;2716.55&#34;,&#34;13&#34;:&#34;0&#34;,&#34;14&#34;:&#34;0.00&#34;,&#34;15&#34;:&#34;0&#34;,&#34;16&#34;:&#34;0&#34;,&#34;17&#34;:&#34;0&#34;,&#34;18&#34;:&#34;0&#34;,&#34;19&#34;:&#34;0&#34;,&#34;20&#34;:&#34;0&#34;,&#34;21&#34;:&#34;0&#34;,&#34;22&#34;:&#34;0&#34;,&#34;23&#34;:&#34;0&#34;,&#34;24&#34;:&#34;0&#34;,&#34;25&#34;:&#34;0&#34;,&#34;26&#34;:&#34;0&#34;,&#34;27&#34;:&#34;0&#34;,&#34;28&#34;:&#34;0&#34;,&#34;29&#34;:&#34;0&#34;,&#34;30&#34;:&#34;0&#34;,&#34;31&#34;:&#34;0.00&#34;,&#34;32&#34;:&#34;0&#34;,&#34;33&#34;:&#34;0&#34;,&#34;34&#34;:&#34;25&#34;,&#34;35&#34;:&#34;1&#34;,&#34;36&#34;:&#34;6.0&#34;,&#34;37&#34;:&#34;101&#34;,&#34;38&#34;:&#34;46&#34;,&#34;39&#34;:&#34;648&#34;,&#34;40&#34;:&#34;0&#34;,&#34;41&#34;:&#34;1&#34;,&#34;42&#34;:&#34;1&#34;,&#34;43&#34;:&#34;B1&#34;,&#34;44&#34;:&#34;S&#34;,&#34;45&#34;:&#34;2&#34;,&#34;46&#34;:&#34;1208.94&#34;,&#34;47&#34;:&#34;0&#34;,&#34;48&#34;:&#34;0&#34;},{&#34;1&#34;:&#34;12.3&#34;,&#34;2&#34;:&#34;1&#34;,&#34;3&#34;:&#34;1069.78&#34;,&#34;4&#34;:&#34;0&#34;,&#34;5&#34;:&#34;13&#34;,&#34;6&#34;:&#34;1&#34;,&#34;7&#34;:&#34;0&#34;,&#34;8&#34;:&#34;0&#34;,&#34;9&#34;:&#34;2&#34;,&#34;10&#34;:&#34;9&#34;,&#34;11&#34;:&#34;0&#34;,&#34;12&#34;:&#34;0.00&#34;,&#34;13&#34;:&#34;0&#34;,&#34;14&#34;:&#34;0.00&#34;,&#34;15&#34;:&#34;0&#34;,&#34;16&#34;:&#34;0&#34;,&#34;17&#34;:&#34;0&#34;,&#34;18&#34;:&#34;0&#34;,&#34;19&#34;:&#34;0&#34;,&#34;20&#34;:&#34;0&#34;,&#34;21&#34;:&#34;0&#34;,&#34;22&#34;:&#34;0&#34;,&#34;23&#34;:&#34;0&#34;,&#34;24&#34;:&#34;0&#34;,&#34;25&#34;:&#34;0&#34;,&#34;26&#34;:&#34;0&#34;,&#34;27&#34;:&#34;0&#34;,&#34;28&#34;:&#34;0&#34;,&#34;29&#34;:&#34;0&#34;,&#34;30&#34;:&#34;1&#34;,&#34;31&#34;:&#34;38.62&#34;,&#34;32&#34;:&#34;0&#34;,&#34;33&#34;:&#34;0&#34;,&#34;34&#34;:&#34;19&#34;,&#34;35&#34;:&#34;0&#34;,&#34;36&#34;:&#34;3.0&#34;,&#34;37&#34;:&#34;107&#34;,&#34;38&#34;:&#34;55&#34;,&#34;39&#34;:&#34;662&#34;,&#34;40&#34;:&#34;0&#34;,&#34;41&#34;:&#34;1&#34;,&#34;42&#34;:&#34;1&#34;,&#34;43&#34;:&#34;B7&#34;,&#34;44&#34;:&#34;U&#34;,&#34;45&#34;:&#34;5&#34;,&#34;46&#34;:&#34;6813.58&#34;,&#34;47&#34;:&#34;0&#34;,&#34;48&#34;:&#34;0&#34;}],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(data_types &amp;lt;- data %&amp;gt;%
   map_dfc(class) %&amp;gt;%
   gather(key = &amp;quot;Variable&amp;quot;, value = &amp;quot;Type&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
&lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;Variable&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;Type&#34;],&#34;name&#34;:[2],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]}],&#34;data&#34;:[{&#34;1&#34;:&#34;AcctAge&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;DDA&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;DDABal&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;CashBk&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;Checks&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;DirDep&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;NSF&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;NSFAmt&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;Phone&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;Teller&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;Sav&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;SavBal&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;ATM&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;ATMAmt&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;POS&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;POSAmt&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;CD&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;CDBal&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;IRA&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;IRABal&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;LOC&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;LOCBal&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;ILS&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;ILSBal&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;MM&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;MMBal&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;MMCred&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;MTG&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;MTGBal&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;CC&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;CCBal&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;CCPurc&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;SDB&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;Income&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;HMOwn&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;LORes&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;HMVal&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;Age&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;CRScore&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;Moved&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;InArea&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;Ins&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;Branch&#34;,&#34;2&#34;:&#34;character&#34;},{&#34;1&#34;:&#34;Res&#34;,&#34;2&#34;:&#34;character&#34;},{&#34;1&#34;:&#34;Dep&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;DepAmt&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;Inv&#34;,&#34;2&#34;:&#34;numeric&#34;},{&#34;1&#34;:&#34;InvBal&#34;,&#34;2&#34;:&#34;numeric&#34;}],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_types %&amp;gt;%
  count(Type)&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
&lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;Type&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;n&#34;],&#34;name&#34;:[2],&#34;type&#34;:[&#34;int&#34;],&#34;align&#34;:[&#34;right&#34;]}],&#34;data&#34;:[{&#34;1&#34;:&#34;character&#34;,&#34;2&#34;:&#34;2&#34;},{&#34;1&#34;:&#34;numeric&#34;,&#34;2&#34;:&#34;46&#34;}],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;p&gt;We can notice that 2 out of 48 Variables are strings and all the rest are Numeric. This is not necessary correct because some of the variables could be factors. Having Metadata will be super useful right know.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;redefining-categorical-variables&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Redefining Categorical Variables&lt;/h1&gt;
&lt;p&gt;After taking a look at the data and the Metadata (that I can´t find now, but I promise I will upload) all the Variables listed next are not correctly numbers but Factors:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Listing all of the Categorical Variables according to Metadata
categorical &amp;lt;- c(&amp;quot;ATM&amp;quot;, &amp;quot;Branch&amp;quot;, &amp;quot;CC&amp;quot;, &amp;quot;CD&amp;quot;, &amp;quot;DDA&amp;quot;, &amp;quot;DirDep&amp;quot;, &amp;quot;HMOwn&amp;quot;, &amp;quot;ILS&amp;quot;, &amp;quot;IRA&amp;quot;, &amp;quot;InArea&amp;quot;, &amp;quot;Ins&amp;quot;, &amp;quot;Inv&amp;quot;, &amp;quot;LOC&amp;quot;, &amp;quot;MM&amp;quot;, &amp;quot;MTG&amp;quot;, &amp;quot;Moved&amp;quot;, &amp;quot;NSF&amp;quot;, &amp;quot;Res&amp;quot;, &amp;quot;SDB&amp;quot;, &amp;quot;Sav&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can quickly transform this into factors by using dplyr, with no need to even loop.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Transforming to Factor (Categorical Data Type in R)
data &amp;lt;- data %&amp;gt;%
  mutate_at(vars(categorical), as_factor)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The excellent package forcats offers really easy functions to recode the numbers 1 and 0 into “yes” and “no”.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Factor variables will be relabeled for better intepretation of the data
data &amp;lt;- data %&amp;gt;%
  mutate_if(is.factor, ~ fct_recode(. , yes = &amp;#39;1&amp;#39;, no = &amp;#39;0&amp;#39;)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Unknown levels in `f`: 1, 0

## Warning: Unknown levels in `f`: 1, 0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data &amp;lt;- data %&amp;gt;%
  mutate_at(&amp;quot;Res&amp;quot;, ~fct_recode(
    . ,
    rural = &amp;#39;R&amp;#39;,
    suburb = &amp;#39;S&amp;#39;,
    urban = &amp;#39;U&amp;#39;
  ))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The “Ins” Variable is the response variable and by using forcats we can shift the order of the Event Variable correctly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Defining Yes as the Event/Positive Category.
data$Ins &amp;lt;- data$Ins %&amp;gt;% fct_shift&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;discovering-missing-values&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Discovering Missing Values&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data %&amp;gt;%
  summarize_all(funs(. %&amp;gt;% is.na %&amp;gt;% sum)) %&amp;gt;%
  map_df( ~ .x * 100 / nrow(data)) %&amp;gt;%
  gather(key = &amp;quot;Variable&amp;quot;, value = &amp;quot;percent_NAs&amp;quot;) %&amp;gt;%
  arrange(desc(percent_NAs)) %&amp;gt;%
  filter(percent_NAs &amp;gt; 0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: funs() is soft deprecated as of dplyr 0.8.0
## Please use a list of either functions or lambdas: 
## 
##   # Simple named list: 
##   list(mean = mean, median = median)
## 
##   # Auto named with `tibble::lst()`: 
##   tibble::lst(mean, median)
## 
##   # Using lambdas
##   list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))
## This warning is displayed once per session.&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
&lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;Variable&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;percent_NAs&#34;],&#34;name&#34;:[2],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]}],&#34;data&#34;:[{&#34;1&#34;:&#34;Age&#34;,&#34;2&#34;:&#34;19.703075&#34;},{&#34;1&#34;:&#34;Income&#34;,&#34;2&#34;:&#34;17.920903&#34;},{&#34;1&#34;:&#34;LORes&#34;,&#34;2&#34;:&#34;17.920903&#34;},{&#34;1&#34;:&#34;HMVal&#34;,&#34;2&#34;:&#34;17.920903&#34;},{&#34;1&#34;:&#34;HMOwn&#34;,&#34;2&#34;:&#34;17.149145&#34;},{&#34;1&#34;:&#34;Phone&#34;,&#34;2&#34;:&#34;12.809943&#34;},{&#34;1&#34;:&#34;POS&#34;,&#34;2&#34;:&#34;12.809943&#34;},{&#34;1&#34;:&#34;POSAmt&#34;,&#34;2&#34;:&#34;12.809943&#34;},{&#34;1&#34;:&#34;CC&#34;,&#34;2&#34;:&#34;12.809943&#34;},{&#34;1&#34;:&#34;CCBal&#34;,&#34;2&#34;:&#34;12.809943&#34;},{&#34;1&#34;:&#34;CCPurc&#34;,&#34;2&#34;:&#34;12.809943&#34;},{&#34;1&#34;:&#34;Inv&#34;,&#34;2&#34;:&#34;12.809943&#34;},{&#34;1&#34;:&#34;InvBal&#34;,&#34;2&#34;:&#34;12.809943&#34;},{&#34;1&#34;:&#34;AcctAge&#34;,&#34;2&#34;:&#34;6.415819&#34;},{&#34;1&#34;:&#34;CRScore&#34;,&#34;2&#34;:&#34;2.191297&#34;}],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;p&gt;We can show this results in a fancy way with the following code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Counting if columns have any NAs in them
data_NA &amp;lt;- data %&amp;gt;%
  map_dfr(anyNA) %&amp;gt;%
  gather(key = &amp;quot;Variable&amp;quot;, value = &amp;quot;any_NA&amp;quot;) %&amp;gt;%
  filter(any_NA == TRUE)

#This hunk was run before to obtain the atual data types of every column
data_types &amp;lt;- data %&amp;gt;%
  map_dfc(class) %&amp;gt;%
  gather(key = &amp;quot;Variable&amp;quot;, value = &amp;quot;Type&amp;quot;)

#This Chunk counts the actual Number of NAs 
n_NA &amp;lt;- data %&amp;gt;%
  summarize_all(funs(. %&amp;gt;%
    is.na %&amp;gt;%
      sum)) %&amp;gt;%
    gather(key = &amp;quot;Variable&amp;quot;, value = &amp;quot;n_NA&amp;quot;)

# All the previous results are joined into a summary Table
data_NA %&amp;gt;%
  left_join(data_types, by = &amp;quot;Variable&amp;quot;) %&amp;gt;%
  left_join(n_NA, by = &amp;quot;Variable&amp;quot;) %&amp;gt;%
    arrange(Type)&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
&lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;Variable&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;any_NA&#34;],&#34;name&#34;:[2],&#34;type&#34;:[&#34;lgl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;Type&#34;],&#34;name&#34;:[3],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;n_NA&#34;],&#34;name&#34;:[4],&#34;type&#34;:[&#34;int&#34;],&#34;align&#34;:[&#34;right&#34;]}],&#34;data&#34;:[{&#34;1&#34;:&#34;CC&#34;,&#34;2&#34;:&#34;TRUE&#34;,&#34;3&#34;:&#34;factor&#34;,&#34;4&#34;:&#34;4133&#34;},{&#34;1&#34;:&#34;HMOwn&#34;,&#34;2&#34;:&#34;TRUE&#34;,&#34;3&#34;:&#34;factor&#34;,&#34;4&#34;:&#34;5533&#34;},{&#34;1&#34;:&#34;Inv&#34;,&#34;2&#34;:&#34;TRUE&#34;,&#34;3&#34;:&#34;factor&#34;,&#34;4&#34;:&#34;4133&#34;},{&#34;1&#34;:&#34;AcctAge&#34;,&#34;2&#34;:&#34;TRUE&#34;,&#34;3&#34;:&#34;numeric&#34;,&#34;4&#34;:&#34;2070&#34;},{&#34;1&#34;:&#34;Phone&#34;,&#34;2&#34;:&#34;TRUE&#34;,&#34;3&#34;:&#34;numeric&#34;,&#34;4&#34;:&#34;4133&#34;},{&#34;1&#34;:&#34;POS&#34;,&#34;2&#34;:&#34;TRUE&#34;,&#34;3&#34;:&#34;numeric&#34;,&#34;4&#34;:&#34;4133&#34;},{&#34;1&#34;:&#34;POSAmt&#34;,&#34;2&#34;:&#34;TRUE&#34;,&#34;3&#34;:&#34;numeric&#34;,&#34;4&#34;:&#34;4133&#34;},{&#34;1&#34;:&#34;CCBal&#34;,&#34;2&#34;:&#34;TRUE&#34;,&#34;3&#34;:&#34;numeric&#34;,&#34;4&#34;:&#34;4133&#34;},{&#34;1&#34;:&#34;CCPurc&#34;,&#34;2&#34;:&#34;TRUE&#34;,&#34;3&#34;:&#34;numeric&#34;,&#34;4&#34;:&#34;4133&#34;},{&#34;1&#34;:&#34;Income&#34;,&#34;2&#34;:&#34;TRUE&#34;,&#34;3&#34;:&#34;numeric&#34;,&#34;4&#34;:&#34;5782&#34;},{&#34;1&#34;:&#34;LORes&#34;,&#34;2&#34;:&#34;TRUE&#34;,&#34;3&#34;:&#34;numeric&#34;,&#34;4&#34;:&#34;5782&#34;},{&#34;1&#34;:&#34;HMVal&#34;,&#34;2&#34;:&#34;TRUE&#34;,&#34;3&#34;:&#34;numeric&#34;,&#34;4&#34;:&#34;5782&#34;},{&#34;1&#34;:&#34;Age&#34;,&#34;2&#34;:&#34;TRUE&#34;,&#34;3&#34;:&#34;numeric&#34;,&#34;4&#34;:&#34;6357&#34;},{&#34;1&#34;:&#34;CRScore&#34;,&#34;2&#34;:&#34;TRUE&#34;,&#34;3&#34;:&#34;numeric&#34;,&#34;4&#34;:&#34;707&#34;},{&#34;1&#34;:&#34;InvBal&#34;,&#34;2&#34;:&#34;TRUE&#34;,&#34;3&#34;:&#34;numeric&#34;,&#34;4&#34;:&#34;4133&#34;}],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;p&gt;It can be seen that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;15 Variables have Missing Values. T&lt;/li&gt;
&lt;li&gt;The range of Missing values varies from 2.19 % to 19.7 %.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;3 out of 15 are Categorical Values whereas the rest are Numeric Variables.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;During the Diploma a 2% threshold for Missing Values was discussed. Imputation was not recommended if Missing Values are greater than that. So in order to simplify the problem we will just get rid of NAs. The tidyr package does this really easily.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Sidenote&lt;/strong&gt;: I´m not completely sure about this criterion. I will be asking about this during LatinR_2019.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Droping observations with missing Values
data &amp;lt;-  data %&amp;gt;% drop_na()
#Showing distribution of records of th Target Variable
data %&amp;gt;% count(Ins)&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
&lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;Ins&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;fctr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;n&#34;],&#34;name&#34;:[2],&#34;type&#34;:[&#34;int&#34;],&#34;align&#34;:[&#34;right&#34;]}],&#34;data&#34;:[{&#34;1&#34;:&#34;yes&#34;,&#34;2&#34;:&#34;7504&#34;},{&#34;1&#34;:&#34;no&#34;,&#34;2&#34;:&#34;13373&#34;}],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Conclusion&lt;/h1&gt;
&lt;p&gt;So far we have been able to import a SAS dataset and apply a high level cleansing to organize the data, discover factor variables, reorganize the event Variable correctly and get rid of NAs.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data %&amp;gt;% glimpse&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 20,877
## Variables: 48
## $ AcctAge &amp;lt;dbl&amp;gt; 0.3, 0.7, 4.1, 0.5, 6.7, 12.3, 8.8, 9.3, 0.9, 3.0, 4.8...
## $ DDA     &amp;lt;fct&amp;gt; yes, yes, no, yes, yes, yes, yes, yes, yes, yes, yes, ...
## $ DDABal  &amp;lt;dbl&amp;gt; 419.27, 1986.81, 0.00, 1594.84, 2813.45, 1069.78, 1437...
## $ CashBk  &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
## $ Checks  &amp;lt;dbl&amp;gt; 0, 1, 0, 1, 2, 13, 12, 2, 4, 1, 0, 0, 5, 4, 9, 8, 0, 2...
## $ DirDep  &amp;lt;fct&amp;gt; no, yes, no, no, no, yes, yes, yes, no, yes, no, no, n...
## $ NSF     &amp;lt;fct&amp;gt; no, no, no, no, no, no, no, no, no, no, no, no, no, ye...
## $ NSFAmt  &amp;lt;dbl&amp;gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, ...
## $ Phone   &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, ...
## $ Teller  &amp;lt;dbl&amp;gt; 0, 0, 0, 1, 5, 9, 0, 0, 2, 1, 0, 0, 3, 2, 1, 1, 0, 2, ...
## $ Sav     &amp;lt;fct&amp;gt; yes, no, no, yes, yes, no, no, yes, yes, no, no, yes, ...
## $ SavBal  &amp;lt;dbl&amp;gt; 10233.72, 0.00, 0.00, 425.06, 2716.55, 0.00, 0.00, 967...
## $ ATM     &amp;lt;fct&amp;gt; yes, yes, no, yes, no, no, yes, yes, yes, no, no, no, ...
## $ ATMAmt  &amp;lt;dbl&amp;gt; 106.74, 268.88, 0.00, 278.07, 0.00, 0.00, 391.63, 276....
## $ POS     &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 5, 0, 0, 0, 0, ...
## $ POSAmt  &amp;lt;dbl&amp;gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 23.13,...
## $ CD      &amp;lt;fct&amp;gt; no, no, no, no, no, no, no, no, no, no, no, no, no, no...
## $ CDBal   &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
## $ IRA     &amp;lt;fct&amp;gt; no, no, no, no, no, no, no, no, no, no, no, no, no, no...
## $ IRABal  &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
## $ LOC     &amp;lt;fct&amp;gt; no, no, no, no, no, no, no, no, no, no, no, no, no, no...
## $ LOCBal  &amp;lt;dbl&amp;gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, ...
## $ ILS     &amp;lt;fct&amp;gt; no, no, no, no, no, no, no, no, no, no, no, no, no, no...
## $ ILSBal  &amp;lt;dbl&amp;gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, ...
## $ MM      &amp;lt;fct&amp;gt; no, no, no, no, no, no, no, no, no, no, no, no, no, no...
## $ MMBal   &amp;lt;dbl&amp;gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, ...
## $ MMCred  &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, ...
## $ MTG     &amp;lt;fct&amp;gt; no, no, no, no, no, no, yes, no, no, no, no, no, no, n...
## $ MTGBal  &amp;lt;dbl&amp;gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 94539.95, 0.00, 0....
## $ CC      &amp;lt;fct&amp;gt; yes, yes, yes, yes, no, yes, yes, yes, no, no, yes, ye...
## $ CCBal   &amp;lt;dbl&amp;gt; 483.65, 0.00, 0.00, 65.76, 0.00, 38.62, 85202.99, 0.00...
## $ CCPurc  &amp;lt;dbl&amp;gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...
## $ SDB     &amp;lt;fct&amp;gt; no, no, no, no, no, no, no, no, yes, no, no, no, no, n...
## $ Income  &amp;lt;dbl&amp;gt; 16, 4, 30, 125, 25, 19, 55, 13, 54, 25, 100, 13, 7, 9,...
## $ HMOwn   &amp;lt;fct&amp;gt; yes, yes, yes, yes, yes, no, yes, no, no, yes, yes, ye...
## $ LORes   &amp;lt;dbl&amp;gt; 11.0, 7.0, 8.5, 7.5, 6.0, 3.0, 3.5, 4.5, 4.0, 7.5, 13....
## $ HMVal   &amp;lt;dbl&amp;gt; 89, 87, 97, 145, 101, 107, 128, 99, 129, 95, 135, 77, ...
## $ Age     &amp;lt;dbl&amp;gt; 63, 51, 60, 44, 46, 55, 57, 58, 73, 29, 75, 51, 49, 39...
## $ CRScore &amp;lt;dbl&amp;gt; 696, 674, 640, 672, 648, 662, 659, 675, 667, 612, 715,...
## $ Moved   &amp;lt;fct&amp;gt; no, no, no, no, no, no, no, no, no, no, no, no, no, no...
## $ InArea  &amp;lt;fct&amp;gt; yes, yes, yes, yes, yes, yes, yes, yes, yes, yes, yes,...
## $ Ins     &amp;lt;fct&amp;gt; yes, no, yes, no, yes, yes, no, yes, yes, no, no, no, ...
## $ Branch  &amp;lt;fct&amp;gt; B17, B2, B3, B1, B1, B7, B1, B5, B6, B4, B9, B7, B7, B...
## $ Res     &amp;lt;fct&amp;gt; rural, rural, suburb, suburb, suburb, urban, urban, ur...
## $ Dep     &amp;lt;dbl&amp;gt; 2, 1, 0, 1, 2, 5, 2, 3, 2, 2, 0, 1, 3, 5, 2, 4, 1, 1, ...
## $ DepAmt  &amp;lt;dbl&amp;gt; 1170.06, 446.93, 0.00, 1144.24, 1208.94, 6813.58, 2237...
## $ Inv     &amp;lt;fct&amp;gt; no, no, no, no, no, no, no, no, no, no, no, no, no, no...
## $ InvBal  &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;More to come on this problem. Stay Tuned!!!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>My Final Project at the ML Diploma (Part II)</title>
      <link>/post/machine-learning-diploma-ii/</link>
      <pubDate>Tue, 24 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/machine-learning-diploma-ii/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/pagedtable/css/pagedtable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/pagedtable/js/pagedtable.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#checking-numerical-distribution&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Checking Numerical Distribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#checking-categorical-variables&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2&lt;/span&gt; Checking Categorical Variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#chi-square-test&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3&lt;/span&gt; Chi-Square Test&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4&lt;/span&gt; Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;Last time we conducted a high level cleansing of the data. Now it´s time to understand what is going on in it. In order to do that we´ll use a lot ggplot to visualize the data.&lt;/p&gt;
&lt;div id=&#34;checking-numerical-distribution&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Checking Numerical Distribution&lt;/h1&gt;
&lt;p&gt;In order to do this I should pick Numerical Variables one by one and create a ggplot.
This ould actually be quite tedious, why not to use the power of the tidyverse?&lt;/p&gt;
&lt;p&gt;We will combine select_if and walk 2 to create histograms for every of the 28 Numerical Variables.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Notice that in order to make walk work silently I had to add a print function that will use .x (every column) to create a histogram labeling it with .y that is the actual name of the current .x.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Take the data
data %&amp;gt;%
  # I select only data that is numerical
  select_if(is.numeric) %&amp;gt;%
  # I use walk 2 where .x is every numerical column seleted by select_if and
  #.y are the names of .x that will be used to add the proper label.
  walk2(names(.), ~ print( data %&amp;gt;%
                             ggplot(aes(.x)) + geom_histogram() + labs(x = .y))) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-1-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-1-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-1-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-1-6.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-1-7.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-1-8.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-1-9.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-1-10.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-1-11.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-1-12.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-1-13.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-1-14.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-1-15.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-1-16.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-1-17.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-1-18.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-1-19.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-1-20.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-1-21.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-1-22.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-1-23.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-1-24.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-1-25.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-1-26.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-1-27.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-1-28.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;checking-categorical-variables&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Checking Categorical Variables&lt;/h1&gt;
&lt;p&gt;Something equivalent can be done with categorical variables to check how they are distributed with the following code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data %&amp;gt;%
  select_if(is.factor) %&amp;gt;%
  summary()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   DDA        DirDep       NSF         Sav         ATM          CD       
##  no : 3742   no :14838   no :19044   no :11135   no : 8085   no :18244  
##  yes:17135   yes: 6039   yes: 1833   yes: 9742   yes:12792   yes: 2633  
##                                                                         
##                                                                         
##                                                                         
##                                                                         
##                                                                         
##   IRA         LOC         ILS          MM         MTG          CC       
##  no :19837   no :19742   no :19926   no :18588   no :19932   no :10936  
##  yes: 1040   yes: 1135   yes:  951   yes: 2289   yes:  945   yes: 9941  
##                                                                         
##                                                                         
##                                                                         
##                                                                         
##                                                                         
##   SDB        HMOwn       Moved       InArea       Ins       
##  no :18573   no : 9617   no :20251   no :  823   yes: 7504  
##  yes: 2304   yes:11260   yes:  626   yes:20054   no :13373  
##                                                             
##                                                             
##                                                             
##                                                             
##                                                             
##      Branch         Res        Inv       
##  B4     :4586   rural :5532   no :20272  
##  B3     :2332   suburb:7359   yes:  605  
##  B1     :2292   urban :7986              
##  B5     :2269                            
##  B2     :2267                            
##  B16    :1261                            
##  (Other):5870&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In case you want something more visual you could go with this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Take the data
data %&amp;gt;%
  # I select only data that is numerical
  select_if(is.factor) %&amp;gt;%
  # I use walk 2 where .x is every numerical column seleted by select_if and
  #.y are the names of .x that will be used to add the proper label.
  walk2(names(.), ~ print( data %&amp;gt;%
                             ggplot(aes(.x)) + geom_bar() + labs(x = .y))) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-3-2.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-3-3.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-3-4.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-3-5.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-3-6.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-3-7.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-3-8.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-3-9.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-3-10.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-3-11.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-3-12.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-3-13.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-3-14.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-3-15.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-3-16.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-3-17.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-3-18.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-3-19.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/Machine-Learning-Diploma-II/index_files/figure-html/unnamed-chunk-3-20.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;chi-square-test&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Chi-Square Test&lt;/h1&gt;
&lt;p&gt;What about performing a Chi-Square test to check the relationship between the Response variable and the Categorical Variables.&lt;/p&gt;
&lt;p&gt;Let´s create a NSE function to apply Chi-Square using purrr.&lt;/p&gt;
&lt;p&gt;We´ll use the Categorical Object created in the previous part to be looped over the chi-square function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Listing all of the Categorical Variables according to Metadata
categorical &amp;lt;- c(&amp;quot;ATM&amp;quot;, &amp;quot;Branch&amp;quot;, &amp;quot;CC&amp;quot;, &amp;quot;CD&amp;quot;, &amp;quot;DDA&amp;quot;, &amp;quot;DirDep&amp;quot;, &amp;quot;HMOwn&amp;quot;, &amp;quot;ILS&amp;quot;, &amp;quot;IRA&amp;quot;, &amp;quot;InArea&amp;quot;, &amp;quot;Ins&amp;quot;, &amp;quot;Inv&amp;quot;, &amp;quot;LOC&amp;quot;, &amp;quot;MM&amp;quot;, &amp;quot;MTG&amp;quot;, &amp;quot;Moved&amp;quot;, &amp;quot;NSF&amp;quot;, &amp;quot;Res&amp;quot;, &amp;quot;SDB&amp;quot;, &amp;quot;Sav&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Loading rlang
suppressPackageStartupMessages(library(rlang))
  
#since I want to use var as a Non Standard Evaluation Variable I need to pass that variable using the Curly-Curly Operator. That way I don´t need to quote variables and can go directly into dplyr functions such as select.
  chi_comparison &amp;lt;- function(var){
    
   pred &amp;lt;- data %&amp;gt;%
     select({{ var }})
   
   #Performs Chi-Square test and returns p.value
   return(tibble(p_val = chisq.test(pred, data$Ins)$p.value))
  }

(independent &amp;lt;- categorical %&amp;gt;%
    map_dfr(chi_comparison) %&amp;gt;%
    cbind(independent = categorical) %&amp;gt;%
    filter(p_val &amp;gt; 0.05) 
)&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
&lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;p_val&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;independent&#34;],&#34;name&#34;:[2],&#34;type&#34;:[&#34;fctr&#34;],&#34;align&#34;:[&#34;left&#34;]}],&#34;data&#34;:[{&#34;1&#34;:&#34;0.1629036&#34;,&#34;2&#34;:&#34;HMOwn&#34;},{&#34;1&#34;:&#34;0.5485035&#34;,&#34;2&#34;:&#34;ILS&#34;},{&#34;1&#34;:&#34;0.1298597&#34;,&#34;2&#34;:&#34;MTG&#34;},{&#34;1&#34;:&#34;0.8984779&#34;,&#34;2&#34;:&#34;Moved&#34;}],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;p&gt;This results in 4 Variables returning a p-value grater than 0.05. This means this variables are independent to the Response Variables, so no relationship between them exist, hence they could be removed from the model to build.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Conclusion&lt;/h1&gt;
&lt;p&gt;A quick EDA has been performed using ggplot2 combined with purrr and dplyr.
* It can be seen that Age and CRSore have distribution fairly close to Normal.
* Income is right skewed.
* Most of the Numerical Variables are higly concentrated at lower values.
On the categorical side:
* Most of the categorical variables show severe problems with class imbalances.
* HMOwn, ILS, MTG and Moved seem to have no relationship with the Response Variable.
* The Response Variable Ins show some imbalances but nothing to severe to be treated in a special way.&lt;/p&gt;
&lt;p&gt;More to come on this problem. Stay Tuned!!!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>My Final Project at the ML Diploma (Part III)</title>
      <link>/post/machine-learning-diploma-iii/</link>
      <pubDate>Tue, 24 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/machine-learning-diploma-iii/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/pagedtable/css/pagedtable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/pagedtable/js/pagedtable.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#coming-soon&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; COMING SOON&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;coming-soon&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; COMING SOON&lt;/h1&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>My Thesis</title>
      <link>/project/my-thesis/</link>
      <pubDate>Mon, 23 Sep 2019 00:00:00 -0300</pubDate>
      
      <guid>/project/my-thesis/</guid>
      <description>&lt;p&gt;Coming up with an Interesting Thesis Project is not easy at all. Actually I had 3 different projets and 5 different professors. None of them were really interested in my propositions. Thank God I found &lt;a href=&#34;http://www.ociv.usm.cl/profesores/valdebenito-c-marcos-2/&#34; target=&#34;_blank&#34;&gt;Dr. Marcos Valdebenito&lt;/a&gt;. He is really interested in Reliability Analysis in Structures and Study the Response of Random Field Variables into Strutures, you can learn more about his work on his website. Once I talked to him about I was doing in my former job he was really interested in applying Machine Learning techniques to solve this kind of problems.&lt;/p&gt;

&lt;p&gt;Normally to study the effect of Ramdom fields in Structures a Montecarlo Simulation is run several times to determine how the Structure response is affected. This is done by analyzing the mean and the Covariance of the simulations. This process is omputationally expensive since normally 10,000 to 1,000,000 simulations are needed. Every one of those simulations solves the following problem, also called the Rayleigh - Ritz Method:&lt;/p&gt;

&lt;p&gt;$$ [K] {u} = {f}$$&lt;/p&gt;

&lt;p&gt;Where $ [K] $ is the Finite Element Matrix representing the Equivalent Stiffness of the different Degrees of Fredom of the Structure.  $ {f} $ is the Equivalent Load Vector representing the forces affecting the Structure. In order to solve this problem $ [K]^{-1} $ needs to be pre-multiplied with $ {f}$ to obtain the Struture Response $ {u} $ representing the Structure displacement at every Degree of freedom. Normally $ [K] $ is a fairly large Matrix and the Inversion process costly so alternative methods to Montecarlo Simulation are deeply appreciated.&lt;/p&gt;

&lt;p&gt;So that is how we came up with a Project. What about considering $ [K] $ as a black and white image (1 channel), representing the stiffness of a Truss. Therefore, Convolutional Networks could be a good alternative to analyze the Matrix and train a Network capable of determinimg in a first instance the displacement of the Structure (${u}$) and afterwards the failure of the Structure, transforming the problem into a Clasification Binary Problem (Failing - not Failing).&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ll be posting more technical content about how I&amp;rsquo;ve been tackling the problem. Stay tuned!!!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Stiffness Method</title>
      <link>/project/stiffness-method/</link>
      <pubDate>Mon, 23 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/stiffness-method/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#the-method&#34;&gt;The Method&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#the-problem&#34;&gt;The Problem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rcpp-basics&#34;&gt;Rcpp Basics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#creating-an-rcpp-file&#34;&gt;Creating an Rcpp file&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#stiffness-method&#34;&gt;Stiffness Method&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#stiff-matrix-by-element&#34;&gt;Stiff Matrix by Element&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#active-dof-assembly&#34;&gt;Active DoF Assembly&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#connectivity-array&#34;&gt;Connectivity Array&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#stiffness-matrix-assembly&#34;&gt;Stiffness Matrix Assembly&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#load-vector-assembly&#34;&gt;Load Vector Assembly&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#solving-the-problem&#34;&gt;Solving the Problem&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusions&#34;&gt;Conclusions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;the-method&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Method&lt;/h1&gt;
&lt;p&gt;The Rayleigh Ritz Method is nothing but applying Finite Elements to Structural problems. Basically you split your structure into smaller structures that can easily be solved By solving, I mean, Calculate the specific stifness of the Structure in order to determine how the loads affects the structure. Once the individual mini-strutures are solved they are ensembled into a Merged Matrix equivalent to the total Stiffness of the Structure.&lt;/p&gt;
&lt;p&gt;The purpose of this Document is not get into deep details about the Method. If you want to learn about this you can go to this &lt;a href=&#34;https://www.sciencedirect.com/topics/engineering/stiffness-method&#34;&gt;paper&lt;/a&gt; to learn the Maths behind this. The idea is to show how to implement this in R. Since this is a computational expensive method I’ll be using &lt;code&gt;library(Rcpp)&lt;/code&gt;.&lt;/p&gt;
&lt;div id=&#34;the-problem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Problem&lt;/h2&gt;
&lt;center&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:figs1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/img/3_bar_problem.jpg&#34; alt=&#34;\label{fig:figs1}Problem Structure&#34;  /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Problem Structure
&lt;/p&gt;
&lt;/div&gt;
&lt;/center&gt;
&lt;p&gt;This is a simple problem and useful to understand the different steps of the Method.
This is implementation is for a Truss with 3 Nodes and 3 Elements where:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Number of Nodes by Element
NN_e &amp;lt;- 2
#Number of Degrees of Freedom (DoF) by Node
Ngl_N &amp;lt;- 2
L &amp;lt;- 1 #Value of L
E &amp;lt;- 2 * 10 ^ 11 # Young Module / Elasticity Metric
A &amp;lt;- 0.0001 # Cross Sectional Area
P &amp;lt;- 1000 # Load&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The First and most simple Step is to organize the Input Data. All of the Data will be input in &lt;code&gt;tibble&lt;/code&gt; form.&lt;/p&gt;
&lt;p&gt;Row i of the &lt;code&gt;Nodes&lt;/code&gt; Matrix will store the X and Y Coordinates for Every Node.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(Nodes &amp;lt;-
   tibble::tribble(~ Xi, ~ Yi,
                   0,   0,
                   sqrt(2) / 2 * L,  sqrt(2) / 2 * L,
                   sqrt(2) * L, 0))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 2
##      Xi    Yi
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 0     0    
## 2 0.707 0.707
## 3 1.41  0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Row j of the &lt;code&gt;Elements&lt;/code&gt; Matrix will contain the Initial Node &lt;code&gt;ni&lt;/code&gt;, the ending Node &lt;code&gt;nf&lt;/code&gt; and the corresponding E and A properties for Element j. In this case all the Elements share the same properties.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(Elements &amp;lt;-
   tibble::tribble(~ ni, ~ nf,   ~ E,   ~ A,
                   1,   2,    E,    A,
                   2,   3,    E,    A,
                   3,   1,    E,    A))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 4
##      ni    nf            E      A
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
## 1     1     2 200000000000 0.0001
## 2     2     3 200000000000 0.0001
## 3     3     1 200000000000 0.0001&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Row i of the &lt;code&gt;Loads&lt;/code&gt; Matrix contains the x and y vectorial component of the Loads for Node i.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(Loads &amp;lt;-
   tibble::tribble(~ Px, ~ Py,
                   0,   0,
                   0,   P,
                   0,   0))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 2
##      Px    Py
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1     0     0
## 2     0  1000
## 3     0     0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Row i correspond to the freedom of the X and Y Component of the Node i. 1 meaning no Movement and 0 meaning free movement.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(Supports &amp;lt;-
   tibble::tribble(~ Rx, ~ Ry,
                   1,   1,
                   0,   0,
                   0,   1))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 2
##      Rx    Ry
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1     1     1
## 2     0     0
## 3     0     1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;rcpp-basics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Rcpp Basics&lt;/h2&gt;
&lt;p&gt;Rcpp is the R API package to access to the huge benefits that C++ offers. I´m not an expert in C++ actually I just learned a bit of C++ because Rcpp offers easy sintax to access to C++ Elements but always showing equivalents in the R Environment.&lt;/p&gt;
&lt;p&gt;C++ is far for being an adequate language for Data Science, but once you want to optimize code or algorithms is definitely the way to go. In these case I´ll be showing the algorithm to the different steps of the Stiffness Method and how can be implemented in Rcpp.&lt;/p&gt;
&lt;p&gt;My main sources to learn Rcpp were this excellent &lt;a href=&#34;https://teuder.github.io/rcpp4everyone_en/&#34;&gt;Rcpp for Everyone&lt;/a&gt; and of course &lt;a href=&#34;http://adv-r.had.co.nz/Rcpp.html&#34;&gt;Hadley´s Help&lt;/a&gt;. With these two resources you should have more than enough to create your first Rcpp functions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-an-rcpp-file&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Creating an Rcpp file&lt;/h2&gt;
&lt;center&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:figs2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/img/cpp_file.png&#34; alt=&#34;\label{fig:figs2}Create a C++ File&#34;  /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Create a C++ File
&lt;/p&gt;
&lt;/div&gt;
&lt;/center&gt;
&lt;p&gt;If you work with RStudio you can go to File &amp;gt; New File &amp;gt; C++ File and will open a C++ Template like this:&lt;/p&gt;
&lt;center&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:figs3&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/img/cpp_template.png&#34; alt=&#34;\label{fig:figs3}C++ Template&#34;  /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: C++ Template
&lt;/p&gt;
&lt;/div&gt;
&lt;/center&gt;
&lt;p&gt;The main thing you need to be aware of is loading the required libraries from C++. In this case we will use the following:&lt;/p&gt;
&lt;p&gt;All C++ code chunks will be combined to the chunk below:&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;// [[Rcpp::depends(RcppEigen)]]
#include &amp;lt;Rcpp.h&amp;gt;
#include &amp;lt;RcppEigen.h&amp;gt;
#include &amp;lt;Eigen/LU&amp;gt; 
#include &amp;lt;Eigen/Eigenvalues&amp;gt; 

using namespace Rcpp;
using namespace Eigen;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you may know C++ is a compiled language. Compilation means, in really simple words, to optimize and speed up the code making it available in R through functions. If you want functions to be available in the R environment they need to be preceeded by this special comment. Otherwise they can be called from within the C++ environment as intermediate functions but they won´t work in R.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;stiffness-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Stiffness Method&lt;/h2&gt;
&lt;div id=&#34;stiff-matrix-by-element&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Stiff Matrix by Element&lt;/h3&gt;
&lt;p&gt;This Step calculates Stiff for the mini-structures, meaning every single bar.&lt;/p&gt;
&lt;p&gt;Every Element Matrix has the following form that needs to be created according to its properties.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
[K]_j=\begin{bmatrix}
    c^2 &amp;amp;  &amp;amp;  &amp;amp; sim\\
    cs &amp;amp; s^2 &amp;amp;  &amp;amp; \\
    -c^2 &amp;amp; -cs &amp;amp; c^2 &amp;amp; \\
    -cs &amp;amp; -s^2 &amp;amp; cs &amp;amp; s^2 \\
    \end{bmatrix}
\]&lt;/span&gt;
The pseudo code is as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Ne \leftarrow \text{Number of Rows in the Element Matrix} \\
c \leftarrow  \text{ Sparse Matrix for Director Cosines, Dimension Ne x 1 } \\
s \leftarrow  \text{ Sparse Matrix for Director Sinus, Dimension Ne x 1 } \\
L \leftarrow  \text{ Sparse Matrix for Element Length, Dimension Ne x 1 } \\
\text{for j = 1 to Ne do}
\left\{ \begin{array}{lcc}
             Ni=Elements(j,1) \\ 
             Nf=Elements(j,2) \\
             \Delta x = Nodes(Nf,1) - Nodes(Ni,1) \\
             \Delta y = Nodes(Nf,2) - Nodes(Ni,2) \\
             L(j)=\sqrt{\Delta x^2 + \Delta y^2} \\
             c(j) = {\Delta x\over L(j)} \\
             s(j) = {\Delta y\over L(j)}
             \end{array}
   \right.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now translating this into Rcpp looks like this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You need define every object to use preceeded by its type.&lt;/li&gt;
&lt;li&gt;The output will be an R List since I want object storing the different Element Matrix.&lt;/li&gt;
&lt;li&gt;All of the Function arguments are Mandatory by default and need to go in the same order that will be used. If an Optional Argument is needed the default value needs to be defined as in NN_e.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;// [[Rcpp::export]]
// First you define the Output Type. In this case an R List.
List K_Element(NumericMatrix Nodes, NumericMatrix Elements, int NN_e = 2){
  
  // Ne is defined by using the nrow method to calculate number of rows.
  int Num_Elements = Elements.nrow();
  // c, s and L are defined Vectors since the second Dimension is 1.
  NumericVector c (Num_Elements);
  NumericVector s (Num_Elements);
  NumericVector L (Num_Elements);
  
  int j,Ni,Nf;
  // dx and dy are defined as doubles since they can contain decimals
  double dx,dy;
  List K_list (Num_Elements);
  
  
  // C++ is defined from 0 as the first element. So the pseudo code needs to be adjusted accordingly.
  // Notice the for syntax, from 0 to NE-1 defined as j&amp;lt;Num_Elementos and the ++j iterator
  for(j=0;j&amp;lt;Num_Elements;++j){
    Ni=Elements(j,0) -1;
    Nf=Elements(j,1) - 1;
    dx=Nodes(Nf,0)-Nodes(Ni,0);
    dy=Nodes(Nf,1)-Nodes(Ni,1);
    //pow is the C++ operator for ^
    L[j]=sqrt(pow(dx,2)+pow(dy,2));
    c(j)=dx/L(j);
    s(j)=dy/L(j);
    
  // This is a special way to define a Matrix by Element coming from library(RcppEigen)
    Matrix4f ke;
    ke &amp;lt;&amp;lt; pow(c[j],2),c[j]*s[j],-pow(c[j],2),-c[j]*s[j],
         c[j]*s[j],pow(s[j],2),-c[j]*s[j], -pow(s[j],2),
         -pow(c[j],2),-c[j]*s[j],pow(c[j],2),c[j]*s[j],
          -c[j]*s[j],-pow(s[j],2),c[j]*s[j],pow(s[j],2);
    //Here you populate every List Element with the corresponding Element Matrix
    K_list[j]= Elements(j,NN_e)*Elements(j,NN_e + 1)/L[j]*ke;  
    
    
  }
  
 
  
  return K_list;
}
/*** R
(K_E &amp;lt;- K_Element(Nodes,Elements))
*/&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;active-dof-assembly&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Active DoF Assembly&lt;/h3&gt;
&lt;p&gt;The Stiffness Method needs to determine what Dof are actually active, meaning that are free to move, hence are unknowns of the equation of the problem.
In order to do that it is necessary to determine which ones are free to move depending on the support Matrix and a Position Number is assigned to them.&lt;/p&gt;
&lt;p&gt;Pseudocode as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Nn \leftarrow \text{Number of Rows in the Node Matrix} \\
Gl_act \leftarrow  \text{ Sparse Matrix Dimension (NN \cdot Ngl_N) x 1 } \\
cont = 0 \\
\begin{aligned}
&amp;amp; \text{for i = 1 to Nn do } \\
&amp;amp; \text{for k = 1 to Ngl_N do} \\
\end{aligned} \\
\left\{ \begin{array}{lcc}
             \text{if Apoyos(i,k) = 0 then} \\
             cont= cont +1 \\
             pos=Ngl_N \cdot (i-1) + k \\
             Gl_act(pos)=cont \\
             \end{array}
   \right.
\]&lt;/span&gt;
Rcpp Code:&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;// [[Rcpp::export]] 
// Sparse Vector that uses Support Matrix as Input 
NumericVector Gr_Active(NumericMatrix Support, int Ngl_N = 2){
  int Num_Nodes = Support.nrow();
  int cont=0, i, k;
  //Defining Dimension of Gl Vector
  NumericVector Gl (Num_Nodes*Ngl_N);
  
  for(i = 0; i &amp;lt; Num_Nodes; ++i){
    for(k = 0; k &amp;lt; Ngl_N; ++k){
      
      if(Apoyos(i,k)==0){
        //Counter needs to be adapted since C++ starts off at Zero
        Gl[Ngl_N*i+k] = ++cont;
        
      }
    }  
    
  }
  return Gl;
  
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;connectivity-array&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Connectivity Array&lt;/h3&gt;
&lt;p&gt;The Method determines an array to identify how the different elements are connected each other. This way it is possible to create an equivalent Matrix representing the Equivalent Stiffness of the ensembled elements.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Ngle = Ngl_N \cdot NN_e \\
conect \leftarrow  \text{ Sparse Matrix Dimension Ne x Ngle } \\
\begin{aligned}
&amp;amp; \text{for j = 1 to Ne do } \\
&amp;amp; \text{for k = 1 to NN_e do} \\
&amp;amp; N_k=Elementos(j,k) \\
&amp;amp; pos1= (N_k - 1) \cdot Ngl_N \\
\end{aligned} \\
\text{ for l= 1 to Ngl_N do } \\
\left\{ \begin{array}{lcc}
             pos2=pos1+l \\
             pos3= (k-1) \cdot Ngl_N + l \\
             conect(j,pos3) = Gl_act(pos2) \\
             \end{array}
   \right.
\]&lt;/span&gt;
Rcpp Code:&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;// [[Rcpp::export]]
// This is a Numeric Matrix using Elements Matrix and Gl Vector as Input
NumericMatrix Arr_Connect(NumericMatrix Elements, NumericVector Gl, int NN_e = 2, int Ngl_N = 2){
  int Num_Elements = Elements.nrow();
  // Several counters an be defined simultaneously if sharing the same properties.
  int j, k, l, pos1, pos2, pos3;
  NumericMatrix conect(Num_Elements, NN_e * Ngl_N);
  
  for(j=0; j &amp;lt; Num_Elements; ++j){
    for(k=0; k &amp;lt; NN_e; ++k){
      pos1 = (Elements(j,k) - 1) * Ngl_N;
      for(l=0; l &amp;lt; Ngl_N; ++l){
        pos2 = pos1 + l;
       // pos3 had to be adjusted because C++ index starting at 0     
        pos3 = k * Ngl_N + l;
        conect(j,pos3) = Gl[pos2];
      }
    }
  }
  
  return conect;
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;stiffness-matrix-assembly&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Stiffness Matrix Assembly&lt;/h3&gt;
&lt;p&gt;Once the Connectivity Array and the Active DoFs are determined the Global Stiffness Matrix can be assembled. This matrix contains the Contribution of every element to an specific Node. Less Elements joined to a specific Node will end up adding less stiffness than a lot of elements being part of a Node.&lt;/p&gt;
&lt;p&gt;Pseudocode as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ N_R \leftarrow \text{ sum of all of the entries of the support Matrix }  \\
NGl_total = Ngl_N \cdot Nn - N_R \\
K \leftarrow \text{ Sparse Matrix Ngl_total x Ngl_total }  \\
\begin{aligned}
&amp;amp; \text{for j = 1 to Ne do } \\
&amp;amp; \text{for k = 1 to Ngle do} \\
\end{aligned} \\
\text{ for l= 1 to Ngl_e do } \\
\left\{ \begin{array}{lcc}
             pos1=conect(j,k) \\
             pos2=conect(j,l) \\
             text{ if conect(j,k) \neq 0 and conect(j,l) \neq 0 then } \\
             K(pos1,pos2)=K_E{j}(k,l) + K(pos1,pos2) \\
             \end{array}
   \right.
\]&lt;/span&gt;
Rcpp Code:&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;// [[Rcpp::export]]
//Numeric Matrix using Support, Gl and Conect Matrix and K_E List as Inputs
NumericMatrix K_Total(List K_E, NumericMatrix Support, NumericVector Gl, NumericMatrix conect, 
                      int NN_e = 2, int Ngl_N =2 ){
  
  int Num_Elements = K_E.length();
  int Num_Nodes = Support.nrow();
  int Nr=sum(Support), j, k, l, pos1, pos2;
  NumericMatrix K( Ngl_N * Num_Nodos- Nr );
  int Ngl_E = NN_e * Ngl_N;
  
  
  for(j=0; j&amp;lt;Num_Elements; ++j){
    for(k=0; k&amp;lt;Ngl_E; ++k){
      for(l=0; l&amp;lt;Ngl_E; ++l){
        pos1 = conect(j,k);
        pos2 = conect(j,l);
        //Notice that List Elements need to be pulled using brakets
        NumericMatrix Ke = K_E[j];
        // and operator uses double ampersand and inequality syntax follow same rules than R
        if(pos1 != 0 &amp;amp;&amp;amp; pos2 !=0){
          // += is the C++ operator to sum the new value to the current one.
          K(pos1 - 1, pos2 - 1) += Ke(k,l);
        }
      }
    }
  }
  
  return K;
  
  
  
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;load-vector-assembly&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Load Vector Assembly&lt;/h3&gt;
&lt;p&gt;This is the equivalent load Vector considering only Loads for active DoFs that are participating in the solution of the problem.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ F \leftarrow \text{ Sparse Matrix dimension Ngl_total x 1 }  \\
\begin{aligned}
&amp;amp; \text{for i = 1 to Nn do } \\
\end{aligned} \\
\text{ for k= 1 to Ngl_n do } \\
\left\{ \begin{array}{lcc}
             pos1=Ngl_n \cdot (i-1) + k \\
             pos2=Gl_act(pos1) \\
             \text{ if pos2 Loads(i,k) } \\
             F(pos2)=Cargas(i,k)\\
             \end{array}
   \right.
\]&lt;/span&gt;
Rcpp Code:&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;// [[Rcpp::export]]
NumericVector f_Total(NumericMatrix Loads, NumericVector Gl, int Nr, int Ngl_N = 2 ){
  int Num_Nodos = Loads.nrow();
  int N_t = Ngl_N * Num_Nodos - Nr;
  NumericVector F (N_t);
  int i,k,pos1,pos2;
  
  for(i=0; i &amp;lt; Num_Nodos; ++i){
    for(k=0; k &amp;lt; Ngl_N; ++k){
      pos1 = Ngl_N * i + k;
      pos2 = Gl[pos1];
      if(pos2 != 0){
        F[pos2 - 1] = Cargas(i,k);
      }
    }
  }
  
  return F;
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;solving-the-problem&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Solving the Problem&lt;/h3&gt;
&lt;p&gt;All this Steps allows to pose the following problem:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ [K] \cdot \{u\} = \{F\} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In order to get the desired displacements it is just necessary to inverse $ [K] $.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[  \{u\} = [K]^{-1} \cdot \{F\}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For this case I´ll be using RcppEigen, a Rcpp Linear Algebra Library that allows some extra Matrix operations that are useful for, in this case, Matrix inversion:&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;// I have defined a new object type called MapMatd whih is a Matrix with no specific size of doubles
typedef Map&amp;lt;MatrixXd&amp;gt; MapMatd;
// Defined a Vector with same characteristics as before
typedef Map&amp;lt;VectorXd&amp;gt; MapVecd;

// [[Rcpp::export]]
// I use a VectorXd non defined size X with double data type d
VectorXd u_vect(NumericMatrix K_Total, NumericVector f_Total){
  //I need to cast R Objects coming from Inputs into Eigen Objects. In this case i would just say trust me.
  const MapMatd K(as&amp;lt;MapMatd&amp;gt;(K_Total));
  const MapVecd f(as&amp;lt;MapVecd&amp;gt;(f_Total));
  
  //Applying Inverse Method, this is only available because K and f are already Eigen objets
  VectorXd result = K.inverse()*f;
  
  return result;
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;R and Rcpp share a very similar syntax.&lt;/li&gt;
&lt;li&gt;All R objects are compatible with Rcpp, even Lists&lt;/li&gt;
&lt;li&gt;The Main advantage of using Rcpp is that is way too faster than Regular R. This makes it especially suitable for Algorithms and Matrix manipulation.&lt;/li&gt;
&lt;li&gt;Notice that Matrices use () for indexing whereas Vectors and Lists use [].&lt;/li&gt;
&lt;li&gt;Rcpp starts at 0, make the proper adjustments when dealing with indices.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I´ll be posting another Entry using the recently reated functions to show how fast they are. Stay tuned!!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Propuesta Redes Convolucionales</title>
      <link>/project/convolutional-nets-sp/memoria-1/</link>
      <pubDate>Thu, 11 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/convolutional-nets-sp/memoria-1/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#redes-convolucionales&#34;&gt;Redes Convolucionales&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#como-se-relaciona-esto-con-las-redes-convolucionales&#34;&gt;¿Cómo se relaciona esto con las Redes Convolucionales?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#como-se-va-especializando-la-red-para-ser-cada-vez-mas-detallista&#34;&gt;¿Cómo se va especializando la red para ser cada vez más detallista?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#como-se-relaciona-este-problema-con-estructuras&#34;&gt;¿Cómo se relaciona este problema con Estructuras?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#el-paper-de-finol&#34;&gt;El Paper de Finol&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#solucion&#34;&gt;Solución&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#puntos-interesantes-para-la-investigacion.&#34;&gt;Puntos Interesantes para la Investigación.&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#era-sugerencia-quizas-no-para-la-memoria-pero-para-seguir-investigando&#34;&gt;1era Sugerencia (Quizás no para la Memoria, pero para seguir investigando)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#da-sugerencia&#34;&gt;2da Sugerencia&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#era-sugerencia.&#34;&gt;3era Sugerencia.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ta-sugerencia-crear-imagenes-estructurales&#34;&gt;4ta Sugerencia “Crear Imágenes Estructurales”&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;redes-convolucionales&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Redes Convolucionales&lt;/h1&gt;
&lt;p&gt;Las redes convolucionales es un tipo de Red que esta especializada principalmente en extraer características de imágenes. Si bien es cierto es posible utilizar Redes convolucionales para otras aplicaciones, el análisis de imágenes y la Visión Computacional son la especialidad de estas redes.&lt;/p&gt;
&lt;p&gt;La Principal ventaja sobre una Red neural normal es que no está densamente conectada. Sino que selecciona cuidadosamente las neuronas a las que se conectará cambiando la operación matricial de Producto Punto por una convolución.&lt;/p&gt;
&lt;p&gt;La definición formal de Convolución es:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ (f * g)(x)= \int_{-\infty}^{\infty}{f(\eta)\cdot g(x-\eta) d\eta} \]&lt;/span&gt;
La interpretación más clásica de esta integral corresponde al producto de funciones al desplazar una por sobre la otra.&lt;/p&gt;
&lt;div id=&#34;como-se-relaciona-esto-con-las-redes-convolucionales&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;¿Cómo se relaciona esto con las Redes Convolucionales?&lt;/h2&gt;
&lt;p&gt;El análisis de Imágenes es bastante complicado. La manera en la que un computador es capaz de ver una imágen es parseando e interpretandola por un Tensor de Pixeles. Dependiendo de la resolución dela Imagen este Tensor será cada vez más grande, por lo tanto usar Redes Densas supone un gran costo computacional por todos los $ w_{i,j}$ que será necesario calcular.&lt;/p&gt;
&lt;p&gt;Normalmente el análisis de Imágenes utiliza tensor de este estilo:&lt;/p&gt;
&lt;center&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:figs1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/img/4D_Tensor.png&#34; alt=&#34;\label{fig:figs1}Estructura de Tensores de Imágenes&#34;  /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Estructura de Tensores de Imágenes
&lt;/p&gt;
&lt;/div&gt;
&lt;/center&gt;
&lt;p&gt;La Matriz de Pixeles de Dimensiones Alto x Ancho más una Pofundidad de Canales RGB que dan el color.&lt;/p&gt;
&lt;p&gt;Es por esto que las redes convolucionales ocupan esta operación para reconocer patrones específicos dentro del Tensor de Imagen. La manera en que está convolución se lleva a cabo es por medio de un Filtro. Este filtro es otro Tensor que posee generalmente dimensiones de 3x3 o 5x5 el cual se desplaza a través del tensor de imágenes para construir un Mapa de Características.&lt;/p&gt;
&lt;center&gt;
&lt;p&gt;&lt;img src=&#34;https://saama-dbe0.kxcdn.com/wp-content/uploads/2017/12/01.jpg?iv=124&#34; /&gt;&lt;/p&gt;
&lt;/center&gt;
&lt;center&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:figs2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/img/Invisible.PNG&#34; alt=&#34;\label{fig:figs2}Aplicación de un Filtro en redes Convolucionales&#34;  /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Aplicación de un Filtro en redes Convolucionales
&lt;/p&gt;
&lt;/div&gt;
&lt;/center&gt;
&lt;p&gt;&lt;img src=&#34;http://ufldl.stanford.edu/tutorial/images/Convolution_schematic.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Normalmente luego de la Aplicación del Filtro el Mapa de Características es pasado por una ReLU lo que genera que se realcen ciertos atributos de la imagen lo que permite que la red aprenda a diferenciar características específicas.&lt;/p&gt;
&lt;p&gt;Dependiendo del filtro aplicado, se verán ciertos atributos de la imagen o no. Cabe destacar que los filtros parten como números aleatorios que se van entrenando en los distintos Pasos de la red (Forward and Backward).&lt;/p&gt;
&lt;p&gt;Si intentamos analizar la foto de un gato, luego de aplicar filtros, esto es lo que un computador comienza a ver:&lt;/p&gt;
&lt;center&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:figs3&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/img/Cat.PNG&#34; alt=&#34;\label{fig:figs3}Feature Map&#34;  /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: Feature Map
&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:figs4&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/img/Cat_Filters.PNG&#34; alt=&#34;\label{fig:figs4}Resultado del Entrenamiento de Filtros&#34;  /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: Resultado del Entrenamiento de Filtros
&lt;/p&gt;
&lt;/div&gt;
&lt;/center&gt;
&lt;/div&gt;
&lt;div id=&#34;como-se-va-especializando-la-red-para-ser-cada-vez-mas-detallista&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;¿Cómo se va especializando la red para ser cada vez más detallista?&lt;/h2&gt;
Luego de la red Convolucional hay una reducción de Dimensionalidad, para llevarse acabo se utiliza una Capa de pooling, esta puede ser una Average Pooling o Max Pooling (generalmente ésta última es la más utilizada) la cual reduce la dimensión del problema para luego aplicar una nueva Capa de Red convolucional pero con más filtros que el paso anterior.
&lt;center&gt;
&lt;img src=&#34;https://cdn-images-1.medium.com/max/800/1*vbfPq-HvBCkAcZhiSTZybg.png&#34; /&gt;
&lt;/center&gt;
&lt;center&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:figs5&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/img/Invisible.PNG&#34; alt=&#34;\label{fig:figs5}Max Pooling&#34;  /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 5: Max Pooling
&lt;/p&gt;
&lt;/div&gt;
&lt;/center&gt;
&lt;p&gt;Una vez que se ha llegado al nivel de espacialización deseado, se llevan todas las caracterícticas a un vector que es pasado por una capa densamente conectada para entregar los outputs correspondientes.&lt;/p&gt;
&lt;p&gt;Una Configuración típica podría verse así:&lt;/p&gt;
&lt;center&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:figs6&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/img/Car_Convnet.PNG&#34; alt=&#34;\label{fig:figs6}Especialización de la Red&#34;  /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 6: Especialización de la Red
&lt;/p&gt;
&lt;/div&gt;
&lt;/center&gt;
&lt;/div&gt;
&lt;div id=&#34;como-se-relaciona-este-problema-con-estructuras&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;¿Cómo se relaciona este problema con Estructuras?&lt;/h2&gt;
&lt;p&gt;No tiene relación alguna. Porque este tipo de redes fue diseñada para el desarrollo del análisis de imágenes. Además es la manera más intuitiva de analizar el funcionamiento de estas redes.
Las dos principales caracerísticas de las Redes Convolucionales son:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;La extracción de Características&lt;/li&gt;
&lt;li&gt;Translation/Shift Invariance&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Esto quiere decir que la red se especializa en detectar Patrones que no dependen de su posición ni que deben ser identicos para ser detectados. Por ejemplo una oreja de gato se detectara si es grande, si es pequeña, si es de otro color si esta rotada o desplazada.&lt;/p&gt;
&lt;p&gt;Son estas características las que permitirían desarrollar otro tipo de problemas si es que pueden ser planteadas como un tensor de imágen o similar.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;el-paper-de-finol&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;El Paper de Finol&lt;/h1&gt;
&lt;p&gt;El Paper propone solucionar un problema cualquiera de estructuras usando redes Convolucionales. Lo que ellos primeramente plantean es la obtención de Valores propios para un material de propiedades variables.&lt;/p&gt;
&lt;p&gt;Para ello se utiliza una barra dividida en 100 Elementos en las que se miden dos Propiedades, el Módulo de Elasticidad E y la Densidad $ $ .&lt;/p&gt;
&lt;p&gt;En simple lo que se está planteando es una red convolucional para analizar una Imagen de 1 Pixel de Alto por 100 de Ancho, donde los Canales que representan color, en este caso representan propiedades físicas del Elemento. Dada las propiedades de la imagen esto puede extrapolarse a que las Redes convolucionales pueden analizar Data Secuencial, en este caso cada Elemento de la Barra están en secuencia debido a la continuidad del la barra. Aunque cabe destacar que la red Convolucional no se deja llevar por el orden ya que sus patrones no dependen de la posición.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Mi Supuesto&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;Esto podría traducirse en que los invesigadores pensaron en quizás un elemento equivalente de un Modulo $ E_{eq} $ en serie dado que es una barra unidimensional y el orden de los $ E_{i} $ y $ _{i} $ no influyen en la propiedad equivalente final del elemento.&lt;/p&gt;
&lt;p&gt;Además el problema que se está resolviendo es de valores propios, no hay elementos externos que indiquen que el orden de las propiedades mecánicas influyan en el cálculo final.&lt;/p&gt;
&lt;p&gt;Bajo estas características es que el uso de la Red Convolucional es válida.&lt;/p&gt;
&lt;div id=&#34;solucion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Solución&lt;/h2&gt;
&lt;p&gt;Para resolver el problema de Valores propios para un Cristal dividido en 100 Elementos se utiliza la siguiente configuración:&lt;/p&gt;
&lt;center&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:figs7&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/img/Finol_Net.PNG&#34; alt=&#34;\label{fig:figs7}Solución de Finol&#34;  /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 7: Solución de Finol
&lt;/p&gt;
&lt;/div&gt;
&lt;/center&gt;
&lt;ul&gt;
&lt;li&gt;Se utiliza una Red Secuencial aplicando un Filtro de Largo 3.&lt;/li&gt;
&lt;li&gt;Al no utilizar padding el vector se reduce a 98 x 1 x 275 donde 275 corresponde teóricamente al número de filtros que está utilizando. Esto no sale definido en el Paper y es algo que averigué investigando por mi parte.&lt;/li&gt;
&lt;li&gt;Se utiliza una segunda capa convolucional. La razón de esto no lo sé. Normalmente no he visto casos de dos Capas convolucionales seguidas excepto en modelos muy avanzados.&lt;/li&gt;
&lt;li&gt;Se utiliza una capa Max Pooling para reducir de largo 2 que disminuye el tamaño de la red a la mitad.&lt;/li&gt;
&lt;li&gt;Luego de esto se une con una Red Convencional densamente conectada. Lo que normalmente he visto es que se usa una flatten Layer que aplana la reducción de dimensiones que se ha dado hasta ahora. El valor esperado debería ser de 275 x 48, pero no sé porque ocupa 500 nodos.
*Se utilizan 3 capas ocultas para luego generar una capa de 20 unidades de salida por los 20 valores propios esperados para el problema.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;puntos-interesantes-para-la-investigacion.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Puntos Interesantes para la Investigación.&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Estoy tomando un curso en Datacamp y uno público de Stanford acerca del uso de redes Convolucionales. Dentro del curso se dan varias recomendaciones:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Para un problema de imágenes de 64 x 64 x 3 para predecir si hay un gato en la imágen o no se requieren aproximadamente 10.000 imágenes.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Se recomienda usar la mínima resolución posible para distinguir a nivel humano de tal manera de bajar el poder computacional que requiere la red.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A mayor resolución más imágenes. Menor Resolución menos Imágenes.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Cualquier Matriz puede ser interpretada como una imágen. En la cual una Red Convolucional puede encontrar patrones. Por lo que perfectamente la matriz de rigidez puede ser tratado como una imagen.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:figs8&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/img/matrix_image.PNG&#34; alt=&#34;\label{fig:figs8}Transformación de Matriz en Imágen&#34;  /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 8: Transformación de Matriz en Imágen
&lt;/p&gt;
&lt;/div&gt;
&lt;/center&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Las redes convolucionales poseen propiedades de Memoria dado que su principal función es la Extracción de Características. Existen modelos pre-entrenados de redes que Pueden usarse para perfeccionar otras tareas.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Un ejemplo que leí fue una red que fue pre-entrenada con imágenes de muebles pero que luego se adaptó para mejor la identificación de animales. La red a pesar de no ser entrenada con animales, era capaz de reconocer bordes y características muy finas.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;era-sugerencia-quizas-no-para-la-memoria-pero-para-seguir-investigando&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;1era Sugerencia (Quizás no para la Memoria, pero para seguir investigando)&lt;/h4&gt;
&lt;p&gt;Uso de Redes convolucionales para calcular Inversas de la Matriz de Rigidez. Siendo este el proceso más caro de la resolución de estructuras quizás podría realizarse el desarrollo de una Red Pre-entrenada que sea capaz de calcular Inversas de Muchas Matrices de tal Manera de agilizar el proceso más caro de la resolución de estructuras.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;da-sugerencia&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;2da Sugerencia&lt;/h4&gt;
&lt;p&gt;Si bien las redes Convolucionales pueden dar buenos resultados para data secuencial, dada su caracteristica de &lt;em&gt;“Invariante a la Traslación”&lt;/em&gt;, puede encontrar patrones incorrectos cuando el orden sí importa. Se desaconseja su uso en series de tiempo, ya que el orden de la información importa al momento de predecir.
Para contrarestar esto, se agrega una capa de redes recurrentes (RNN) que ayuda a establecer la secuencia como un prerequisito del análisis.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Es posible realizar modelos paralelos en la que se ingrese data distinta como input de la Red. Por ejemplo la Matriz de Rigidez del problema como &lt;em&gt;“imagen”&lt;/em&gt; y los Propiedades Mecánicas como vector o hasta las Cargas a las que está sometida la estructura como vector, de esa manera todos los elementos son parte de los Input del problema para predecir de manera más acertada los desplazamientos normales o umbrales del problema.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;era-sugerencia.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;3era Sugerencia.&lt;/h4&gt;
&lt;p&gt;Generar una red no secuencial considerando todos los inputs del problema.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Dado que el análisis de las Imágenes poseen una característica Espacial y secuencial me da la impresión que se puede trabajar en optimizaciones al proceso de Condensación de Grados de Libertad. Sería posible que la red redujera problemas con gran cantidad de grados de libertad en Rigideces equivalentes que pueden ser planteadas como una imágen.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Así mismo dado que también posee propiedades de Profundidad podría ayudar al desarrollo de analisis de estructuras en 3 dimensiones que es algo que yo no recuerdo haber visto si no era usando algo como ETABS o SAP.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Me da la impresión de que Redes Convolucionales pueden ayudar al desarrollo de Propiedades equivalentes usando su característica secuencial para propiedades en serie y su profundidad como propiedades en paralelo.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;ta-sugerencia-crear-imagenes-estructurales&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;4ta Sugerencia “Crear Imágenes Estructurales”&lt;/h4&gt;
&lt;p&gt;Utilizar la estructura de la matriz de Rigidez con 3 Canales simulando el RGB: Rigidez, Cargas en Eje X y Cargas en Eje Y.
Habría que buscar una manera de modelar dentro de la matriz los apoyos, que normalmente no son considerados dentro de la Matriz a priori.&lt;/p&gt;
&lt;p&gt;Para un Enrejado de 9 Elementos esto sería una Imágen de 9 x 9 x 3, lo cual no supondría una gran cantidad de casos para entrenar.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Activation Functions</title>
      <link>/project/activation-functions/activation-functions/</link>
      <pubDate>Mon, 08 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/activation-functions/activation-functions/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#activation-functions&#34;&gt;Activation Functions&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#identity&#34;&gt;Identity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-function&#34;&gt;Step Function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#linear-function&#34;&gt;Linear Function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sigmoid-function&#34;&gt;Sigmoid Function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hyperbolic-tangent&#34;&gt;Hyperbolic Tangent&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#relu&#34;&gt;ReLU&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#leaky-relu&#34;&gt;Leaky ReLU&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#softmax&#34;&gt;Softmax&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#how-to-choose-the-perfect-activation-function&#34;&gt;How to choose the perfect Activation function?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;activation-functions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Activation Functions&lt;/h1&gt;
&lt;p&gt;Activation functions are one of the most important characteristic of ANN. They basically decide whether a neuron should be activated or not. When a particular threshold is reached the Neuron will fire, meaning they will transmit the input signal to the next layer of the Network. Another Important feature of Activation Functions is that some of them provide the non-linearity. This is particular important because Activation functions help expand the range of problems that the Neural Networks can address. Finally Activation functions will play a major role when optimizing the edges weights when Backpropagation Algorithm comes into play, depending of their derivatives values is how the Gradient will change helping to decrease the error associated the Network prediction.&lt;/p&gt;
&lt;p&gt;Some of these Activation Functions are:&lt;/p&gt;
&lt;div id=&#34;identity&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Identity&lt;/h2&gt;
&lt;p&gt;It is the most basic Activation, basically, do not alter the Neuron at all. The problem with this type of activation function is that is linear, transforming the Network into a Linear Regression limiting its classification capabilities for non-linear phenomena.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step Function&lt;/h2&gt;
&lt;p&gt;The binary function is extremely simple. It returns 1 if certain threshold is reached or 0 otherwise. The main drawback of this function is that his derivative is 0, meaning it is not useful in the optimizing process.&lt;/p&gt;
&lt;p&gt;The function is defined as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ f(x)= \left\{ \begin{array}{lcc}
             0 &amp;amp;   if  &amp;amp; x &amp;lt; 0 \\
             \\ 1 &amp;amp;  if  &amp;amp; x \geq 0 
             \end{array}
   \right. \]&lt;/span&gt;&lt;/p&gt;
&lt;center&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:figs&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/img/Step.PNG&#34; alt=&#34;\label{fig:figs}Step Function&#34;  /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Step Function
&lt;/p&gt;
&lt;/div&gt;
&lt;/center&gt;
&lt;/div&gt;
&lt;div id=&#34;linear-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Linear Function&lt;/h2&gt;
&lt;p&gt;This is another option, being the main difference the existence of a slope. In this case the derivative will be constant, which can be problematic because when trying to decrease the error no matter how right or off you are the gradient will be the same.&lt;/p&gt;
&lt;p&gt;The function goes as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ f(x) = a x \]&lt;/span&gt;&lt;/p&gt;
&lt;center&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:figs2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/img/Linear.PNG&#34; alt=&#34;\label{fig:figs2}Linear Function&#34;  /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Linear Function
&lt;/p&gt;
&lt;/div&gt;
&lt;/center&gt;
&lt;/div&gt;
&lt;div id=&#34;sigmoid-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sigmoid Function&lt;/h2&gt;
&lt;p&gt;This is a very popular activation function. The main advantages of this function is that it is smooth, S-shaped, it is continuously differentiable and non-linear.&lt;/p&gt;
&lt;p&gt;The function is defined as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ f(x)= \frac{1}{1+e^{-x}} \]&lt;/span&gt;&lt;/p&gt;
&lt;center&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:figs3&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/img/Sigmoid.PNG&#34; alt=&#34;\label{fig:figs3}Sigmoid Function&#34;  /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: Sigmoid Function
&lt;/p&gt;
&lt;/div&gt;
&lt;/center&gt;
&lt;p&gt;The derivative of this function is always positive and greater than 0 and x-dependent so it is very helpful when optimizing.&lt;/p&gt;
&lt;p&gt;One of the setbacks is that only ranges from 0 to 1, for one thing is very limiting with the output but for the other it is particularly useful when dealing with probabilities.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hyperbolic-tangent&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hyperbolic Tangent&lt;/h2&gt;
&lt;p&gt;Hyperbolic Tangent or $ tanh(x) $ is just an scaled version of the Sigmoid function. It is defined as follows:&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[ tanh(x)= 2 \cdot sigmoid(2x) - 1 = \frac{2}{1+e^{-2x}} - 1 \]&lt;/span&gt;
&lt;center&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:figs4&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/img/Tanh.PNG&#34; alt=&#34;\label{fig:figs4}Hyperbolic Tangent Function&#34;  /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: Hyperbolic Tangent Function
&lt;/p&gt;
&lt;/div&gt;
&lt;/center&gt;
&lt;p&gt;$ tanh $ works similarly to sigmoid but it is symmetric at the x - axis. Normally $ tanh $ and sigmoid can be used interchangeably depending on the requirements of the problem.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;relu&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ReLU&lt;/h2&gt;
&lt;p&gt;ReLu stands for Rectified Linear unit and it is defined as follows:&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[ f(x)= max(0,x) \]&lt;/span&gt;
&lt;center&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:figs5&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/img/Relu.PNG&#34; alt=&#34;\label{fig:figs5}ReLU Function&#34;  /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 5: ReLU Function
&lt;/p&gt;
&lt;/div&gt;
&lt;/center&gt;
&lt;p&gt;It is the most used function nowadays in hidden layers. The main capability is that it doesn’t activate all of the functions creating sparsity in the network, allowing efficiency in computation.&lt;/p&gt;
&lt;p&gt;This function is limited at the positive side so it is not suggested for Output Layers. Another drawback is that not activated neurons in the range $ x &amp;lt; 0 $ will not be optimized since derivative is zero.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;leaky-relu&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Leaky ReLU&lt;/h2&gt;
&lt;p&gt;This is an improved version of the ReLU function. It is not widely used yet and it has a subtle difference with ReLu: &lt;span class=&#34;math display&#34;&gt;\[ f(x)= \left\{ \begin{array}{lcc}
             ax &amp;amp;   if  &amp;amp; x &amp;lt; 0 \\
             \\ x &amp;amp;  if  &amp;amp; x \geq 0 
             \end{array}
   \right. \]&lt;/span&gt;&lt;/p&gt;
&lt;center&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:figs6&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/img/Leaky.PNG&#34; alt=&#34;\label{fig:figs6}Leaky ReLU Function&#34;  /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 6: Leaky ReLU Function
&lt;/p&gt;
&lt;/div&gt;
&lt;/center&gt;
&lt;p&gt;This solves the problem of dead neurons during Optimization process, since the derivative of $ x &amp;lt; 0 $ is not zero.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;softmax&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Softmax&lt;/h2&gt;
&lt;p&gt;This is a sigmoid kind-of function capable of handling more than 2 classes. The function is defined as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \sigma(z)_{j}=\frac{e^{z_{j}}}{\sum_{k=1}^{K}e^{z_{k}}} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The softmax functions are normally used in output layers when trying to solve classification problems with more than 2 classes..&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-choose-the-perfect-activation-function&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;How to choose the perfect Activation function?&lt;/h1&gt;
&lt;p&gt;Well, there is not a clear answer to this, but definitely some guidelines we can follow:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sigmoid functions generally work better in classification problems.&lt;/li&gt;
&lt;li&gt;Sigmoids and tanh functions are sometimes avoided due to the &lt;em&gt;vanishing gradient problem&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;ReLU function is a general activation function and is used in most cases these days.&lt;/li&gt;
&lt;li&gt;If we encounter a case of dead neurons in our networks the leaky ReLU function is the best choice.&lt;/li&gt;
&lt;li&gt;Always keep in mind that ReLU function should only be used in the hidden layers.&lt;/li&gt;
&lt;li&gt;As a rule of thumb, you can begin with using ReLU function and then move over to other activation functions in case ReLU doesn’t provide with optimum results.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://www.manning.com/books/deep-learning-with-r&#34;&gt;Deep Learning with R&lt;/a&gt; provides some other Guidelines to use Activation Functions in the Output Layer:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Binary Classification: &lt;strong&gt;Sigmoid&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Multiclass Single-Label Classification: &lt;strong&gt;Softmax&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Multiclass Multi-Label Classification: &lt;strong&gt;Sigmoid&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Regression to Arbitrary Values: &lt;strong&gt;Identity&lt;/strong&gt; or &lt;strong&gt;None&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Regression to Values between 0 to 1: &lt;strong&gt;Sigmoid&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Intro to Neural Networks</title>
      <link>/project/neural-nets-101/intro-to-nn/</link>
      <pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/neural-nets-101/intro-to-nn/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-is-an-artificial-neural-network&#34;&gt;What is an Artificial Neural Network?&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#basic-structure&#34;&gt;Basic Structure&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#what-problems-can-neural-networks-solve&#34;&gt;What Problems can Neural Networks solve?&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#classification&#34;&gt;Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#regression&#34;&gt;Regression&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#typical-problems-solved-with-neural-networks&#34;&gt;Typical Problems solved with Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;what-is-an-artificial-neural-network&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What is an Artificial Neural Network?&lt;/h2&gt;
&lt;p&gt;The Picture you see up there is far a way from what a real Neural Network looks like:&lt;/p&gt;
&lt;p&gt;Actually it is more similar to something like this:&lt;/p&gt;
&lt;center&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:figs&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/img/ANN.jpg&#34; alt=&#34;\label{fig:figs}Multi-Layer Perceptron&#34;  /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Multi-Layer Perceptron
&lt;/p&gt;
&lt;/div&gt;
&lt;/center&gt;
&lt;p&gt;The configuration showed above is nothing but a visual representation of serial Matrix Multiplications. The neural network (aka ANN that stands for Artificial Neural Networks) itself is not an algorithm, but rather a framework for many different machine learning algorithms to work together and process complex data inputs&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;ANN are inspired by Neurons but they work in a completely different fashion. The main advantage of this system is that organizes successive Matrix multiplications and provides a visual representation of the different steps in the Optimization algorithm.&lt;/p&gt;
&lt;p&gt;There are different types of Networks, Densely Connected, Convolutional, Recurrent, Long Short Memory, Radial Biased, Autoencoders, and so on. All of them having their own area of specialization, strengths and shortcomings.&lt;/p&gt;
&lt;div id=&#34;basic-structure&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Basic Structure&lt;/h3&gt;
&lt;p&gt;Every Network will contain Nodes/Units, emulating Neurons and Edges, emulating the connection between Units.&lt;/p&gt;
&lt;p&gt;Normally the Units are organized by Layers, every Network should contain a first layer for Inputs, a last Layer for Outputs and Intermediate Layers, also known as Hidden Layers.&lt;/p&gt;
&lt;center&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:figs2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/img/Basic_ANN.jpg&#34; alt=&#34;\label{fig:figs2}Basic Neural Network&#34;  /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Basic Neural Network
&lt;/p&gt;
&lt;/div&gt;
&lt;/center&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Input Nodes $ i_{j} $ will contain Input Values to train the Network.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Edges will provide weights $ w_{j} $.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Hidden Layer Nodes $ h_{j} $ will contain the result of Sum Product between Input Nodes and weighted edges connected to them.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Finally Output Nodes $ o_{j} $ will contain the result of Sum Product between hidden Nodes and the Edges Connected to them.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Optionally, Networks can include a Bias $ b_{j} $ to control values of the network.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The Network then will calculate values in the following way:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ h_{1} = w_{1} i_{1} + w_{2} i_{2} + b_{1} = 0.15 \cdot 0.05 + 0.25 \cdot 0.10 + 0.35 = 0.3825 \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ h_{2} = w_{2} i_{1} + w_{4} i_{2} + b_{1} = 0.20 \cdot 0.05 + 0.30 \cdot 0.10 + 0.35 = 0.39 \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ o_{1} = w_{5} h_{1} + w_{7} h_{2} + b_{2} = 0.40 \cdot 0.3825 + 0.50 \cdot 0.39 + 0.60 = 0.948 \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ o_{2} = w_{6} h_{1} + w_{8} h_{2} + b_{2} = 0.45 \cdot 0.3825 + 0.55 \cdot 0.39 + 0.60 = 0.986625 \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is called a forward pass. All the Input Values were able to move through the Network by the Edge Connections up to the Output. Normally, when the Network is trained, there are expected Output values that will be compared with the ones obtained with the Forward Pass in order to compute the Error. In this case 0.01 and 0.99 respectively.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-problems-can-neural-networks-solve&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What Problems can Neural Networks solve?&lt;/h3&gt;
&lt;p&gt;Normally there are two Problems that Neural Networks Solves, Classification and Regression.&lt;/p&gt;
&lt;div id=&#34;classification&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Classification&lt;/h4&gt;
&lt;p&gt;The Classification problem is the most common problem addressed by Neural Networks. It implies to classify based on a Probability. The output will calculate how likely is that an specific label corresponds to a class. Classification problems can be sub-divided into other sub-types:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Binary Classification: As the name implies, it involves two classes: Spam or not Span, Positive or Negative, Man or Woman, etc.&lt;/li&gt;
&lt;li&gt;Multiclass Classification: In this case several labels can be applied: Is it a Dog, Cat, Horse? What Car Brand is that? etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;regression&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Regression&lt;/h4&gt;
&lt;p&gt;This kind of problems involved calculate a number associated to a Metric. Typical Problems are predicting House Values, Temperature, Balances, Displacements, etc.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;typical-problems-solved-with-neural-networks&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Typical Problems solved with Neural Networks&lt;/h3&gt;
&lt;p&gt;Neural Networks are powerful and they are the most cutting-edge methodology to make computers do the most incredibly/creepy things.&lt;/p&gt;
&lt;p&gt;Even though we think computers can do anything like Analyzing Photos, Driving Cars, Recognizing Animals, Predicting Prices and so on, the scope of their work is completely limited to just one thing: &lt;strong&gt;Computing Tensors&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Tensors are the generalization to N dimensions of Matrices. Basically any problem that can be represented by Tensors is something that Neural Networks could potentially solve.&lt;/p&gt;
&lt;p&gt;Different kind of Tensors can solve specific problems. Here some examples taken from &lt;a href=&#34;https://www.manning.com/books/deep-learning-with-r&#34;&gt;Deep Learning with R&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Vector data—2D tensors of shape (samples, features): This is the Most common Data Structure Data Scientist uses in a daily basis. Basically a Matrix, having Features as Columns and Samples as Rows.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Timeseries data or sequence data—3D tensors of shape (samples, timesteps, features): This is something a little bit fancier, having several timeseries organized as a collection of matrices, this creates a 3D Tensor.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:figs3&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/img/3D_Tensor.png&#34; alt=&#34;\label{fig:figs3}Multiple Timeseries data&#34;  /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: Multiple Timeseries data
&lt;/p&gt;
&lt;/div&gt;
&lt;/center&gt;
&lt;ul&gt;
&lt;li&gt;Images—4D tensors of shape (samples, height, width, channels) or (samples, channels, height, width): Images are represented as Pixel Matrices, Every Pixel also has RGB Channels giving the color properties to it, thus a 3D Tensor. Adding several samples of Images to analize and you have a 4D Tensor.&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:figs4&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/img/4D_Tensor.png&#34; alt=&#34;\label{fig:figs4}Image Data&#34;  /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: Image Data
&lt;/p&gt;
&lt;/div&gt;
&lt;/center&gt;
&lt;ul&gt;
&lt;li&gt;Video—5D tensors of shape (samples, frames, height, width, channels) or (samples, frames, channels, height, width): Videos are nothing but a collection of Sequencial Images. So You’ll have different Samples of Sequencial Images producing which is a 4D Tensor, since you analize several samples of Videos, you’ll get a 5D Tensor.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Neural Networks are simple visual representations of Tensor Calcultions that are capable of addressing different real life problems. So far we have covered how the Networks transmit Information from Input to Output also called Forward Pass, but there are some other concepts that are necessary to understan in order to fully unerstand how to properly train a Neural Network.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Wikipedia: &lt;a href=&#34;https://en.wikipedia.org/wiki/Artificial_neural_network&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Artificial_neural_network&lt;/a&gt;&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Deep Learning</title>
      <link>/post/deep-learning/</link>
      <pubDate>Wed, 27 Mar 2019 00:50:00 +0000</pubDate>
      
      <guid>/post/deep-learning/</guid>
      <description>

&lt;h2 id=&#34;deep-learning&#34;&gt;Deep Learning&lt;/h2&gt;

&lt;p&gt;Well, I got to know Deep Learning by chance. I remember to have had a College subject called Operational Investigation Fundamentals and they taugth Artifical Intelligence algorithms. Neural networks were mentioned but I never thought I could even understand what they were about.
During my first real job (as a quasi-Engineer) I always thougth I never paid sufficient attention to that subject. The subject basically covered some optimization models and for some reason &lt;strong&gt;AI&lt;/strong&gt; was there, just sky-high level mentions in some classes.&lt;/p&gt;

&lt;p&gt;Then I remember during my &lt;strong&gt;Data Science Program&lt;/strong&gt;, we had 3 different Research Projects (this was at my Butterflies and Unicorns time, if you don&amp;rsquo;t know what I&amp;rsquo;m talking, go here). One was Data Analytics to Measure Cars Price (super-duper boring), the second was Using Sentiment Analysis to Understand whether Lyrics of Top 20 Billboard were related to Decade&amp;rsquo;s Most Important Facts (I did this, and it was super interesting) and there was a third one I really tried to sneak out: Sentiment Annalysis with Machine Learning. Machine Learning sounded really scary to be my very first Data Science Project, so I put it off.&lt;/p&gt;

&lt;p&gt;It turns out that a colleague just joined my team at EVS and he had this awesome Max Kuhn book: &lt;a href=&#34;https://www.bookdepository.com/Applied-Predictive-Modeling-Max-Kuhn/9781461468486?redirected=true&amp;amp;utm_medium=Google&amp;amp;utm_campaign=Base1&amp;amp;utm_source=CL&amp;amp;utm_content=Applied-Predictive-Modeling&amp;amp;selectCurrency=CLP&amp;amp;w=AF4JAU961V0NQ9A803TP&amp;amp;pdg=pla-309526196374:kwd-309526196374:cmp-1653769863:adg-64191594660:crv-318349950654:pid-9781461468486:dev-c&amp;amp;gclid=CjwKCAjw-ZvlBRBbEiwANw9UWplud3A9daFx70SYI7-0t29OmknuQProNRgQtt39d572kbgsKqflBhoCwV8QAvD_BwE&#34; target=&#34;_blank&#34;&gt;Applied Predictive Modeling&lt;/a&gt;. And I thought: &lt;strong&gt;&amp;ldquo;I&amp;rsquo;m more experienced now, so probably I can take a look at this&amp;rdquo;&lt;/strong&gt;. And well I discovered &lt;code&gt;caret&lt;/code&gt; and completely blew my mind (But I still dislike its little to null compatibility with Tidyverse, but thanks Max Kuhn for creating &lt;code&gt;parsnip&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;Then Nico, I think, was the first guy mentioning Deep Learning, because he created the XoR problem in VBA and also took Andrew Ng specialization Course, and
I started to feel interested in the Field.&lt;/p&gt;

&lt;p&gt;Well, I dicovered Keras, the R API just came out (I think so) and I got this other super Book: &lt;a href=&#34;https://www.amazon.com/Deep-Learning-R-Francois-Chollet/dp/161729554X&#34; target=&#34;_blank&#34;&gt;Deep Learning with R&lt;/a&gt; and I just learned so much, but not enough. I think I learned a bit of Keras but I suddenly realized I had no idea about the inner black-box, so I just wanted to learn and understand what the hell is happening inside that box.&lt;/p&gt;

&lt;p&gt;Well here I am&amp;hellip; Trying to understand&amp;hellip; and I didn&amp;rsquo;t do very well. Internet has little to no information about theory. Codes are everywhere, but why that code is useful is just not important for users.&lt;/p&gt;

&lt;p&gt;So my intent is creating content (R based obviously, because R is not popular for Deep Learning either) for me to learn, and hopefully spread the word about this.&lt;/p&gt;

&lt;p&gt;Well, the thing is I need to finish my Thesis, and my Professor told me: &lt;strong&gt;&amp;ldquo;Why don&amp;rsquo;t you combine what you know about Deep Learning (sincerely, not too much) with Structure Engineering?&lt;/strong&gt; And you know what, this is something really unexplored in the field, so here I go.&lt;/p&gt;

&lt;p&gt;Hopefully publishing my findings help to have a better understanding of what I&amp;rsquo;m doing, also i can keep track of it and help others on my way (I love teaching, so this is a good way to get started).&lt;/p&gt;

&lt;p&gt;Hope to have news about my Thesis soon&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep Learning</title>
      <link>/publication/deep-learning/</link>
      <pubDate>Wed, 27 Mar 2019 00:50:00 +0000</pubDate>
      
      <guid>/publication/deep-learning/</guid>
      <description>

&lt;h2 id=&#34;deep-learning&#34;&gt;Deep Learning&lt;/h2&gt;

&lt;p&gt;Well, I got to know Deep Learning by chance. I remember to have had a College subject called Operation Investigation Fundamentals and they taugth Artifical Intelligence algorithms. Neural networks were mentioned but I never thought I could even understand what they were about.
During my first real job (as an engineer) I always thougth I never paid sufficient attention to that subject. The subject basically covered some optimization models and for some reason &lt;strong&gt;AI&lt;/strong&gt; was there, just sky-high level mentions in some classes.&lt;/p&gt;

&lt;p&gt;Then I remember during my Data Science Program, we had 3 different Research Projects (this was at my Butterflies and Unicorns time, if you don&amp;rsquo;t know what I&amp;rsquo;m talking, go here). One was Data Analytics to Measure Cars Price (super-duper boring), the second was Using Sentiment Analysis to Understand if the Topics of Top 20 Billboard were related to Decade&amp;rsquo;s Most Important Facts (I did this, and it was super interesting) and there was a third one I really tried to sneak out: Sentiment Annalysis with Machine Learning. Machine Learning sounded really scary to be my very first Data Science Project, so I put it off.&lt;/p&gt;

&lt;p&gt;It turns out that a colleague just joined my team at EVS and he had this awesome Max Kuhn book: &lt;a href=&#34;https://www.bookdepository.com/Applied-Predictive-Modeling-Max-Kuhn/9781461468486?redirected=true&amp;amp;utm_medium=Google&amp;amp;utm_campaign=Base1&amp;amp;utm_source=CL&amp;amp;utm_content=Applied-Predictive-Modeling&amp;amp;selectCurrency=CLP&amp;amp;w=AF4JAU961V0NQ9A803TP&amp;amp;pdg=pla-309526196374:kwd-309526196374:cmp-1653769863:adg-64191594660:crv-318349950654:pid-9781461468486:dev-c&amp;amp;gclid=CjwKCAjw-ZvlBRBbEiwANw9UWplud3A9daFx70SYI7-0t29OmknuQProNRgQtt39d572kbgsKqflBhoCwV8QAvD_BwE&#34; target=&#34;_blank&#34;&gt;Applied Predictive Modeling&lt;/a&gt;. And I thought: &lt;strong&gt;&amp;ldquo;I&amp;rsquo;m more experienced now, so probably I can take a look at this&amp;rdquo;&lt;/strong&gt;. And well I discovered &lt;code&gt;caret&lt;/code&gt; and completely blew my mind (But I still dislike its little to null compatibility with Tidyverse, thanks Max Kuhn for creating Parsnip).&lt;/p&gt;

&lt;p&gt;Then Nico, I think, was the first guy mentioning Deep Learning, because he created the XoR problem in VBA and also took Andrew Ng specialization, and
I started to feel interested in the Field.&lt;/p&gt;

&lt;p&gt;Well, I dicovered Keras, the R API just came out (I think so) and I got this other super Book: &lt;a href=&#34;https://www.amazon.com/Deep-Learning-R-Francois-Chollet/dp/161729554X&#34; target=&#34;_blank&#34;&gt;Deep Learning with R&lt;/a&gt; and I just learned so much, but not enough. I think I learned a bit of Keras but suddenly realized I had no idea about the inner black-box, so I just wanted to learn and understand what the hell is happening inside that box.&lt;/p&gt;

&lt;p&gt;Well here I am&amp;hellip; Trying to understand&amp;hellip; and I didn&amp;rsquo;t do very well, The Internet has little to no information about theory. Codes are everywhere, but why that code is useful is just not important for users.&lt;/p&gt;

&lt;p&gt;So my intent is creating content (R based obviously, because R is not popular for Deep Learning either) for me to learn, and hopefully spread the word about this.&lt;/p&gt;

&lt;p&gt;Well, the thing is I need to finish my Thesis, and my Professor told me why don&amp;rsquo;t we combine what you know about Deep Learning (sincerely, not too much) with Structure Engineering, and you know what this is something really unexplored in the field, so here I go. Hopefully publishing my findings help to have a better understanding of what I&amp;rsquo;m doing, also i can keep track of it, and I can help others (I love teaching, so this is a good way to get started).&lt;/p&gt;

&lt;p&gt;Hope to have news about my Thesis soon&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>My Thesis Project</title>
      <link>/post/my-thesis/</link>
      <pubDate>Wed, 27 Mar 2019 00:50:00 +0000</pubDate>
      
      <guid>/post/my-thesis/</guid>
      <description>

&lt;h2 id=&#34;why-am-i-not-graduated-yet&#34;&gt;Why am I not graduated yet?&lt;/h2&gt;

&lt;p&gt;Graduation is something we all are longing. Well, I recognize this is something I&amp;rsquo;ve been evading. The main reason is because I&amp;rsquo;m not working on what I studied. But, what the hell, I think I like Data Science.&lt;/p&gt;

&lt;p&gt;Ok, Long Story Short&amp;hellip; I was about to study Mathematical Engineering but I asked myself, where can I possibly work as a Mathematical Engineer ? (In my current job, obviously 😅)&lt;/p&gt;

&lt;p&gt;So I decide to go for Civil Engineering because it has a decent amount of advanced Maths that was something I was looking for and a High Employment Index.&lt;/p&gt;

&lt;p&gt;Well I started working as a Civil Engineer in INVAR as soon as I finished my internship. I just loved it, I learned so much, but the thingwas that they didn&amp;rsquo;t pay too much. So I decided to take a position at Aguas del Valle. Sincerely, and very respectfully, it has been the worst choice of my life, not only because it was 5 hours away of my Hometown, but also because they took my soul out of my body and make me spend awful moments doing absolutely nothing. There is nothing more frustrating than doing nothing all day long in a desk in a Company you don&amp;rsquo;t feel a part of.&lt;/p&gt;

&lt;p&gt;So the Evalueserve Post showed up. A friend referred me to the Data Science Program (DSP) and then I fell in love with Data Science. During the program I learned SQL, VBA, SAS 👍, R, Tableau and definitely built up my English and Communication skills. Felipe, help me prepare my first interview in English and thanks to him I had the English I needed to learn all I know so far.&lt;/p&gt;

&lt;p&gt;Upskilling and moving through cities took all of my time, besides I got married (Best Decision so far) but a lot to do, hence, no time to finish my Thesis.&lt;/p&gt;

&lt;h2 id=&#34;what-happened-with-my-thesis&#34;&gt;What happened with my Thesis?&lt;/h2&gt;

&lt;p&gt;Well in INVAR I worked as an Hidraulics Engineer so I started a Thesis back in 2013 about Water Hammer. Really interesting topic, that needed Finite Differences Methods to solve really complicated Partial Differential Equations. The only proffesor with this knowledge started working wih me, but after a year he got retired. So I had to find another Professor, so in order to avoid this issue to happen again, I decided to go with one of the youngest Professor, well he left me for his Phd.&lt;/p&gt;

&lt;p&gt;Damn, Suddenly working in INVAR I noticed the CEO was my professor of Planning so I ask him to mentor me. This happened just before leaving to Aguas del Valle in La Serena. 6 months later a Cancer was diagnosed and he passed away, so freaking fast.
The Chief of Department took over all of the abandoned Thesis that my Professor&amp;rsquo;s dead left behind, but he couldn&amp;rsquo;t continue to mentor me, because he considered being in La Serena was too far away.
In 2015 I came back to Viña and I started to learn, upskill, teach, develop as a Data Scientist that I decided to put it on hold.
On 2017 I started another Thesis. Another Professor wanted to mentor me but he was of the Structural Area. That meant I needed to move to that Area.
We started investigating uncertainty on Structural Analysis. The thing is my work again didn&amp;rsquo;t give me any chance to continue.
Until now. Why now? Probably I will explain it later, because it is also related with the creation of this site.&lt;/p&gt;

&lt;h2 id=&#34;what-my-thesis-is-going-to-be-about&#34;&gt;What My Thesis is going to be about?&lt;/h2&gt;

&lt;p&gt;Good news, I will be able to mix my previous work with one thing I discovered as I developed as Data Science: &lt;strong&gt;Deep Learning in R&lt;/strong&gt; 👏.&lt;/p&gt;

&lt;p&gt;I will be posting some progress about that. Hang tight!!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>My Thesis Project</title>
      <link>/publication/my-thesis/</link>
      <pubDate>Wed, 27 Mar 2019 00:50:00 +0000</pubDate>
      
      <guid>/publication/my-thesis/</guid>
      <description>

&lt;h2 id=&#34;why-am-i-not-graduated-yet&#34;&gt;Why am I not graduated yet?&lt;/h2&gt;

&lt;p&gt;Graduation is something we all are longing. Well, I recognize this is something I&amp;rsquo;ve been evading. The main reason is because I&amp;rsquo;m not working on what I studied. But, what the hell, I think I like Data Science.&lt;/p&gt;

&lt;p&gt;Ok, Long Story Short&amp;hellip; I was about to study Mathematical Engineering but I asked myself, where can I possibly work as a Mathematical Engineer ? (In my current job, obviously 😅)&lt;/p&gt;

&lt;p&gt;So I decide to go for Civil Engineering because it has a decent amount of advanced Maths that was something I was looking for and a High Employment Index.&lt;/p&gt;

&lt;p&gt;Well I started working as a Civil Engineer in INVAR as soon as I finished my internship. I just loved it, I learned so much, but the thingwas that they didn&amp;rsquo;t pay too much. So I decided to take a position at Aguas del Valle. Sincerely, and very respectfully, it has been the worst choice of my life, not only because it was 5 hours away of my Hometown, but also because they took my soul out of my body and make me spend awful moments doing absolutely nothing. There is nothing more frustrating than doing nothing all day long in a desk in a Company you don&amp;rsquo;t feel a part of.&lt;/p&gt;

&lt;p&gt;So the Evalueserve Post showed up. A friend referred me to the Data Science Program (DSP) and then I fell in love with Data Science. During the program I learned SQL, VBA, SAS 👍, R, Tableau and definitely built up my English and Communication skills. Felipe, help me prepare my first interview in English and thanks to him I had the English I needed to learn all I know so far.&lt;/p&gt;

&lt;p&gt;Upskilling and moving through cities took all of my time, besides I got married (Best Decision so far) but a lot to do, hence, no time to finish my Thesis.&lt;/p&gt;

&lt;h2 id=&#34;what-happened-with-my-thesis&#34;&gt;What happened with my Thesis?&lt;/h2&gt;

&lt;p&gt;Well in INVAR I worked as an Hidraulics Engineer so I started a Thesis back in 2013 about Water Hammer. Really interesting topic, that needed Finite Differences Methods to solve really complicated Partial Differential Equations. The only proffesor with this knowledge started working wih me, but after a year he got retired. So I had to find another Professor, so in order to avoid this issue to happen again, I decided to go with one of the youngest Professor, well he left me for his Phd.&lt;/p&gt;

&lt;p&gt;Damn, Suddenly working in INVAR I noticed the CEO was my professor of Planning so I ask him to mentor me. This happened just before leaving to Aguas del Valle in La Serena. 6 months later a Cancer was diagnosed and he passed away, so freaking fast.
The Chief of Department took over all of the abandoned Thesis that my Professor&amp;rsquo;s dead left behind, but he couldn&amp;rsquo;t continue to mentor me, because he considered being in La Serena was too far away.
In 2015 I came back to Viña and I started to learn, upskill, teach, develop as a Data Scientist that I decided to put it on hold.
On 2017 I started another Thesis. Another Professor wanted to mentor me but he was of the Structural Area. That meant I needed to move to that Area.
We started investigating uncertainty on Structural Analysis. The thing is my work again didn&amp;rsquo;t give me any chance to continue.
Until now. Why now? Probably I will explain it later, because it is also related with the creation of this site.&lt;/p&gt;

&lt;h2 id=&#34;what-my-thesis-is-going-to-be-about&#34;&gt;What My Thesis is going to be about?&lt;/h2&gt;

&lt;p&gt;Good news, I will be able to mix my previous work with one thing I discovered as I developed as Data Science: &lt;strong&gt;Deep Learning in R&lt;/strong&gt; 👏.&lt;/p&gt;

&lt;p&gt;I will be posting some progress about that. Hang tight!!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bio</title>
      <link>/bio/</link>
      <pubDate>Fri, 01 Mar 2019 00:00:00 -0300</pubDate>
      
      <guid>/bio/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
