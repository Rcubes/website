<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>datacubeR on datacubeR</title>
    <link>/</link>
    <description>Recent content in datacubeR on datacubeR</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright> &lt;i class=&#39;fab fa-creative-commons fa-2x&#39;&gt;&lt;/i&gt;&lt;i class=&#39;fab fa-creative-commons-by fa-2x&#39;&gt;&lt;/i&gt;&lt;i class=&#39;fab fa-creative-commons-sa fa-2x&#39;&gt;&lt;/i&gt;&lt;br&gt;&amp;copy;Alfonso Tobar. Made with &lt;i class=&#39;fab fa-r-project&#39;&gt;&lt;/i&gt; Blogdown Package.</copyright>
    <lastBuildDate>Wed, 19 Aug 2020 00:50:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Using Naive Bayes as a Baseline Model</title>
      <link>/publication/problem-11/</link>
      <pubDate>Wed, 19 Aug 2020 00:50:00 +0000</pubDate>
      
      <guid>/publication/problem-11/</guid>
      <description>

&lt;h1 id=&#34;naive-bayes&#34;&gt;Naive bayes&lt;/h1&gt;

&lt;p&gt;The other day I had to prepare a class showing the benefits of using Naive Bayes. I have to say this is not a super powerful model, mainly because it makes assumptions that are most of the time not true. Nevertheless, I noticed this can be an excellent way to create a baseline model. It is easy, not very complicated to implement and the best thing is that is super fast.&lt;/p&gt;

&lt;p&gt;As you may know, this model is based on the Bayes Theorem, namely:&lt;/p&gt;

&lt;p&gt;$$P[B/A] = \frac{P[A / B] \cdot P[B]}{P[A]}$$&lt;/p&gt;

&lt;p&gt;I´m not gonna demonstrate why this happens (basically because I don´t know how), but this models is based on probabilities that are calculated out of the dataset itself. In order to assign a class the class is calculated as:&lt;/p&gt;

&lt;p&gt;$$y = k = argmax\, P[y = k] \cdot \prod{}_{i = 1}^p P[X_i/y = k]$$&lt;/p&gt;

&lt;p&gt;Where the class is denoted by the maximum probability (between the different classes) of the product of the a priori probability of y and the different likelihood of Variables $X_i$ given y = k.&lt;/p&gt;

&lt;p&gt;This example will be implemented using a lyrics dataset that I found &lt;a href=&#34;https://github.com/hiteshyalamanchili/SongGenreClassification/blob/master/dataset/english_cleaned_lyrics.zip&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. Shoutouts to Hitesh Yalamanchili for making this data available.&lt;/p&gt;

&lt;p&gt;So this naive Bayes model will be implemented in Python trying to Predict what genre a song belongs to by using its lyrics. So the implementation in Python looks like this:&lt;/p&gt;

&lt;h1 id=&#34;importing-data&#34;&gt;Importing Data&lt;/h1&gt;

&lt;p&gt;When trying to import the data I noticed this has the following form:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;data_cap.PNG&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;For some reason there is a duplicated Index. In order to avoid a weird &lt;code&gt;Unnamed: 0&lt;/code&gt; column I had to use &lt;code&gt;names&lt;/code&gt; argument in &lt;code&gt;pd.read_csv()&lt;/code&gt; to declare the actual column names to import. Even by doing that the DataFrame was imported as a double Index dataset so I had to remove one of the index using &lt;code&gt;.reset_index()&lt;/code&gt;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: In order to make the dataset manageable for demonstration purposes only I decided to use only four genres: Rock, Pop, Hip-Hop and Metal.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%%time
import pandas as pd
df = pd.read_csv(&#39;english_cleaned_lyrics.csv&#39;, header = 0, names = [&#39;song&#39;,&#39;year&#39;,&#39;artist&#39;,&#39;genre&#39;, &#39;lyrics&#39;], index_col = None).reset_index(level = 1, drop = True)
df.query(&#39;genre in [&amp;quot;Rock&amp;quot;,&amp;quot;Pop&amp;quot;,&amp;quot;Hip-Hop&amp;quot;,&amp;quot;Metal&amp;quot;]&#39;, inplace = True)
df
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Wall time: 3.32 s
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;song&lt;/th&gt;
      &lt;th&gt;year&lt;/th&gt;
      &lt;th&gt;artist&lt;/th&gt;
      &lt;th&gt;genre&lt;/th&gt;
      &lt;th&gt;lyrics&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;ego-remix&lt;/td&gt;
      &lt;td&gt;2009&lt;/td&gt;
      &lt;td&gt;beyonce-knowles&lt;/td&gt;
      &lt;td&gt;Pop&lt;/td&gt;
      &lt;td&gt;Oh baby how you doing You know I&#39;m gonna cut r...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;then-tell-me&lt;/td&gt;
      &lt;td&gt;2009&lt;/td&gt;
      &lt;td&gt;beyonce-knowles&lt;/td&gt;
      &lt;td&gt;Pop&lt;/td&gt;
      &lt;td&gt;playin everything so easy it&#39;s like you seem s...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;honesty&lt;/td&gt;
      &lt;td&gt;2009&lt;/td&gt;
      &lt;td&gt;beyonce-knowles&lt;/td&gt;
      &lt;td&gt;Pop&lt;/td&gt;
      &lt;td&gt;If you search For tenderness It isn&#39;t hard to ...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;you-are-my-rock&lt;/td&gt;
      &lt;td&gt;2009&lt;/td&gt;
      &lt;td&gt;beyonce-knowles&lt;/td&gt;
      &lt;td&gt;Pop&lt;/td&gt;
      &lt;td&gt;Oh oh oh I oh oh oh I If I wrote a book about ...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;black-culture&lt;/td&gt;
      &lt;td&gt;2009&lt;/td&gt;
      &lt;td&gt;beyonce-knowles&lt;/td&gt;
      &lt;td&gt;Pop&lt;/td&gt;
      &lt;td&gt;Party the people the people the party it&#39;s pop...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;362210&lt;/th&gt;
      &lt;td&gt;photographs-you-are-taking-now&lt;/td&gt;
      &lt;td&gt;2014&lt;/td&gt;
      &lt;td&gt;damon-albarn&lt;/td&gt;
      &lt;td&gt;Pop&lt;/td&gt;
      &lt;td&gt;When the photographs you&#39;re taking now Are tak...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;362211&lt;/th&gt;
      &lt;td&gt;you-and-me&lt;/td&gt;
      &lt;td&gt;2014&lt;/td&gt;
      &lt;td&gt;damon-albarn&lt;/td&gt;
      &lt;td&gt;Pop&lt;/td&gt;
      &lt;td&gt;I met Moko jumbie He walks on stilts through a...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;362212&lt;/th&gt;
      &lt;td&gt;hollow-ponds&lt;/td&gt;
      &lt;td&gt;2014&lt;/td&gt;
      &lt;td&gt;damon-albarn&lt;/td&gt;
      &lt;td&gt;Pop&lt;/td&gt;
      &lt;td&gt;Chill on the hollow ponds Set sail by a kid In...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;362213&lt;/th&gt;
      &lt;td&gt;the-selfish-giant&lt;/td&gt;
      &lt;td&gt;2014&lt;/td&gt;
      &lt;td&gt;damon-albarn&lt;/td&gt;
      &lt;td&gt;Pop&lt;/td&gt;
      &lt;td&gt;Celebrate the passing drugs Put them on the ba...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;362214&lt;/th&gt;
      &lt;td&gt;hostiles&lt;/td&gt;
      &lt;td&gt;2014&lt;/td&gt;
      &lt;td&gt;damon-albarn&lt;/td&gt;
      &lt;td&gt;Pop&lt;/td&gt;
      &lt;td&gt;When the serve is done And the parish shuffled...&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;178054 rows × 5 columns&lt;/p&gt;
&lt;/div&gt;

&lt;h2 id=&#34;feature-extraction&#34;&gt;Feature Extraction&lt;/h2&gt;

&lt;p&gt;In this step, we´ll use the &lt;code&gt;CountVectorizer()&lt;/code&gt; class to provide a Occurrence Matrix. In this Matrix every row will be a Document, in this case a song, whereas every column is a Word. If a word ocurrs in the Document the is denoted by a 1. The only processing to the data is stopwords removal, that is removing all the words that are too common that end up adding noise to the analysis.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ll then use the word ocurrences as predictors for the genre. The predictor will look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%%time
from sklearn.feature_extraction.text import CountVectorizer

c_vec = CountVectorizer(stop_words = &#39;english&#39;, max_features = 20000) ## I´m removing english stopwords, and setting the max number of predictors to 20000 to avoid my computer to crush.
vectorizer = c_vec.fit_transform(df[&#39;lyrics&#39;]) 
# Transform output into pandas Df for visualization
pd.DataFrame(vectorizer.toarray(), columns = c_vec.get_feature_names()) 
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Wall time: 35.6 s
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;00&lt;/th&gt;
      &lt;th&gt;000&lt;/th&gt;
      &lt;th&gt;02&lt;/th&gt;
      &lt;th&gt;03&lt;/th&gt;
      &lt;th&gt;05&lt;/th&gt;
      &lt;th&gt;06&lt;/th&gt;
      &lt;th&gt;07&lt;/th&gt;
      &lt;th&gt;09&lt;/th&gt;
      &lt;th&gt;10&lt;/th&gt;
      &lt;th&gt;100&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;zones&lt;/th&gt;
      &lt;th&gt;zonin&lt;/th&gt;
      &lt;th&gt;zoo&lt;/th&gt;
      &lt;th&gt;zoom&lt;/th&gt;
      &lt;th&gt;zoomin&lt;/th&gt;
      &lt;th&gt;zoovie&lt;/th&gt;
      &lt;th&gt;zoovier&lt;/th&gt;
      &lt;th&gt;zoowap&lt;/th&gt;
      &lt;th&gt;zu&lt;/th&gt;
      &lt;th&gt;zulu&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;178049&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;178050&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;178051&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;178052&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;178053&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;178054 rows × 20000 columns&lt;/p&gt;
&lt;/div&gt;

&lt;h2 id=&#34;setting-up-the-model&#34;&gt;Setting up the Model&lt;/h2&gt;

&lt;p&gt;The model is usper easy to set up. We just need to import &lt;code&gt;MultinomialNB&lt;/code&gt; since this is a multiclass Prediction Model. Additionally, we´ll import &lt;code&gt;train_test_split()&lt;/code&gt; to split the data into train and test, &lt;code&gt;Pipeline()&lt;/code&gt; to create the Model Pipeline (the steps to come up with the model) and &lt;code&gt;classification_report()&lt;/code&gt; to measure model performance.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.metrics import confusion_matrix, classification_report
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# X will be song lyrics and y is the genre. We´ll split the data using 40% for test purposes.
X_train, X_test, y_train, y_test = train_test_split(df[&#39;lyrics&#39;], df[&#39;genre&#39;], test_size = 0.4, random_state = 123) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then a pipeline will be set using 2 steps. The first one being the &lt;code&gt;CountVectorizer()&lt;/code&gt; named &amp;lsquo;cv&amp;rsquo; and the &lt;code&gt;MultinomialNB()&lt;/code&gt; model named &amp;lsquo;nb&amp;rsquo;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%%time
text_clf = Pipeline(steps = [
    (&#39;cv&#39;, CountVectorizer(stop_words = &#39;english&#39;, max_features = 20000)),
    (&#39;nb&#39;, MultinomialNB(alpha = 0.1))
])
text_clf.fit(X_train, y_train) # fitting the Pipeline
#predicting using the Test Set to measure performance
y_pred = text_clf.predict(X_test)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Wall time: 26.4 s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first thing to notice is that even having 178K rows and 20000 predictors the model fits in under 30 seconds. FAST!&lt;/p&gt;

&lt;p&gt;Now when it comes to results, it is not a terrible model, it has a 63% of accuracy and the Macro F1 is 62%. Not bad for just using a couple of lines of code.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(classification_report(y_test,y_pred))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;              precision    recall  f1-score   support

     Hip-Hop       0.72      0.77      0.74      9062
       Metal       0.48      0.75      0.59      8551
         Pop       0.42      0.53      0.47     13582
        Rock       0.78      0.60      0.68     40027

    accuracy                           0.63     71222
   macro avg       0.60      0.66      0.62     71222
weighted avg       0.66      0.63      0.63     71222
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;improving-the-model&#34;&gt;Improving the Model&lt;/h2&gt;

&lt;p&gt;In order to improve the model, we could run a GridSearch trying to play around with the alpha smoothing parameter that NB has. By adjusting this parameter correctly we could easily improve a bit the model without too much effort.&lt;/p&gt;

&lt;p&gt;In this case we´ll run a Grid using values from 0 to 1, as shown below. Additionally we´ll use the &amp;lsquo;f1_macro&amp;rsquo; as the metric to choose the best model using a 5-Fold Cross Validation.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%%time
from sklearn import set_config
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import precision_score, make_scorer
#Grilla de parámetros a buscar
parameters = {&#39;nb__alpha&#39;: [0, 0.001, 0.01, 0.1, 0.5, 1] }

text_clf = Pipeline(steps = [
    (&#39;cv&#39;, CountVectorizer(stop_words = &#39;english&#39;)),
    (&#39;nb&#39;, MultinomialNB())
])


searchCV = GridSearchCV(text_clf, parameters, n_jobs = -1, scoring = &#39;f1_macro&#39;, cv = 5) # 5 Fold CV optimizando el modelo por f1 macro
searchCV.fit(X_train, y_train)

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Wall time: 3min 27s

GridSearchCV(cv=5,
             estimator=Pipeline(steps=[(&#39;cv&#39;,
                                        CountVectorizer(stop_words=&#39;english&#39;)),
                                       (&#39;nb&#39;, MultinomialNB())]),
             n_jobs=-1, param_grid={&#39;nb__alpha&#39;: [0, 0.001, 0.01, 0.1, 0.5, 1]},
             scoring=&#39;f1_macro&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The GridSearch takes around 3 minutes to run 6 models using 5-Fold CV. And we can inmediately notice, small improvements for the best model:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;best_nb = searchCV.best_estimator_  # Extracting Best Model
y_pred = best_nb.predict(X_test) # Predicting the Test Set
print(classification_report(y_test,y_pred))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;              precision    recall  f1-score   support

     Hip-Hop       0.73      0.77      0.75      9062
       Metal       0.56      0.70      0.62      8551
         Pop       0.45      0.49      0.47     13582
        Rock       0.76      0.69      0.73     40027

    accuracy                           0.66     71222
   macro avg       0.63      0.66      0.64     71222
weighted avg       0.67      0.66      0.67     71222
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can inmediately see that:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Overall Accuracy improves 3%.&lt;/li&gt;
&lt;li&gt;F1 macro average improved 2%.&lt;/li&gt;
&lt;li&gt;The Rock category is the one that improves the most from 68 to 73%.&lt;/li&gt;
&lt;li&gt;There is a trade off, even though some classess improve we can see that Metal decreases, whereas Pop keep the same results.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Finally we can check that the best is achieved when using alpha equals to 1.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;best_nb.named_steps.nb.get_params()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;{&#39;alpha&#39;: 1, &#39;class_prior&#39;: None, &#39;fit_prior&#39;: True}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is just a short example on how to set up a baseline model. Hopefully this can be useful for you.&lt;/p&gt;

&lt;p&gt;See you next time!!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to use MLflow</title>
      <link>/post/mlflow/</link>
      <pubDate>Fri, 07 Aug 2020 00:50:00 +0000</pubDate>
      
      <guid>/post/mlflow/</guid>
      <description>

&lt;h1 id=&#34;how-to-use-mlflow&#34;&gt;How to use MLflow&lt;/h1&gt;

&lt;p&gt;I have to say that I love modeling, but at the same time it demands the best of me in terms of being organized. I´m a mess, and that definitely is not helpful when modeling. So I had to put lots of efforts on creating my own folder system that helps me to organize my code, my data and my outputs. But even having all of that there is something I couldn&amp;rsquo;t managed to organize and those are my experiments.&lt;/p&gt;

&lt;p&gt;When trying to come up with a model, is all about trial and error. You never know priorly what type of model, what type of preprocessing, feature engineering or whatever other hyperparameter will be the ones that will provide the best performing model. The thing is, how to organize your notebooks, and know exactly what combination of hyperparameters you used to get the best results.&lt;/p&gt;

&lt;p&gt;Here is where MLflow comes into play, providing a way to have everything organized into a nice UI. I have to say, I have never used MLflow before and at the same time I´ve never found a good tutorial that helps me understand in detail how this thing works.&lt;/p&gt;

&lt;p&gt;My idea is to test some of its functionalities by myself and with time come up with a confortable workflow that help to organize my code and my models in a better way.&lt;/p&gt;

&lt;p&gt;So first things first. Installing this thing was quite easy by using &lt;code&gt;pip install mlflow&lt;/code&gt;. Then, by running &lt;code&gt;mlflow ui&lt;/code&gt; into conda you will serve a UI in the following link: &lt;a href=&#34;http://localhost:5000&#34; target=&#34;_blank&#34;&gt;http://localhost:5000&lt;/a&gt; that looks like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;mlflow.PNG&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I have no idea how to use it. So I will keep playing around with it. In order to test out this thing, I will again use the &lt;a href=&#34;https://www.kaggle.com/c/titanic/data&#34; target=&#34;_blank&#34;&gt;titanic dataset&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
import numpy as np

df = pd.read_csv(&#39;train.csv&#39;)
df.head()
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;PassengerId&lt;/th&gt;
      &lt;th&gt;Survived&lt;/th&gt;
      &lt;th&gt;Pclass&lt;/th&gt;
      &lt;th&gt;Name&lt;/th&gt;
      &lt;th&gt;Sex&lt;/th&gt;
      &lt;th&gt;Age&lt;/th&gt;
      &lt;th&gt;SibSp&lt;/th&gt;
      &lt;th&gt;Parch&lt;/th&gt;
      &lt;th&gt;Ticket&lt;/th&gt;
      &lt;th&gt;Fare&lt;/th&gt;
      &lt;th&gt;Cabin&lt;/th&gt;
      &lt;th&gt;Embarked&lt;/th&gt;
      &lt;th&gt;Signing_date&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;Braund, Mr. Owen Harris&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;22.0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;A/5 21171&lt;/td&gt;
      &lt;td&gt;7.2500&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;S&lt;/td&gt;
      &lt;td&gt;1911-05-17&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Cumings, Mrs. John Bradley (Florence Briggs Th...&lt;/td&gt;
      &lt;td&gt;female&lt;/td&gt;
      &lt;td&gt;38.0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;PC 17599&lt;/td&gt;
      &lt;td&gt;71.2833&lt;/td&gt;
      &lt;td&gt;C85&lt;/td&gt;
      &lt;td&gt;C&lt;/td&gt;
      &lt;td&gt;1911-07-23&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;Heikkinen, Miss. Laina&lt;/td&gt;
      &lt;td&gt;female&lt;/td&gt;
      &lt;td&gt;26.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;STON/O2. 3101282&lt;/td&gt;
      &lt;td&gt;7.9250&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;S&lt;/td&gt;
      &lt;td&gt;1911-09-08&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Futrelle, Mrs. Jacques Heath (Lily May Peel)&lt;/td&gt;
      &lt;td&gt;female&lt;/td&gt;
      &lt;td&gt;35.0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;113803&lt;/td&gt;
      &lt;td&gt;53.1000&lt;/td&gt;
      &lt;td&gt;C123&lt;/td&gt;
      &lt;td&gt;S&lt;/td&gt;
      &lt;td&gt;1911-06-26&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;Allen, Mr. William Henry&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;35.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;373450&lt;/td&gt;
      &lt;td&gt;8.0500&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;S&lt;/td&gt;
      &lt;td&gt;1911-10-25&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Initially I will only Impute Nulls and will encode categories as ordinal numbers. This approach is simplistic and not necessarily the best but I just want to make a model work with this data.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.isnull().sum()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;PassengerId       0
Survived          0
Pclass            0
Name              0
Sex               0
Age             177
SibSp             0
Parch             0
Ticket            0
Fare              0
Cabin           687
Embarked          2
Signing_date      0
dtype: int64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;According to this, I will impute &lt;code&gt;Age&lt;/code&gt; with its mean. I will drop &lt;code&gt;Cabin&lt;/code&gt; and &lt;code&gt;Signing_date&lt;/code&gt;, and Impute &lt;code&gt;Embarked&lt;/code&gt; with its Mode.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: &lt;code&gt;Signing_date&lt;/code&gt; is a fake variable I created for other side project I have. Please disregard.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mean_age = df.Age.mean()
mode_embarked = df.Embarked.mode()
mean_fare = df.Fare.mean()

import category_encoders as ce

def make_data_ready(data):
    result = (data.fillna(value = {&#39;Age&#39;: mean_age, &#39;Embarked&#39;: mode_embarked, &#39;Fare&#39;: mean_fare})
        .drop(columns = [&#39;Cabin&#39;,&#39;Signing_date&#39;], errors = &#39;ignore&#39;)
        .set_index(&#39;PassengerId&#39;)
     )
    
    ord = ce.OrdinalEncoder()
    out = ord.fit_transform(result)
    return out

df = make_data_ready(df)
df.head()
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Survived&lt;/th&gt;
      &lt;th&gt;Pclass&lt;/th&gt;
      &lt;th&gt;Name&lt;/th&gt;
      &lt;th&gt;Sex&lt;/th&gt;
      &lt;th&gt;Age&lt;/th&gt;
      &lt;th&gt;SibSp&lt;/th&gt;
      &lt;th&gt;Parch&lt;/th&gt;
      &lt;th&gt;Ticket&lt;/th&gt;
      &lt;th&gt;Fare&lt;/th&gt;
      &lt;th&gt;Embarked&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;PassengerId&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;22.0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;7.2500&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;38.0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;71.2833&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;26.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;7.9250&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;35.0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;53.1000&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;35.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;8.0500&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Then, the only thing that´s left is splitting the data into Train and validation. Again, simple approach just to show more MLflow features.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.model_selection import train_test_split

X_train, X_val, y_train, y_val = train_test_split(df.drop(columns = &#39;Survived&#39;), df.Survived, test_size = 0.25, random_state = 123)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;time-to-model&#34;&gt;Time to Model&lt;/h1&gt;

&lt;p&gt;After spending more than 3 hours reading the documentation because some things didn´t work as expected (There is a high chance that I´m not smart enough and I couldn&amp;rsquo;t understand the really well organized &lt;a href=&#34;https://www.mlflow.org/docs/latest/index.html&#34; target=&#34;_blank&#34;&gt;documentation&lt;/a&gt; I managed to figure out how to make this thing to work. So the first recommended thing is to create an experiment. This can be done directly in the UI by clicing the &lt;code&gt;+&lt;/code&gt; sign in the top left corner, which is nice:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;create_exp.PNG&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Or it can be done by commands, that is my prefered choice:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import mlflow
mlflow.set_experiment(experiment_name = &#39;New Experiment&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;INFO: &#39;New Experiment&#39; does not exist. Creating a new experiment
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This command is quite nice, because it will create a experiment in case it doesn&amp;rsquo;t exist, or it will set the experiment as your active experiment in case it exists. So even though there is a &lt;code&gt;.create_experiment()&lt;/code&gt; function, I prefer this.&lt;/p&gt;

&lt;p&gt;Then the logic MLflow has is super straight forward. Once you have an experiment you open a Run, the best way to this is using &lt;code&gt;mlflow.start_run()&lt;/code&gt; as a context manager. That way once the indented block is done the run automatically closes, no need to call &lt;code&gt;mlflow.end_run()&lt;/code&gt;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;mlflow&lt;/code&gt; is the high level API which helps making easier, but some of tits functionalities can be little bit cumbersome when you have no experience, like in my case. The pros about this API is that all of the IDs will be created automatically, which is good since you&amp;rsquo;ll not be overwriting things if you forgot to change an ID, but the cons are that IDs are extremely not human readable and hard to access. In case you´ll want to control everything manually you&amp;rsquo;ll have to go to the &lt;code&gt;mlflow.tracking&lt;/code&gt; which is low level.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Ok, so once this is clear the logic is easy, in an open run you can log things, such as:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;parameters: Model Hyperparameters or anything you want to track from your current notebook. Normally these things are given.&lt;/li&gt;
&lt;li&gt;metrics: This is more related to the model and are values that are measurable, such as performance metrics.&lt;/li&gt;
&lt;li&gt;artifacts: It can be any file you want to attach to your model. It can be your data, it can be a chart, images, etc.&lt;/li&gt;
&lt;li&gt;models: This is the model as such, that will be serialized into a .pkl file. This functionality will have a separate API for every model flavor such as: &lt;code&gt;mlflow.sklearn&lt;/code&gt; for sklearn models, &lt;code&gt;mlflow.xgboost&lt;/code&gt; for xgboost models and you get the idea.
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this example, I&amp;rsquo;ll run a simple Logistic Regression, in which I will like to save some parameters such as: solver, C, and max_iter. I will open 3 Runs:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.linear_model import LogisticRegression
import mlflow
import mlflow.sklearn
C = 1.5
max_iter = 1000
name = &#39;Run 1&#39;

with mlflow.start_run(run_name = name) as run:
    
    lr = LogisticRegression(solver = &#39;lbfgs&#39;, random_state = 123, max_iter = max_iter, C = C)
    lr.fit(X_train,y_train)
    acc = lr.score(X_val,y_val)  
    
    mlflow.log_param(&amp;quot;run_id&amp;quot;, run.info.run_id)
    mlflow.log_param(&amp;quot;solver&amp;quot;, &#39;lbfgs&#39;)
    mlflow.log_param(&amp;quot;max_iter&amp;quot;, max_iter)
    mlflow.log_param(&amp;quot;C&amp;quot;, C)

    mlflow.log_metric(&amp;quot;Accuracy&amp;quot;, acc)
    mlflow.sklearn.log_model(lr, name)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The code above was run several times, and the results can be seen in the UI as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;exp_results.PNG&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;4 Runs were run. Every run automatically logs the Startting time. Right next to it there is a green or red icon indicating if the run was succesful or not. Also the Run can be logged with a Run Name which is optional, but I recommend it, because it is the only way to recognize what is your Run about. You will see that all the parameters using the &lt;code&gt;.log_param()&lt;/code&gt; are there as well as the metrics logged with &lt;code&gt;.log_metric()&lt;/code&gt;. One thing to note is that the Run name can be repeated because the unique identifier of every run is the run_id which is automatically created.&lt;/p&gt;

&lt;p&gt;I found this run_id is particularly difficult to get with the current API, and define it manually carries some other issues I don&amp;rsquo;t want to deal with. That is why is super important that when running the Run you add &lt;code&gt;mlflow.log_param(&amp;quot;run_id&amp;quot;, run.info.run_id)&lt;/code&gt; to store the run id. This will be really helpful to get access to other functionalities of the UI.&lt;/p&gt;

&lt;p&gt;Now, if you click in one of the Runs you go to another View like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;results_1.PNG&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In this first part you&amp;rsquo;ll get the logged parameters plus Info regarding the start run, the duration of the run, which is quite nice to avoid the &lt;code&gt;%%time&lt;/code&gt; magic commands, and the status.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;results_2.PNG&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;And there is another interesting part that shows the metrics and the artifacts. In our case we have only logged the model that as you can see is stored as a .pkl file.&lt;/p&gt;

&lt;p&gt;There are some other commands that are quite handy to get access to the logged objects. The first one is &lt;code&gt;mlflow.get_experiment_by_name()&lt;/code&gt;. I think this is important because It will provide the most important info about your experiment, namely the experiment_id.
The format of this information is super complicated to deal with so I recommmend to convert it into a dictionary like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dict(mlflow.get_experiment_by_name(&#39;New Experiment&#39;))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;{&#39;artifact_location&#39;: &#39;file:///C:/Users/fata2810/OneDrive%20-%20Retail%20Financiero%20Scotiabank%20Cencosud/Clases%20Python/mlruns/0&#39;,
 &#39;experiment_id&#39;: &#39;0&#39;,
 &#39;lifecycle_stage&#39;: &#39;active&#39;,
 &#39;name&#39;: &#39;New Experiment&#39;,
 &#39;tags&#39;: {}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Some other important commands are based in the &lt;code&gt;mlflow.tracking&lt;/code&gt; API such as &lt;code&gt;.get_run()&lt;/code&gt; that will provide the info about the runs and &lt;code&gt;.list_run_infos&lt;/code&gt; that will retrieve basically all the run_ids but in a really ugly way.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: Always remember the experiment_id is a string, even if they look as an integer.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;th &lt;code&gt;mlflow.tracking&lt;/code&gt; API works like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from mlflow.tracking import MlflowClient

client = MlflowClient()
client.list_run_infos(experiment_id = &#39;0&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[&amp;lt;RunInfo: artifact_uri=&#39;file:///C:/Users/fata2810/OneDrive%20-%20Retail%20Financiero%20Scotiabank%20Cencosud/Clases%20Python/mlruns/0/86ba898ad44049edb55203166cfab227/artifacts&#39;, end_time=1594022373954, experiment_id=&#39;0&#39;, lifecycle_stage=&#39;active&#39;, run_id=&#39;86ba898ad44049edb55203166cfab227&#39;, run_uuid=&#39;86ba898ad44049edb55203166cfab227&#39;, start_time=1594022373922, status=&#39;FAILED&#39;, user_id=&#39;FATA2810&#39;&amp;gt;,
 &amp;lt;RunInfo: artifact_uri=&#39;file:///C:/Users/fata2810/OneDrive%20-%20Retail%20Financiero%20Scotiabank%20Cencosud/Clases%20Python/mlruns/0/cf34867bb1ee45bc9755446eba6e073e/artifacts&#39;, end_time=1594022347517, experiment_id=&#39;0&#39;, lifecycle_stage=&#39;active&#39;, run_id=&#39;cf34867bb1ee45bc9755446eba6e073e&#39;, run_uuid=&#39;cf34867bb1ee45bc9755446eba6e073e&#39;, start_time=1594022347331, status=&#39;FINISHED&#39;, user_id=&#39;FATA2810&#39;&amp;gt;,
 &amp;lt;RunInfo: artifact_uri=&#39;file:///C:/Users/fata2810/OneDrive%20-%20Retail%20Financiero%20Scotiabank%20Cencosud/Clases%20Python/mlruns/0/d1534e5faf7642faa0f028bdaf68a6bd/artifacts&#39;, end_time=1594022336377, experiment_id=&#39;0&#39;, lifecycle_stage=&#39;active&#39;, run_id=&#39;d1534e5faf7642faa0f028bdaf68a6bd&#39;, run_uuid=&#39;d1534e5faf7642faa0f028bdaf68a6bd&#39;, start_time=1594022336288, status=&#39;FINISHED&#39;, user_id=&#39;FATA2810&#39;&amp;gt;,
 &amp;lt;RunInfo: artifact_uri=&#39;file:///C:/Users/fata2810/OneDrive%20-%20Retail%20Financiero%20Scotiabank%20Cencosud/Clases%20Python/mlruns/0/c55159dcfe884e21b8c35f18168fdcde/artifacts&#39;, end_time=1594022312153, experiment_id=&#39;0&#39;, lifecycle_stage=&#39;active&#39;, run_id=&#39;c55159dcfe884e21b8c35f18168fdcde&#39;, run_uuid=&#39;c55159dcfe884e21b8c35f18168fdcde&#39;, start_time=1594022311962, status=&#39;FINISHED&#39;, user_id=&#39;FATA2810&#39;&amp;gt;]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;client.get_run(run_id = &#39;c55159dcfe884e21b8c35f18168fdcde&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;Run: data=&amp;lt;RunData: metrics={&#39;Accuracy&#39;: 0.7937219730941704}, params={&#39;C&#39;: &#39;1.5&#39;,
 &#39;max_iter&#39;: &#39;1000&#39;,
 &#39;run_id&#39;: &#39;c55159dcfe884e21b8c35f18168fdcde&#39;,
 &#39;solver&#39;: &#39;lbfgs&#39;}, tags={&#39;mlflow.log-model.history&#39;: &#39;[{&amp;quot;run_id&amp;quot;: &amp;quot;c55159dcfe884e21b8c35f18168fdcde&amp;quot;, &#39;
                             &#39;&amp;quot;artifact_path&amp;quot;: &amp;quot;Run 1&amp;quot;, &amp;quot;utc_time_created&amp;quot;: &#39;
                             &#39;&amp;quot;2020-07-06 07:58:32.125378&amp;quot;, &amp;quot;flavors&amp;quot;: &#39;
                             &#39;{&amp;quot;python_function&amp;quot;: {&amp;quot;loader_module&amp;quot;: &#39;
                             &#39;&amp;quot;mlflow.sklearn&amp;quot;, &amp;quot;python_version&amp;quot;: &amp;quot;3.7.7&amp;quot;, &#39;
                             &#39;&amp;quot;data&amp;quot;: &amp;quot;model.pkl&amp;quot;, &amp;quot;env&amp;quot;: &amp;quot;conda.yaml&amp;quot;}, &#39;
                             &#39;&amp;quot;sklearn&amp;quot;: {&amp;quot;pickled_model&amp;quot;: &amp;quot;model.pkl&amp;quot;, &#39;
                             &#39;&amp;quot;sklearn_version&amp;quot;: &amp;quot;0.22.2.post1&amp;quot;, &#39;
                             &#39;&amp;quot;serialization_format&amp;quot;: &amp;quot;cloudpickle&amp;quot;}}}]&#39;,
 &#39;mlflow.runName&#39;: &#39;Run 1&#39;,
 &#39;mlflow.source.name&#39;: &#39;C:\\Users\\fata2810\\AppData\\Local\\Continuum\\anaconda3\\envs\\MLprojects\\lib\\site-packages\\ipykernel_launcher.py&#39;,
 &#39;mlflow.source.type&#39;: &#39;LOCAL&#39;,
 &#39;mlflow.user&#39;: &#39;FATA2810&#39;}&amp;gt;, info=&amp;lt;RunInfo: artifact_uri=&#39;file:///C:/Users/fata2810/OneDrive%20-%20Retail%20Financiero%20Scotiabank%20Cencosud/Clases%20Python/mlruns/0/c55159dcfe884e21b8c35f18168fdcde/artifacts&#39;, end_time=1594022312153, experiment_id=&#39;0&#39;, lifecycle_stage=&#39;active&#39;, run_id=&#39;c55159dcfe884e21b8c35f18168fdcde&#39;, run_uuid=&#39;c55159dcfe884e21b8c35f18168fdcde&#39;, start_time=1594022311962, status=&#39;FINISHED&#39;, user_id=&#39;FATA2810&#39;&amp;gt;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I think the most important feature that mlflow provides is the ability to store models as .pkl files and then be able to retrieve them. I think this is not very well explained in the documentation, and I wasted a lot of time understanding the correct way to this properly, so here it goes:&lt;/p&gt;

&lt;p&gt;First you need to import the flavor of your model, in my case a sklearn model, hence &lt;code&gt;import mlflow.sklearn&lt;/code&gt;. Then the documentation describes the usage of &lt;code&gt;mlflow.sklearn.load_models()&lt;/code&gt; with a URI in the following form:&lt;/p&gt;

&lt;p&gt;&amp;lsquo;runs:/&lt;run_id&gt;/relative_path_to_models&amp;rsquo;.&lt;/p&gt;

&lt;p&gt;Put into easy words the URI works like this:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The run id can be obtained from the UI or using the commands shown above.
The relative path will be the information provided in the second argument of &lt;code&gt;mlflow.sklearn.log_model(lr, name)&lt;/code&gt;. This argument name, will create a folder with the same &amp;ldquo;name&amp;rdquo;. Remember that the variable name took the same value as the Run Name.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import mlflow.sklearn
model_lr = mlflow.sklearn.load_model(f&#39;runs:/c55159dcfe884e21b8c35f18168fdcde/Run 1&#39;) #Run 1 is the name of the first experiment
model_lr
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;LogisticRegression(C=1.5, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class=&#39;auto&#39;, n_jobs=None, penalty=&#39;l2&#39;,
                   random_state=123, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=0,
                   warm_start=False)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see, now the LR model was retrieved and can be used normally since it is loaded into the Python environment.&lt;/p&gt;

&lt;p&gt;This was a brief introduction to MLflow. Even though the tool is super intuitive and easy to use it was really difficult to understand how to make it work because there are not many tutorials out there, and the docs although they are super well organized doesn&amp;rsquo;t provide code examples that for me are the best way to understand how this works.&lt;/p&gt;

&lt;p&gt;I will be uploading some other example with more advanced functionalities if I manage to understand them.&lt;/p&gt;

&lt;p&gt;Best,&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Reusable Plotting Functions in Python</title>
      <link>/publication/problem-10/</link>
      <pubDate>Sun, 05 Jul 2020 00:50:00 +0000</pubDate>
      
      <guid>/publication/problem-10/</guid>
      <description>

&lt;h1 id=&#34;creating-reusable-plotting-functions&#34;&gt;Creating reusable plotting Functions&lt;/h1&gt;

&lt;p&gt;In my new job I´ve noticed they like to explain variables impact into some target in the following way:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;example.PNG&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Normally we have a natural Rate of an Event happening shown as this TN dashed line. And you have a particular variable that is splitted into Categories showing what is the specific Rate of the Event by Category.&lt;/p&gt;

&lt;p&gt;Since this is happening so often, I decided to build a simple function to avoid all the work behind the scenes. In order to show this, I will use the well-known Titanic dataset from Kaggle, which can be downloaded from &lt;a href=&#34;https://www.kaggle.com/c/titanic/data&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So first of all. let´s import the data:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
df = pd.read_csv(&#39;train.csv&#39;)
df.head()
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;PassengerId&lt;/th&gt;
      &lt;th&gt;Survived&lt;/th&gt;
      &lt;th&gt;Pclass&lt;/th&gt;
      &lt;th&gt;Name&lt;/th&gt;
      &lt;th&gt;Sex&lt;/th&gt;
      &lt;th&gt;Age&lt;/th&gt;
      &lt;th&gt;SibSp&lt;/th&gt;
      &lt;th&gt;Parch&lt;/th&gt;
      &lt;th&gt;Ticket&lt;/th&gt;
      &lt;th&gt;Fare&lt;/th&gt;
      &lt;th&gt;Cabin&lt;/th&gt;
      &lt;th&gt;Embarked&lt;/th&gt;
      &lt;th&gt;Signing_date&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;Braund, Mr. Owen Harris&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;22.0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;A/5 21171&lt;/td&gt;
      &lt;td&gt;7.2500&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;S&lt;/td&gt;
      &lt;td&gt;1911-05-17&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Cumings, Mrs. John Bradley (Florence Briggs Th...&lt;/td&gt;
      &lt;td&gt;female&lt;/td&gt;
      &lt;td&gt;38.0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;PC 17599&lt;/td&gt;
      &lt;td&gt;71.2833&lt;/td&gt;
      &lt;td&gt;C85&lt;/td&gt;
      &lt;td&gt;C&lt;/td&gt;
      &lt;td&gt;1911-07-23&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;Heikkinen, Miss. Laina&lt;/td&gt;
      &lt;td&gt;female&lt;/td&gt;
      &lt;td&gt;26.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;STON/O2. 3101282&lt;/td&gt;
      &lt;td&gt;7.9250&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;S&lt;/td&gt;
      &lt;td&gt;1911-09-08&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Futrelle, Mrs. Jacques Heath (Lily May Peel)&lt;/td&gt;
      &lt;td&gt;female&lt;/td&gt;
      &lt;td&gt;35.0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;113803&lt;/td&gt;
      &lt;td&gt;53.1000&lt;/td&gt;
      &lt;td&gt;C123&lt;/td&gt;
      &lt;td&gt;S&lt;/td&gt;
      &lt;td&gt;1911-06-26&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;Allen, Mr. William Henry&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;35.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;373450&lt;/td&gt;
      &lt;td&gt;8.0500&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;S&lt;/td&gt;
      &lt;td&gt;1911-10-25&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;This dataset is intended to find survival rate of passengers depending of the features associated to them. In this case the Target variable is &lt;code&gt;Survived&lt;/code&gt;. If we want to understand the Surviving rate we can just simply use &lt;code&gt;.value_counts()&lt;/code&gt; to get those numbers.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.Survived.value_counts(normalize = True)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;0    0.616162
1    0.383838
Name: Survived, dtype: float64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can notice that 61% of passengers did not survived, but is there any difference when we take subsets of the data? For example does the survival rate change when analizing by sex? We expect so:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.groupby([&#39;Sex&#39;]).Survived.value_counts(normalize = True)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Sex     Survived
female  1           0.742038
        0           0.257962
male    0           0.811092
        1           0.188908
Name: Survived, dtype: float64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Actually, when splitting by Sex we notice that 74% of women survived whereas only 18% of men survived. So is there a good way to plot this? So our aim will be to show only survival rate (because death rate is just the complement) and show how that compares to the Natural Survival Rate:&lt;/p&gt;

&lt;h1 id=&#34;the-trick&#34;&gt;The Trick&lt;/h1&gt;

&lt;p&gt;The first trick is that since Survived is a binary variable, it is possible to get the same survival rates, but only for the &amp;ldquo;1&amp;rdquo;  doing the following:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.groupby(&#39;Sex&#39;).Survived.mean()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Sex
female    0.742038
male      0.188908
Name: Survived, dtype: float64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This can be easily plot using &lt;code&gt;pandas.plot()&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import matplotlib.pyplot as plt
df.groupby(&#39;Sex&#39;).Survived.mean().plot(kind = &#39;bar&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;output_12_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I will create the &lt;code&gt;plot_rate_by()&lt;/code&gt; in order to make this interesting and be very flexible. Also this will add some other functionalities such as adding titles and allow this for any dataset:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_rate_by(data,by,Target, TN, title, x, y, x_label = None, rot = 0):
    TN *=100 # converts to percentage
    # plots ading title, and optional label rotation
    ax = (data.groupby(by)[Target].mean()*100).plot(kind = &#39;bar&#39;, title = title, rot = rot) 
    plt.axhline(TN, color = &#39;r&#39;, linestyle = &#39;--&#39;) # adds dashed line
    # adds the red text box, in coordinates x and y to avoid overlapping
    plt.text(x,y,f&#39;TN = {TN}%&#39;,bbox=dict(facecolor=&#39;red&#39;, alpha=0.5)) 
    ax.set_xlabel(x_label) # optional Label for the x Axis
    return plt.show()
    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So now, it is just  matter of tun the function with the parameters and that&amp;rsquo;s it.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
# Natural Rate for survivors
tn = np.round(df.Survived.value_counts(normalize = True).loc[1],3) 
plot_rate_by(df, by = &#39;Sex&#39;, Target = &#39;Survived&#39;, TN = tn, 
  title = &#39;Survivors by Sex&#39;, x = 0.4, y = 40, x_label = None)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;output_16_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;And it can be done for any categorical variable, for example, &lt;code&gt;Pclass&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_rate_by(df, by = &#39;Pclass&#39;, Target = &#39;Survived&#39;, TN = tn, title = &#39;Survivors by Pclass&#39;, x = 1.6, y = 40, x_label = None)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;output_18_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Another thing that normally happens at my job is that they want to do this with variables that are continous. This cannot be done directly, so it is necessary to create some binning. This can be done easily. Let´s say we want to use Fare. So what is the Survival rate for people paying cheaper tickets, let´s say &lt;10, or some other values &lt;100, &lt;300 and &gt;=300. In order to this I will use &lt;code&gt;numpy.select&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;I think numpy select is not very self explanatory. Basically works as a case when in which you have to define a condition list, boolean masks, and a choice list with vallues when condition is met.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;condlist = [df.Fare &amp;lt; 10, df.Fare &amp;lt; 100, df.Fare &amp;lt; 300, df.Fare &amp;gt;= 300] # list of conditions
choicelist = [&#39;&amp;lt;10&#39;,&#39;&amp;lt;100&#39;,&#39;&amp;lt;300&#39;,&#39;&amp;gt;=300&#39;]

df[&#39;Fare_binning&#39;] = np.select(condlist, choicelist)
df[[&#39;Fare&#39;,&#39;Fare_binning&#39;]]
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Fare&lt;/th&gt;
      &lt;th&gt;Fare_binning&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;7.2500&lt;/td&gt;
      &lt;td&gt;&amp;lt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;71.2833&lt;/td&gt;
      &lt;td&gt;&amp;lt;100&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;7.9250&lt;/td&gt;
      &lt;td&gt;&amp;lt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;53.1000&lt;/td&gt;
      &lt;td&gt;&amp;lt;100&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;8.0500&lt;/td&gt;
      &lt;td&gt;&amp;lt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;886&lt;/th&gt;
      &lt;td&gt;13.0000&lt;/td&gt;
      &lt;td&gt;&amp;lt;100&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;887&lt;/th&gt;
      &lt;td&gt;30.0000&lt;/td&gt;
      &lt;td&gt;&amp;lt;100&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;888&lt;/th&gt;
      &lt;td&gt;23.4500&lt;/td&gt;
      &lt;td&gt;&amp;lt;100&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;889&lt;/th&gt;
      &lt;td&gt;30.0000&lt;/td&gt;
      &lt;td&gt;&amp;lt;100&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;890&lt;/th&gt;
      &lt;td&gt;7.7500&lt;/td&gt;
      &lt;td&gt;&amp;lt;10&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;891 rows × 2 columns&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Well normally you don&amp;rsquo;t know priorly what are good binnings to show your data so this type of operation needs to be extremely flexible. How can this transform into a function?
Let´s say we have a list of tha boundary values:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;vals_l = [10,100,300]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A for loop can be used to use this values into conditions using &lt;code&gt;.lt()&lt;/code&gt; method and creating the choices:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;condlist = []
choicelist = []
#Doing this operation for every boundary
for v in vals_l:
    condlist.append(df[&#39;Fare&#39;].lt(v)) # data[&#39;Fare&#39;].lt(v) is equivalent to data.Fare &amp;lt; v
    choicelist.append(&#39;&amp;lt;&#39;+str(v)) #concatening boundary and the &amp;lt; sign

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Additionally, it is necessary to add the last category for the &amp;gt;=. This can be done as:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;condlist.append(df[&#39;Fare&#39;].ge(vals_l[-1]))
choicelist.append(&#39;&amp;gt;=&#39;+str(vals_l[-1]))

choicelist
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[&#39;&amp;lt;10&#39;, &#39;&amp;lt;100&#39;, &#39;&amp;lt;300&#39;, &#39;&amp;gt;=300&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It can be seen that all of the categories has been correctly created. Now putting all together into a function will look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def convert_to_range(data, field, vals_l):
    vals_l = vals_l
    condlist = []
    choicelist = []
    for v in vals_l:
        condlist.append(data[field].lt(v))
        choicelist.append(&#39;&amp;lt;&#39;+str(v))
    
    condlist.append(data[field].ge(vals_l[-1]))
    choicelist.append(&#39;&amp;gt;=&#39;+str(vals_l[-1]))
    return pd.Categorical(np.select(condlist, choicelist), categories=choicelist, ordered = True) #converts everything into a pandas categorical variable
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;binning_function&#39;] = convert_to_range(df,&#39;Fare&#39;,vals_l)
df[[&#39;Fare_binning&#39;,&#39;binning_function&#39;]]
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Fare_binning&lt;/th&gt;
      &lt;th&gt;binning_function&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;&amp;lt;10&lt;/td&gt;
      &lt;td&gt;&amp;lt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;&amp;lt;100&lt;/td&gt;
      &lt;td&gt;&amp;lt;100&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;&amp;lt;10&lt;/td&gt;
      &lt;td&gt;&amp;lt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;&amp;lt;100&lt;/td&gt;
      &lt;td&gt;&amp;lt;100&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;&amp;lt;10&lt;/td&gt;
      &lt;td&gt;&amp;lt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;886&lt;/th&gt;
      &lt;td&gt;&amp;lt;100&lt;/td&gt;
      &lt;td&gt;&amp;lt;100&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;887&lt;/th&gt;
      &lt;td&gt;&amp;lt;100&lt;/td&gt;
      &lt;td&gt;&amp;lt;100&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;888&lt;/th&gt;
      &lt;td&gt;&amp;lt;100&lt;/td&gt;
      &lt;td&gt;&amp;lt;100&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;889&lt;/th&gt;
      &lt;td&gt;&amp;lt;100&lt;/td&gt;
      &lt;td&gt;&amp;lt;100&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;890&lt;/th&gt;
      &lt;td&gt;&amp;lt;10&lt;/td&gt;
      &lt;td&gt;&amp;lt;10&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;891 rows × 2 columns&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;We can check that results are identical, and now this can be easily applied to any continuos variable.
The lesson here is, I´m lazy I don´t want to things everytime, so I do it well once and then I recycle the functions.&lt;/p&gt;

&lt;p&gt;Now the &lt;code&gt;plot_rate_by()&lt;/code&gt; function can be used:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_rate_by(df, by = &#39;binning_function&#39;, Target = &#39;Survived&#39;, TN = tn, title = &#39;Survivors by Fare Categories&#39;, x = -0.3, y = 42, x_label = None)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;output_32_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Hope you like this. And can be useful to understand that being lazy is good if this makes you a recycler. Not sure if this sounds OK but anyways.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Calculating Recency using dplyr</title>
      <link>/publication/problem-9/</link>
      <pubDate>Tue, 04 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/publication/problem-9/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/pagedtable/css/pagedtable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/pagedtable/js/pagedtable.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;the-problem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Problem&lt;/h2&gt;
&lt;p&gt;We have an Intern working on his Thesis Project in our office. He needed to calculate Customer recency, meaning he needed to know the amount of months since the last time the Customer made a Purchase. This was quite intriguing to me because it needs to combine some windows scoped functions with group by and some other things.&lt;/p&gt;
&lt;p&gt;This is the problem with the expected solution:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(data &amp;lt;- tibble::tribble(
  ~Client_ID, ~Date_ID, ~Purchase_Amount, ~Recency,
           1,        1,             2344,        0,
           1,        2,                0,        1,
           1,        3,                0,        2,
           1,        4,             5676,        0,
           1,        5,             4587,        0,
           1,        6,                0,        1,
           1,        7,                0,        2,
           1,        8,                0,        3,
           2,        1,             2500,        0,
           2,        2,             2634,        0,
           2,        3,                0,        1,
           2,        4,                0,        2,
           2,        5,                0,        3,
           2,        6,             4578,        0,
           2,        7,             4562,        0,
           2,        8,                0,        1
  )
)&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
&lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;Client_ID&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;Date_ID&#34;],&#34;name&#34;:[2],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;Purchase_Amount&#34;],&#34;name&#34;:[3],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;Recency&#34;],&#34;name&#34;:[4],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]}],&#34;data&#34;:[{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;1&#34;,&#34;3&#34;:&#34;2344&#34;,&#34;4&#34;:&#34;0&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;2&#34;,&#34;3&#34;:&#34;0&#34;,&#34;4&#34;:&#34;1&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;3&#34;,&#34;3&#34;:&#34;0&#34;,&#34;4&#34;:&#34;2&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;4&#34;,&#34;3&#34;:&#34;5676&#34;,&#34;4&#34;:&#34;0&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;5&#34;,&#34;3&#34;:&#34;4587&#34;,&#34;4&#34;:&#34;0&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;6&#34;,&#34;3&#34;:&#34;0&#34;,&#34;4&#34;:&#34;1&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;7&#34;,&#34;3&#34;:&#34;0&#34;,&#34;4&#34;:&#34;2&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;8&#34;,&#34;3&#34;:&#34;0&#34;,&#34;4&#34;:&#34;3&#34;},{&#34;1&#34;:&#34;2&#34;,&#34;2&#34;:&#34;1&#34;,&#34;3&#34;:&#34;2500&#34;,&#34;4&#34;:&#34;0&#34;},{&#34;1&#34;:&#34;2&#34;,&#34;2&#34;:&#34;2&#34;,&#34;3&#34;:&#34;2634&#34;,&#34;4&#34;:&#34;0&#34;},{&#34;1&#34;:&#34;2&#34;,&#34;2&#34;:&#34;3&#34;,&#34;3&#34;:&#34;0&#34;,&#34;4&#34;:&#34;1&#34;},{&#34;1&#34;:&#34;2&#34;,&#34;2&#34;:&#34;4&#34;,&#34;3&#34;:&#34;0&#34;,&#34;4&#34;:&#34;2&#34;},{&#34;1&#34;:&#34;2&#34;,&#34;2&#34;:&#34;5&#34;,&#34;3&#34;:&#34;0&#34;,&#34;4&#34;:&#34;3&#34;},{&#34;1&#34;:&#34;2&#34;,&#34;2&#34;:&#34;6&#34;,&#34;3&#34;:&#34;4578&#34;,&#34;4&#34;:&#34;0&#34;},{&#34;1&#34;:&#34;2&#34;,&#34;2&#34;:&#34;7&#34;,&#34;3&#34;:&#34;4562&#34;,&#34;4&#34;:&#34;0&#34;},{&#34;1&#34;:&#34;2&#34;,&#34;2&#34;:&#34;8&#34;,&#34;3&#34;:&#34;0&#34;,&#34;4&#34;:&#34;1&#34;}],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;p&gt;As you may see every time I have a Purchase the counter needs to restart at 0 and then start counting how many dates have passed since the last purchase. Aditionally the counter needs to restart for new Customers.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-solution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The solution&lt;/h2&gt;
&lt;p&gt;In this particular case I will detail the different steps of the solution because it can be tricky to get.&lt;/p&gt;
&lt;p&gt;First I will create an auxiliary variable called has_purchased and a date_group. These variables need to be created at the client level, in order to make this easier I will use Client_ID 1 for demonstration purposes:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data %&amp;gt;%
  filter(Client_ID == 1) %&amp;gt;% 
  mutate(has_purchased = as.numeric(Purchase_Amount &amp;gt; 0),
         date_group = cumsum(has_purchased))&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
&lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;Client_ID&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;Date_ID&#34;],&#34;name&#34;:[2],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;Purchase_Amount&#34;],&#34;name&#34;:[3],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;Recency&#34;],&#34;name&#34;:[4],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;has_purchased&#34;],&#34;name&#34;:[5],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;date_group&#34;],&#34;name&#34;:[6],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]}],&#34;data&#34;:[{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;1&#34;,&#34;3&#34;:&#34;2344&#34;,&#34;4&#34;:&#34;0&#34;,&#34;5&#34;:&#34;1&#34;,&#34;6&#34;:&#34;1&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;2&#34;,&#34;3&#34;:&#34;0&#34;,&#34;4&#34;:&#34;1&#34;,&#34;5&#34;:&#34;0&#34;,&#34;6&#34;:&#34;1&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;3&#34;,&#34;3&#34;:&#34;0&#34;,&#34;4&#34;:&#34;2&#34;,&#34;5&#34;:&#34;0&#34;,&#34;6&#34;:&#34;1&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;4&#34;,&#34;3&#34;:&#34;5676&#34;,&#34;4&#34;:&#34;0&#34;,&#34;5&#34;:&#34;1&#34;,&#34;6&#34;:&#34;2&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;5&#34;,&#34;3&#34;:&#34;4587&#34;,&#34;4&#34;:&#34;0&#34;,&#34;5&#34;:&#34;1&#34;,&#34;6&#34;:&#34;3&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;6&#34;,&#34;3&#34;:&#34;0&#34;,&#34;4&#34;:&#34;1&#34;,&#34;5&#34;:&#34;0&#34;,&#34;6&#34;:&#34;3&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;7&#34;,&#34;3&#34;:&#34;0&#34;,&#34;4&#34;:&#34;2&#34;,&#34;5&#34;:&#34;0&#34;,&#34;6&#34;:&#34;3&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;8&#34;,&#34;3&#34;:&#34;0&#34;,&#34;4&#34;:&#34;3&#34;,&#34;5&#34;:&#34;0&#34;,&#34;6&#34;:&#34;3&#34;}],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;p&gt;Noticed that using the cumsum() function we can create groups in which the recency needs to restart. Everytime we change the date_group recency needs to come back to 0.&lt;/p&gt;
&lt;p&gt;Then we can calculate the row_number by group and substract 1 and that’s it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data %&amp;gt;%
  filter(Client_ID == 1) %&amp;gt;%
  mutate(
    has_purchased = as.numeric(Purchase_Amount &amp;gt; 0),
    date_group = cumsum(has_purchased)
  ) %&amp;gt;%
  group_by(date_group) %&amp;gt;%
  mutate(calculated_recency = row_number() - 1)&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
&lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;Client_ID&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;Date_ID&#34;],&#34;name&#34;:[2],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;Purchase_Amount&#34;],&#34;name&#34;:[3],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;Recency&#34;],&#34;name&#34;:[4],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;has_purchased&#34;],&#34;name&#34;:[5],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;date_group&#34;],&#34;name&#34;:[6],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;calculated_recency&#34;],&#34;name&#34;:[7],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]}],&#34;data&#34;:[{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;1&#34;,&#34;3&#34;:&#34;2344&#34;,&#34;4&#34;:&#34;0&#34;,&#34;5&#34;:&#34;1&#34;,&#34;6&#34;:&#34;1&#34;,&#34;7&#34;:&#34;0&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;2&#34;,&#34;3&#34;:&#34;0&#34;,&#34;4&#34;:&#34;1&#34;,&#34;5&#34;:&#34;0&#34;,&#34;6&#34;:&#34;1&#34;,&#34;7&#34;:&#34;1&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;3&#34;,&#34;3&#34;:&#34;0&#34;,&#34;4&#34;:&#34;2&#34;,&#34;5&#34;:&#34;0&#34;,&#34;6&#34;:&#34;1&#34;,&#34;7&#34;:&#34;2&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;4&#34;,&#34;3&#34;:&#34;5676&#34;,&#34;4&#34;:&#34;0&#34;,&#34;5&#34;:&#34;1&#34;,&#34;6&#34;:&#34;2&#34;,&#34;7&#34;:&#34;0&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;5&#34;,&#34;3&#34;:&#34;4587&#34;,&#34;4&#34;:&#34;0&#34;,&#34;5&#34;:&#34;1&#34;,&#34;6&#34;:&#34;3&#34;,&#34;7&#34;:&#34;0&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;6&#34;,&#34;3&#34;:&#34;0&#34;,&#34;4&#34;:&#34;1&#34;,&#34;5&#34;:&#34;0&#34;,&#34;6&#34;:&#34;3&#34;,&#34;7&#34;:&#34;1&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;7&#34;,&#34;3&#34;:&#34;0&#34;,&#34;4&#34;:&#34;2&#34;,&#34;5&#34;:&#34;0&#34;,&#34;6&#34;:&#34;3&#34;,&#34;7&#34;:&#34;2&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;8&#34;,&#34;3&#34;:&#34;0&#34;,&#34;4&#34;:&#34;3&#34;,&#34;5&#34;:&#34;0&#34;,&#34;6&#34;:&#34;3&#34;,&#34;7&#34;:&#34;3&#34;}],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;p&gt;Now, in order to make this computationally efficient and generalize this solution to all of the customers we need to apply this by Client_ID. The way in which we’ll do this is by using the group_modify() function.
This function works very similarly to purrr’s maps but applied to grouped data. The final solution looks like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data %&amp;gt;%
  #grouped by client
  group_by(Client_ID) %&amp;gt;%
  group_modify(
    #This is the same pipeline showed before but applied to element .x that represents each group
    ~ .x %&amp;gt;%
      mutate(
        has_purchased = as.numeric(Purchase_Amount &amp;gt; 0),
        date_group = cumsum(has_purchased)
      ) %&amp;gt;%
      group_by(date_group) %&amp;gt;%
      mutate(calculated_recency = row_number() - 1)
    
  ) %&amp;gt;%
   select(-has_purchased, -date_group)&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
&lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;Client_ID&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;Date_ID&#34;],&#34;name&#34;:[2],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;Purchase_Amount&#34;],&#34;name&#34;:[3],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;Recency&#34;],&#34;name&#34;:[4],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;calculated_recency&#34;],&#34;name&#34;:[5],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]}],&#34;data&#34;:[{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;1&#34;,&#34;3&#34;:&#34;2344&#34;,&#34;4&#34;:&#34;0&#34;,&#34;5&#34;:&#34;0&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;2&#34;,&#34;3&#34;:&#34;0&#34;,&#34;4&#34;:&#34;1&#34;,&#34;5&#34;:&#34;1&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;3&#34;,&#34;3&#34;:&#34;0&#34;,&#34;4&#34;:&#34;2&#34;,&#34;5&#34;:&#34;2&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;4&#34;,&#34;3&#34;:&#34;5676&#34;,&#34;4&#34;:&#34;0&#34;,&#34;5&#34;:&#34;0&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;5&#34;,&#34;3&#34;:&#34;4587&#34;,&#34;4&#34;:&#34;0&#34;,&#34;5&#34;:&#34;0&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;6&#34;,&#34;3&#34;:&#34;0&#34;,&#34;4&#34;:&#34;1&#34;,&#34;5&#34;:&#34;1&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;7&#34;,&#34;3&#34;:&#34;0&#34;,&#34;4&#34;:&#34;2&#34;,&#34;5&#34;:&#34;2&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;8&#34;,&#34;3&#34;:&#34;0&#34;,&#34;4&#34;:&#34;3&#34;,&#34;5&#34;:&#34;3&#34;},{&#34;1&#34;:&#34;2&#34;,&#34;2&#34;:&#34;1&#34;,&#34;3&#34;:&#34;2500&#34;,&#34;4&#34;:&#34;0&#34;,&#34;5&#34;:&#34;0&#34;},{&#34;1&#34;:&#34;2&#34;,&#34;2&#34;:&#34;2&#34;,&#34;3&#34;:&#34;2634&#34;,&#34;4&#34;:&#34;0&#34;,&#34;5&#34;:&#34;0&#34;},{&#34;1&#34;:&#34;2&#34;,&#34;2&#34;:&#34;3&#34;,&#34;3&#34;:&#34;0&#34;,&#34;4&#34;:&#34;1&#34;,&#34;5&#34;:&#34;1&#34;},{&#34;1&#34;:&#34;2&#34;,&#34;2&#34;:&#34;4&#34;,&#34;3&#34;:&#34;0&#34;,&#34;4&#34;:&#34;2&#34;,&#34;5&#34;:&#34;2&#34;},{&#34;1&#34;:&#34;2&#34;,&#34;2&#34;:&#34;5&#34;,&#34;3&#34;:&#34;0&#34;,&#34;4&#34;:&#34;3&#34;,&#34;5&#34;:&#34;3&#34;},{&#34;1&#34;:&#34;2&#34;,&#34;2&#34;:&#34;6&#34;,&#34;3&#34;:&#34;4578&#34;,&#34;4&#34;:&#34;0&#34;,&#34;5&#34;:&#34;0&#34;},{&#34;1&#34;:&#34;2&#34;,&#34;2&#34;:&#34;7&#34;,&#34;3&#34;:&#34;4562&#34;,&#34;4&#34;:&#34;0&#34;,&#34;5&#34;:&#34;0&#34;},{&#34;1&#34;:&#34;2&#34;,&#34;2&#34;:&#34;8&#34;,&#34;3&#34;:&#34;0&#34;,&#34;4&#34;:&#34;1&#34;,&#34;5&#34;:&#34;1&#34;}],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;p&gt;You’ll notice calculated_recency matches with the expected results proving our solution works and a complicated calculation can be easily done using some group_by statements.&lt;/p&gt;
&lt;p&gt;I think the beauty of this solution is that we only used vectorized functions without applying any loop to run through the data by Client and by date_group which normally would take 2 nested for loops.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>R vs Python, Part II</title>
      <link>/post/r-vs-python-ii/</link>
      <pubDate>Mon, 03 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/r-vs-python-ii/</guid>
      <description>


&lt;p&gt;During last blog post I shared my bad experiences with R in these late days. This doesn’t mean I hate R at all, but it does mean that I’m eagerly learning Python to supply some of the R deficencies.&lt;/p&gt;
&lt;p&gt;In this 3 weeks of intensive Python learning I’ve learned a lot and here are some things on why moving to Python.&lt;/p&gt;
&lt;p&gt;The main advantage of the Python Data science environment is that you can find everything condesnsed into the Scipy stack. With just 5 packages you can perform most of the data scientist tasks out there.&lt;/p&gt;
&lt;div id=&#34;scikit---learn&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Scikit - Learn&lt;/h1&gt;
&lt;p&gt;Remember that I had my favorite package in R? Well I have to say this is my favorite in Python. I have never worked before with such a complete package. It is so well organized and documented. I have to say that just by reading its documentation and the API I learned tons of Machine Learning.&lt;/p&gt;
&lt;p&gt;It is true that R is quite biased towards statistics. This bias is quite noticeable when you discover a lot of new techniques that I never found in any other R package. Sklearn includes everything: Data spliting functions, tons of models or estimators (using sklearn terminology), a lot of validation strategies, Grid Search strategies, metrics, pre-processing, calibration helpers, unsupervised algorithms, ensembles algorithms and so many complementary packages that makes this framework the best one among all of the Machine Learning languages.&lt;/p&gt;
&lt;p&gt;I think one advantage that Python has in here is that the creators of this package took this so seriously, they even have acceptation criteria to add new models into the framework that makes this package incredible stable and the state of art when it comes to modeling. Compared to R that has all of the models segregated into many different packages, plus, not having an stable unified interface and not having a lot of support for unsupervised algorithms and ensembles, I have to say Python here is way superior.&lt;/p&gt;
&lt;p&gt;I have to say that in this kind of problems the object oriented programming shines and makes Python really delightful. Cool things I have noticed:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The parallel interface using n_jobs combined with joblib is so smooth and well implemented.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The memory management of Python makes it so suitable for Production and to run large models in limited memory environments.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The creators are really focused in the package scope and they are not trying to aim to everything out there, but I would say 90% of the funtions implemented in sklearn work excellent.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The package is implemented on top of Numpy that I think is the most powerful package in Data Science because of its performance and ease to be used.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The documentation is just beautiful and easy to get access.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Tons of tutorials and examples to get up and running with your model super fast.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Ready to run on top of a cluster and be combine with Spark to be scaled up.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;does-this-thing-have-any-cons&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Does this thing have any cons?&lt;/h3&gt;
&lt;p&gt;Of course, even though it is easy to learn, at the beginning you run into errors all the time. And I would like to say it is because you need to get used to the API.&lt;/p&gt;
&lt;p&gt;Some cons that I have found so far:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;It is quite documentation dependant. You need to work with sklearn.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Since Python it is not a community of statisticians a lot of people makes opinions about how they use some of the sklearn features that some times you can get confused and you just don’t know who to trust to.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Even though Pipelines are a very powerful tool to create models, I still prefer the recipes API, it is way cleaner and easy to learn. The pipeline funtions tends to be messy when combining list and tuples with the normal function parenthesis.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Other than that, I think sklearn is perfect.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;matplotlib&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Matplotlib&lt;/h1&gt;
&lt;p&gt;I have to say I was very reluctant to use this library, because ggplot is powerful and easy to use. But once you get into this library you notice that is as easy to use as ggplot, but the graphics quality and the color palette are aesthetically superior. Something that I never liked about ggplot is the collor palette and the resolution, something is way resolved in matpotlib, and in my opinion when combined with pandas plotting becomes way easier.&lt;/p&gt;
&lt;div id=&#34;charts-comparison&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Charts comparison&lt;/h3&gt;
&lt;p&gt;TODO&lt;/p&gt;
&lt;p&gt;Besides that, I like that matplotlib, pandas and numpy are fully integrated, so you don’t have compatibility issues, something that you do have when trying to combine matrices and ggplot in R for example.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Not too much more to say about this but, pretty, simple and full compatible.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;pandas&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;pandas&lt;/h1&gt;
&lt;p&gt;For me this is the weakest link in the scipy stack. There is no any doubt about pandas capability and performance, but I feel (this is a personal opinion) that pandas doesn’t have an own identity. Pandas is always trying to emulate what dplyr does, and sometimes it is superior (except for the syntax) and even it has some properties that makes it unique like dealing with time series, the usage of index and the ability to transpose any dataframe, I accept it, I respet it, I use it but I don’t love it (yet).&lt;/p&gt;
&lt;p&gt;In my opinion the biggest weakness is the excessive use of the apostrophe (’). I have to say I really miss R’s non-standard evaluation, and an equivalent for the filter function. I just don’t like to use boolean masks within square brackets to filter out some data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;numpy&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;numpy&lt;/h1&gt;
&lt;p&gt;I think is the responsible of Python’s popularity. It can be that you don’t use it all the time. Actually I’m still learning how to use it. But its speed is abosolutely uncomparable, and all of the scipy stack is built on top of this library.&lt;/p&gt;
&lt;p&gt;I have to say my first Python’s impression were not good at all. I remember that we had a trainee implementing an algorithm in Python and we made all of the worst practice mistakes, making Python ridiculously bad performant. But after using the combination of all of these packages in the late weeks has been a pleasure.&lt;/p&gt;
&lt;p&gt;I won’t refer to scipy or seaborn that are some other popular packages that are complementary to these ones, basically because I haven’t used them.&lt;/p&gt;
&lt;p&gt;Since I’m using Python more regularly I will start uploading some use cases and issues I have been encountering the same way I do with R.&lt;/p&gt;
&lt;p&gt;More to come, in the next weeks!!&lt;/p&gt;
&lt;p&gt;See ya.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>R vs Python</title>
      <link>/post/r-vs-python/</link>
      <pubDate>Wed, 29 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/r-vs-python/</guid>
      <description>


&lt;div id=&#34;catching-up-about-me&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Catching up about me&lt;/h1&gt;
&lt;p&gt;I’ve been working in my new job for the last two months, I’ve had some pros and some cons but that will be another day’s discussion. The reason I’m writing about this is because in my new role I have to create a lot of ML models and sadly, and I really mean sadly, R is not suited for that. And it seems Python is.&lt;/p&gt;
&lt;p&gt;This is something that breaks my heart, and makes me wonder what tool is better suited for data science, R or Python? And I want to add my two cents based on my experience and trying to be as objective as possible.&lt;/p&gt;
&lt;p&gt;Aditionally, I run into this updated DataCamp &lt;a href=&#34;https://www.datacamp.com/community/tutorials/r-or-python-for-data-analysis&#34;&gt;Infographic&lt;/a&gt; that I think is one of the most accurate and less biased comparisons I have found so far.&lt;/p&gt;
&lt;div id=&#34;recap-my-background-and-how-i-use-to-approach-r-vs-python&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Recap, my background and How I use to approach R vs Python&lt;/h2&gt;
&lt;div id=&#34;disclaimer&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Disclaimer&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;I’ve been coding with R by 5 five years now, and heavy coding with Python the last 3 weeks.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If you know me (probably not) I’m an R fan, I’ve been programming with R since 2015 I’ve done everything: Reports, ETLs, Data cleaning, Shiny Apps, Machine Learning and Deep Learning (My Thesis) and a long etc.&lt;/p&gt;
&lt;p&gt;And I have to say I’ve been really reluctant to learn Python, I just couldn’t understand why people prefer it and why they consider it to be superior to R. I really get upset when I notice that Google releases some really cool APIs for something that I really want and natively is there for Python and I have to wait for some genius R hacker to create the API for R.&lt;/p&gt;
&lt;p&gt;2 weeks ago I attended to a Microsoft conference and I was in a data science Talk when the keynote asked who in the audience prefered Python, and 95% of the room raised their hand. My boss stared at me saying: “Ouch”. That hurted, but at the same time opened my eyes.&lt;/p&gt;
&lt;p&gt;Normally when I watch R vs Python things I notice a lot of Bias towards Python. I can’t find a founded reason of why you should go for Python instead of R.&lt;/p&gt;
&lt;p&gt;When you check these R vs Python entries the reason a lot of people allege about R not being a good option is because &lt;em&gt;“… it is not suited for Production”&lt;/em&gt;. But what Production means?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Actually Production could be whatever form your data Product could be ready to be consumed, it could be a flat file, a Database, an online dashboard, a PDF report, an API…whatever your want.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Under that definition, R has a lot of different options to meet those needs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When it comes to exporting to files
&lt;ul&gt;
&lt;li&gt;{readr} can export to any flat file out there.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;{arrow} can export to feather and parquet.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;{haven} can export to SAS, SPSS files.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;{jsonlite} can export to JSON.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;a long etc.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If it is about DBs, you can definitely need to check out the {DBI}, {odbc}, {dbplyr} combo. WIth those three packages you can connect to almost any DB type.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;An online dashboard, well you have {Shiny} and a really &lt;a href=&#34;https://github.com/nanxstats/awesome-shiny-extensions&#34;&gt;long list&lt;/a&gt; of extensions to create really professional dashboards using the most cutting edges web frameworks such as Bootstrap, Bulma, AdminLTE, Semantic-UI, etc.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A PDF report, you can use the whole –down stack of packages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Reports {Rmarkdown},&lt;/li&gt;
&lt;li&gt;write your own book {bookdown},&lt;/li&gt;
&lt;li&gt;your website (suck as this) {blogdown},&lt;/li&gt;
&lt;li&gt;posters {posterdown} and {pagedown},&lt;/li&gt;
&lt;li&gt;PDFs, Words, Powerpoints {Rmarkdown}, {xaringan}, {pagedown},&lt;/li&gt;
&lt;li&gt;Scientific articles {distill}, {Rmarkdown},&lt;/li&gt;
&lt;li&gt;A really long etc. again.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;An API, well {plumber} is definitely the easiest way to make your own API.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;When someone says R is not production ready, I would say I’m &lt;strong&gt;NOT SURE&lt;/strong&gt;!!!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;R has all the capabilities to be suited for Production&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Another common issue is that R is slow. Of course it is slow if you use for loops with no previously defined length.&lt;/p&gt;
&lt;p&gt;I think there is enough evidence to show that R is not slow. You can check:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;H2o benchmarks &lt;a href=&#34;https://github.com/Rdatatable/data.table/wiki/Benchmarks-:-Grouping&#34;&gt;here&lt;/a&gt; showing speed for data manipulation.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You can check some Deep Learning frameworks comparisons &lt;a href=&#34;https://deepsense.ai/keras-or-pytorch/&#34;&gt;here&lt;/a&gt; and you will see that R can achieve pretty good times. (Take into account that R has no native DL framework and even with the translation to Python overhead can beat some well established Frameworks in &lt;strong&gt;some&lt;/strong&gt; tasks, not all of them)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;So again, speed is not an issue, if you want full speed of course you’ll want to go to C++, Scala or Java.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Another thing is the syntax. This is something that really gets me upset. Because comparisons are just not fair.&lt;/p&gt;
&lt;p&gt;Python prides itself to have &lt;em&gt;“Among its most important characteristics the use of elegant syntax, which allows the users to read program code easily”&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Sorry guys but this:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df.loc[(df[&amp;#39;var_1&amp;#39;]&amp;gt;3) &amp;amp; df[&amp;#39;var_2&amp;#39;]&amp;lt;5,[&amp;#39;var_3&amp;#39;,&amp;#39;var_4&amp;#39;]].apply(lambda x: (x+3)**2, axis=0)&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Does not read as plain english and it is not elegant.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Of course, there are ways to write that code in a more readable way, but that code is quite compliant with Python Standards and I just can read it at first sight.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df %&amp;gt;%
  filter(var_1 &amp;gt; 3 &amp;amp; var_2 &amp;lt; 5) %&amp;gt;%
  select(var_3, var_4) %&amp;gt;%
  map_df(~ (.x + 3)^2)&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;Sorry but, this is plain english and elegant.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Normally websites says pandas can do something really powerful like sort values with:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df.sort_values(&amp;#39;var_1&amp;#39;, ascending = False)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which is actually pretty powerful but they compare it to:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df[-order(df[&amp;#39;var_1&amp;#39;]),]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which is unreadable, and nobody uses, and if you do, please &lt;strong&gt;STOP doing it&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Tidyverse allows to do this just by:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df %&amp;gt;%
  arrange(var_1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A fair comparison between Python and R needs to incorporate their main packages. The problem here is that Python have everything concentrated into the Scipy stack (Numpy, pandas, Scipy, matplotlib and Scikit-Learn). With those 5 packages you can do almost anything related to Data Science in Python.&lt;/p&gt;
&lt;p&gt;In R, just by using the Tidyverse you have around 20 to 30 packages and you have a lot of small specialized packages to improve productivity.&lt;/p&gt;
&lt;p&gt;But all of these things &lt;strong&gt;do not cover the MAIN reason of why people prefer Python over R&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Another popular reason described in these comparisons is that R is more suited for Statisticians while Python is more suited for programmers. This is &lt;strong&gt;partially&lt;/strong&gt; True. I would say that Python looks more familiar for people with a Computer Science background, while R is more friendly for people that have never programmed in their life (During Latin R I was gladly surprised to notice that a lot of R programmers were not related at all with Data Science but with other fields that leverage data).&lt;/p&gt;
&lt;p&gt;That explains a lot about things that R is being discriminated for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;R has a messy syntax: Not necessarily, But a lot of people with no previous coding experience use R and they don´t care about following best practices, they want to get things done, no matter if you use base R, or tidyverse, or pipe, or data.table, all in the same script and alternating with no prior notice.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;R is slow: Again since most of the people don´t have coding experience they are not worried about imporving code performance.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;R has too many packages: This is true, and you can get lost here. But if you need to deal with data, normally you should go for the tidyverse, and following tidy principals all of the problems described above &lt;strong&gt;should&lt;/strong&gt; be solved.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;why-i-prefer-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Why I prefer R&lt;/h1&gt;
&lt;p&gt;This is really personal, and the main reason is the tidyverse and the pipe friendly syntax. When I code I put a lot of effort to be able to understand it at first sight. Code readability is for me the most important thing.&lt;/p&gt;
&lt;p&gt;Then is code performance, if the code is not performant I modify syntax slightly to improve performance not affecting readability.
{
Then is productivity and complementary packages that I normally I use a lot, packages like {mufflr}, {glue} or {remedy} to make coding easier and more fun are crucial for me.&lt;/p&gt;
&lt;p&gt;Finally my favorite packages, and with this I refer to packages that have no comparison:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;{dplyr} (and {tidyr}) have absolutely no comparison. It is just the most beautiful syntax to deal with data, the function names, the functionality (specially things like mutate and scope variants like *_at, *_if and *_all) are just the best thing to work with.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;{ggplot2} Even though I´m not a chart fan, again the syntax and the ease to make really complete charts (I don´t want to say beautiful because I really hate ggplot’s default color palette) is priceless.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;{recipes} and the tidymodels API (not the documentation, that I have to say is quite messy sometimes). When recipes was released I just didn’t get it and I was so confused. Once I understood how it worked my life changed. There is no easier way to apply preprocessing steps like recipes. And this became my favorite package. The when the rest of packages started to be released I fell in love with tidymodels. I use to use caret and the package was so huge that I usually got lost. With this new framework everything was so organized that I really enjoyed creating Machine Learning workflows.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These 3 packages, namely dplyr, ggplot and recipes, have no comparison in my humble opinion.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-break-up&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The break up&lt;/h1&gt;
&lt;p&gt;When you have your favorite packages and it fails, it just break your heart, and that happened with {recipes}.&lt;/p&gt;
&lt;p&gt;I built a simple Random Forest model with around 1M of rows, and the prepper object was 40GB. The {ranger} model object was 5GB and after the resample I got this lovely message:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;“Vector size X GB cannot be allocated”&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I moved to a cloud server, having 240GB of RAM and I used {furrr} to parallelize my code when R just stopped working running out of memory several times leaving incomplete processes (as many as threads could be run) running in the Task Manager, blocking memory to be used for some others processes.&lt;/p&gt;
&lt;p&gt;After carefully investigate what was happening I noticed that everytime I ran something less RAM was available, and the dissapointment arrived. Once the model finished I was expecting Memory to be released but that never happened.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;After doing some research I found the right term: &lt;strong&gt;Memory Leakage&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Memory leakage refers when the OS is not releasing Memory once a process allocating this memory finishes. Plus, I didn´t understand why having 50GB objects used up my whole memory. The only way to free up this memory was restarting R (Ctrl + Shift + F10)&lt;/p&gt;
&lt;p&gt;And here is the real reason why R is not a top choice for data Science, and for some reason this is something that nobody mentions but it is slightly touched in the DataCamp Infographic. R has a poor memory management, and Hadley knows it. He mentions this Memory Leakage issues in &lt;a href=&#34;http://adv-r.had.co.nz/memory.html&#34;&gt;Advance R first edition&lt;/a&gt; (for some reason it is not detailed in the 2nd Edition).&lt;/p&gt;
&lt;p&gt;Real Data Science, and not just the small examples we use to demonstrate the power and usage of a package, depends a lot on memory usage, and you cannot mount a production system knowing that R will use up your full memory available and you cannot release it.&lt;/p&gt;
&lt;p&gt;Probably this is something R developers are also facing and Hadley and the Rstudio guys know, but it is something intrinsic from R.&lt;/p&gt;
&lt;p&gt;Plus, R is not natively installed in a hosting service or a Linux server as Python is, and has not parallelism “almost natively” implemented. Making Python an easier choice to go.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Ok, here we explained why R is not a top choice option but we haven’t talked anything about Python…&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Why Python? This will be covered in a next Post.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>I got a new job</title>
      <link>/post/new-job/</link>
      <pubDate>Wed, 06 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/new-job/</guid>
      <description>


&lt;div id=&#34;my-new-job&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;My new Job&lt;/h2&gt;
&lt;p&gt;My working relationship with Evalueserve ended the past June and I just dedicated to finish my Thesis. On the first days of October I just started to apply to different Jobs, I have to say my first option was trying to get a Job at H2o, but I´m not there yet.&lt;/p&gt;
&lt;p&gt;The thing is that after the first Interview I knew this would be my new Company, I felt some kind of feeling saying “&lt;em&gt;you will work here&lt;/em&gt;” (although I had some hesitation). So I will be a &lt;strong&gt;Senior Data Scientist&lt;/strong&gt; working at Scotiabank Cencosud, a Retail Company, and I´ll be part of the Advanced Analytics Team.&lt;/p&gt;
&lt;p&gt;It seems there is a lot of interesting opportunities for my carreer. So let´s break them down:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This is a real Modeling Position.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I will be primarily involved in developing Machine Learning in Production. This is something super exciting because I think I have learned and practiced a lot to be involved into this. I feel super prepared and eager to start applying the knowledge learned in the POCs I´ve worked in EVS and in the ML Diploma.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We´ll have a real Data Lake with plenty of Data&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is important, I don´t remember where I read about the things I needed to pay attention when applying to a new Job, and of course if we are doing Data Science we need Data. Well Cencosud is a Company that is positioning itself as an IDO (Insight Driven organization) and its primary focus is to make Data Driven Decisions.&lt;/p&gt;
&lt;p&gt;I think, I will have the chance to work with large amounts of Data and hopefully use Spark, this is something I really want to incorporate into my skill set.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;There´s space for Innovative Research&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;At least in my conversations with the High Management during Interviews they guaranteed some space to innovate and experiment. This is something really fun to me and that I was looking for. So I expect to have the opportunity to implement new models, or algorithms that i haven´t work with before.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I will use R&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This was a main thing to me. I applied to several position where R was not an option, but I just couldn´t stand it. I want to work with R, and here I will have the opportunity to use it from end to end. I even twitted about it 😜 and I got one 😁.&lt;/p&gt;
&lt;center&gt;
{{&amp;lt; tweet 1183107543113093120 &amp;gt;}}
&lt;/center&gt;
&lt;p&gt;Many thanks to all the people that provided assitance and help to find job opportunities.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I´m well paid and with plenty of Benefits&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Of course I will not disclose my salary (to avoid jealousy ), but I feel happy with the Offering. This was really an issue in my previous job, so I expect good things to happen in here.&lt;/p&gt;
&lt;p&gt;More to come after the first month, I think, but at least for now. &lt;strong&gt;I´m happy&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Dealing with dates</title>
      <link>/publication/problem-7/</link>
      <pubDate>Wed, 30 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/publication/problem-7/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/pagedtable/css/pagedtable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/pagedtable/js/pagedtable.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;the-problem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Problem&lt;/h2&gt;
&lt;p&gt;Here is the challenge:&lt;/p&gt;
&lt;p&gt;Calculate the time difference between Max and Min Dates found in a date vector.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
date_vec &amp;lt;- c(&amp;quot;2019/10/24 10:00:00&amp;quot;,&amp;quot;2019/10/23 11:00:00&amp;quot;,&amp;quot;2019/10/25 12:00:00&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-solution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Solution&lt;/h2&gt;
&lt;p&gt;The thing is super easy to get, but the idea is to create a pipeline that can calculate this in just a series of steps:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lubridate)

date_vec %&amp;gt;%
  #Transforming characters into dates using ymd for dates and hms for time
  ymd_hms() %&amp;gt;%
  #range() retrieves max and min date
  range() %&amp;gt;%
  #Calculate the time difference
  diff() %&amp;gt;%
  #Transform into lubridate duration object %&amp;gt;%
   as.duration() &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;176400s (~2.04 days)&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Unique Id Challenge</title>
      <link>/publication/problem-8/</link>
      <pubDate>Wed, 30 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/publication/problem-8/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/pagedtable/css/pagedtable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/pagedtable/js/pagedtable.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;the-problem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Problem&lt;/h2&gt;
&lt;p&gt;Another Twitter Challenge:&lt;/p&gt;
&lt;center&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;&lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; I&amp;#39;m sure there&amp;#39;s an elegant solution that I&amp;#39;m just totally missing. How do I create a unique episode_ID that increases by 1 for instances where episode_flag == &amp;quot;new&amp;quot; but just repeats the value from the row above when episode_flag == &amp;quot;same&amp;quot;? &lt;a href=&#34;https://t.co/Dl5ZtAiE7J&#34;&gt;pic.twitter.com/Dl5ZtAiE7J&lt;/a&gt;&lt;/p&gt;&amp;mdash; Jessica Streeter (@phillynerd) &lt;a href=&#34;https://twitter.com/phillynerd/status/1189641234639400961?ref_src=twsrc%5Etfw&#34;&gt;October 30, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;/center&gt;
&lt;/div&gt;
&lt;div id=&#34;the-solution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Solution&lt;/h2&gt;
&lt;p&gt;It is almost there, I just added a couple of lines to get the expected output elegantly:&lt;/p&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
&lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;member&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;appt&#34;],&#34;name&#34;:[2],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;episode_flag&#34;],&#34;name&#34;:[3],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]}],&#34;data&#34;:[{&#34;1&#34;:&#34;a&#34;,&#34;2&#34;:&#34;1&#34;,&#34;3&#34;:&#34;new&#34;},{&#34;1&#34;:&#34;a&#34;,&#34;2&#34;:&#34;2&#34;,&#34;3&#34;:&#34;same&#34;},{&#34;1&#34;:&#34;b&#34;,&#34;2&#34;:&#34;1&#34;,&#34;3&#34;:&#34;new&#34;},{&#34;1&#34;:&#34;b&#34;,&#34;2&#34;:&#34;2&#34;,&#34;3&#34;:&#34;same&#34;},{&#34;1&#34;:&#34;b&#34;,&#34;2&#34;:&#34;3&#34;,&#34;3&#34;:&#34;same&#34;},{&#34;1&#34;:&#34;b&#34;,&#34;2&#34;:&#34;1&#34;,&#34;3&#34;:&#34;new&#34;},{&#34;1&#34;:&#34;c&#34;,&#34;2&#34;:&#34;1&#34;,&#34;3&#34;:&#34;new&#34;}],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df %&amp;gt;%
  group_by(episode_flag) %&amp;gt;%
  mutate(episode_ID = ifelse(episode_flag ==&amp;quot;new&amp;quot;, row_number(), NA)) %&amp;gt;%
  # Eliminating groups to apply next function
  ungroup() %&amp;gt;%
  # Filling NAs with previous non-NA values
  fill(episode_ID)&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
&lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;member&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;appt&#34;],&#34;name&#34;:[2],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;episode_flag&#34;],&#34;name&#34;:[3],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;episode_ID&#34;],&#34;name&#34;:[4],&#34;type&#34;:[&#34;int&#34;],&#34;align&#34;:[&#34;right&#34;]}],&#34;data&#34;:[{&#34;1&#34;:&#34;a&#34;,&#34;2&#34;:&#34;1&#34;,&#34;3&#34;:&#34;new&#34;,&#34;4&#34;:&#34;1&#34;},{&#34;1&#34;:&#34;a&#34;,&#34;2&#34;:&#34;2&#34;,&#34;3&#34;:&#34;same&#34;,&#34;4&#34;:&#34;1&#34;},{&#34;1&#34;:&#34;b&#34;,&#34;2&#34;:&#34;1&#34;,&#34;3&#34;:&#34;new&#34;,&#34;4&#34;:&#34;2&#34;},{&#34;1&#34;:&#34;b&#34;,&#34;2&#34;:&#34;2&#34;,&#34;3&#34;:&#34;same&#34;,&#34;4&#34;:&#34;2&#34;},{&#34;1&#34;:&#34;b&#34;,&#34;2&#34;:&#34;3&#34;,&#34;3&#34;:&#34;same&#34;,&#34;4&#34;:&#34;2&#34;},{&#34;1&#34;:&#34;b&#34;,&#34;2&#34;:&#34;1&#34;,&#34;3&#34;:&#34;new&#34;,&#34;4&#34;:&#34;3&#34;},{&#34;1&#34;:&#34;c&#34;,&#34;2&#34;:&#34;1&#34;,&#34;3&#34;:&#34;new&#34;,&#34;4&#34;:&#34;4&#34;}],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Transposing a dataframe</title>
      <link>/publication/problem-6/</link>
      <pubDate>Thu, 24 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/publication/problem-6/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/pagedtable/css/pagedtable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/pagedtable/js/pagedtable.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;the-problem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Problem&lt;/h2&gt;
&lt;p&gt;Here is the challenge:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data &amp;lt;- data.frame(
  &amp;quot;id&amp;quot; = c(901, 902, 903, &amp;quot;age&amp;quot;, &amp;quot;gender&amp;quot;, &amp;quot;language&amp;quot;),
  &amp;quot;rater1&amp;quot; = c(7, 9, 9, 21, 1, 1),
  &amp;quot;rater2&amp;quot; = c(9, 9, 9, 39, 2, 2),
  &amp;quot;rater3&amp;quot; = c(9, 9, 9, 38, 2, 1),
  &amp;quot;rater4&amp;quot; = c(9, 9, 9, 33, 2, 1),
  &amp;quot;rater5&amp;quot; = c(2, 9, 9, 21, 2, 1)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Filter all the ratings with gender 1, or language 1, or gender 1 AND language 1.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-solution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Solution&lt;/h2&gt;
&lt;p&gt;The thing is super easy, we need to transpose, the thing transposition is not a valid operation when it comes to data frames, how can we apply this in a data frame using &lt;code&gt;tidyverse&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;So in order to understand what happens I will run the solution by parts.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data %&amp;gt;%
  transpose()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [[1]]$id
## [1] 1
## 
## [[1]]$rater1
## [1] 7
## 
## [[1]]$rater2
## [1] 9
## 
## [[1]]$rater3
## [1] 9
## 
## [[1]]$rater4
## [1] 9
## 
## [[1]]$rater5
## [1] 2
## 
## 
## [[2]]
## [[2]]$id
## [1] 2
## 
## [[2]]$rater1
## [1] 9
## 
## [[2]]$rater2
## [1] 9
## 
## [[2]]$rater3
## [1] 9
## 
## [[2]]$rater4
## [1] 9
## 
## [[2]]$rater5
## [1] 9
## 
## 
## [[3]]
## [[3]]$id
## [1] 3
## 
## [[3]]$rater1
## [1] 9
## 
## [[3]]$rater2
## [1] 9
## 
## [[3]]$rater3
## [1] 9
## 
## [[3]]$rater4
## [1] 9
## 
## [[3]]$rater5
## [1] 9
## 
## 
## [[4]]
## [[4]]$id
## [1] 4
## 
## [[4]]$rater1
## [1] 21
## 
## [[4]]$rater2
## [1] 39
## 
## [[4]]$rater3
## [1] 38
## 
## [[4]]$rater4
## [1] 33
## 
## [[4]]$rater5
## [1] 21
## 
## 
## [[5]]
## [[5]]$id
## [1] 5
## 
## [[5]]$rater1
## [1] 1
## 
## [[5]]$rater2
## [1] 2
## 
## [[5]]$rater3
## [1] 2
## 
## [[5]]$rater4
## [1] 2
## 
## [[5]]$rater5
## [1] 2
## 
## 
## [[6]]
## [[6]]$id
## [1] 6
## 
## [[6]]$rater1
## [1] 1
## 
## [[6]]$rater2
## [1] 2
## 
## [[6]]$rater3
## [1] 1
## 
## [[6]]$rater4
## [1] 1
## 
## [[6]]$rater5
## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The problem using transpose is that the results is a list of lists, so it´s necessary to transform inner list into vectors:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data %&amp;gt;% 
  #select raters
  select(contains(&amp;quot;rater&amp;quot;)) %&amp;gt;%
  #transpose, the problem is that this transform data into lists of lists.
  transpose() %&amp;gt;%
  #unlisting into double vectors
  map(flatten_dbl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## rater1 rater2 rater3 rater4 rater5 
##      7      9      9      9      2 
## 
## [[2]]
## rater1 rater2 rater3 rater4 rater5 
##      9      9      9      9      9 
## 
## [[3]]
## rater1 rater2 rater3 rater4 rater5 
##      9      9      9      9      9 
## 
## [[4]]
## rater1 rater2 rater3 rater4 rater5 
##     21     39     38     33     21 
## 
## [[5]]
## rater1 rater2 rater3 rater4 rater5 
##      1      2      2      2      2 
## 
## [[6]]
## rater1 rater2 rater3 rater4 rater5 
##      1      2      1      1      1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now every list slot can be renamed with the corresponding id:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data %&amp;gt;% 
  #select raters
  select(contains(&amp;quot;rater&amp;quot;)) %&amp;gt;%
  #transpose, the problem is that this transform data into lists of lists.
  transpose() %&amp;gt;%
  #unlisting 
  map(flatten_dbl) %&amp;gt;%
  set_names(data$id)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $`901`
## rater1 rater2 rater3 rater4 rater5 
##      7      9      9      9      2 
## 
## $`902`
## rater1 rater2 rater3 rater4 rater5 
##      9      9      9      9      9 
## 
## $`903`
## rater1 rater2 rater3 rater4 rater5 
##      9      9      9      9      9 
## 
## $age
## rater1 rater2 rater3 rater4 rater5 
##     21     39     38     33     21 
## 
## $gender
## rater1 rater2 rater3 rater4 rater5 
##      1      2      2      2      2 
## 
## $language
## rater1 rater2 rater3 rater4 rater5 
##      1      2      1      1      1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally we can reorganize using map_dfc() function that reorder the data into dataframes by column:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(
  new_data &amp;lt;- data %&amp;gt;%
    #select raters
    select(contains(&amp;quot;rater&amp;quot;)) %&amp;gt;%
    #transpose, the problem is that this transform data into lists of lists.
    transpose() %&amp;gt;%
    #unlisting
    map(flatten_dbl) %&amp;gt;%
    set_names(data$id) %&amp;gt;%
    map_dfc( ~ .x)
)&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
&lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;901&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;902&#34;],&#34;name&#34;:[2],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;903&#34;],&#34;name&#34;:[3],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;age&#34;],&#34;name&#34;:[4],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;gender&#34;],&#34;name&#34;:[5],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;language&#34;],&#34;name&#34;:[6],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]}],&#34;data&#34;:[{&#34;1&#34;:&#34;7&#34;,&#34;2&#34;:&#34;9&#34;,&#34;3&#34;:&#34;9&#34;,&#34;4&#34;:&#34;21&#34;,&#34;5&#34;:&#34;1&#34;,&#34;6&#34;:&#34;1&#34;,&#34;_row&#34;:&#34;rater1&#34;},{&#34;1&#34;:&#34;9&#34;,&#34;2&#34;:&#34;9&#34;,&#34;3&#34;:&#34;9&#34;,&#34;4&#34;:&#34;39&#34;,&#34;5&#34;:&#34;2&#34;,&#34;6&#34;:&#34;2&#34;,&#34;_row&#34;:&#34;rater2&#34;},{&#34;1&#34;:&#34;9&#34;,&#34;2&#34;:&#34;9&#34;,&#34;3&#34;:&#34;9&#34;,&#34;4&#34;:&#34;38&#34;,&#34;5&#34;:&#34;2&#34;,&#34;6&#34;:&#34;1&#34;,&#34;_row&#34;:&#34;rater3&#34;},{&#34;1&#34;:&#34;9&#34;,&#34;2&#34;:&#34;9&#34;,&#34;3&#34;:&#34;9&#34;,&#34;4&#34;:&#34;33&#34;,&#34;5&#34;:&#34;2&#34;,&#34;6&#34;:&#34;1&#34;,&#34;_row&#34;:&#34;rater4&#34;},{&#34;1&#34;:&#34;2&#34;,&#34;2&#34;:&#34;9&#34;,&#34;3&#34;:&#34;9&#34;,&#34;4&#34;:&#34;21&#34;,&#34;5&#34;:&#34;2&#34;,&#34;6&#34;:&#34;1&#34;,&#34;_row&#34;:&#34;rater5&#34;}],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;p&gt;Now we can filter accordingly the requested filterings:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_data %&amp;gt;%
  filter(gender == 1)&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
&lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;901&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;902&#34;],&#34;name&#34;:[2],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;903&#34;],&#34;name&#34;:[3],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;age&#34;],&#34;name&#34;:[4],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;gender&#34;],&#34;name&#34;:[5],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;language&#34;],&#34;name&#34;:[6],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]}],&#34;data&#34;:[{&#34;1&#34;:&#34;7&#34;,&#34;2&#34;:&#34;9&#34;,&#34;3&#34;:&#34;9&#34;,&#34;4&#34;:&#34;21&#34;,&#34;5&#34;:&#34;1&#34;,&#34;6&#34;:&#34;1&#34;}],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_data %&amp;gt;%
  filter(language == 1)&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
&lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;901&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;902&#34;],&#34;name&#34;:[2],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;903&#34;],&#34;name&#34;:[3],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;age&#34;],&#34;name&#34;:[4],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;gender&#34;],&#34;name&#34;:[5],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;language&#34;],&#34;name&#34;:[6],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]}],&#34;data&#34;:[{&#34;1&#34;:&#34;7&#34;,&#34;2&#34;:&#34;9&#34;,&#34;3&#34;:&#34;9&#34;,&#34;4&#34;:&#34;21&#34;,&#34;5&#34;:&#34;1&#34;,&#34;6&#34;:&#34;1&#34;},{&#34;1&#34;:&#34;9&#34;,&#34;2&#34;:&#34;9&#34;,&#34;3&#34;:&#34;9&#34;,&#34;4&#34;:&#34;38&#34;,&#34;5&#34;:&#34;2&#34;,&#34;6&#34;:&#34;1&#34;},{&#34;1&#34;:&#34;9&#34;,&#34;2&#34;:&#34;9&#34;,&#34;3&#34;:&#34;9&#34;,&#34;4&#34;:&#34;33&#34;,&#34;5&#34;:&#34;2&#34;,&#34;6&#34;:&#34;1&#34;},{&#34;1&#34;:&#34;2&#34;,&#34;2&#34;:&#34;9&#34;,&#34;3&#34;:&#34;9&#34;,&#34;4&#34;:&#34;21&#34;,&#34;5&#34;:&#34;2&#34;,&#34;6&#34;:&#34;1&#34;}],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_data %&amp;gt;%
  filter(gender == 1 &amp;amp; language == 1)&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
&lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;901&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;902&#34;],&#34;name&#34;:[2],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;903&#34;],&#34;name&#34;:[3],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;age&#34;],&#34;name&#34;:[4],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;gender&#34;],&#34;name&#34;:[5],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;language&#34;],&#34;name&#34;:[6],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]}],&#34;data&#34;:[{&#34;1&#34;:&#34;7&#34;,&#34;2&#34;:&#34;9&#34;,&#34;3&#34;:&#34;9&#34;,&#34;4&#34;:&#34;21&#34;,&#34;5&#34;:&#34;1&#34;,&#34;6&#34;:&#34;1&#34;}],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>List Challenge</title>
      <link>/publication/problem-5/</link>
      <pubDate>Sat, 19 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/publication/problem-5/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/pagedtable/css/pagedtable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/pagedtable/js/pagedtable.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;the-problem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Problem&lt;/h2&gt;
&lt;p&gt;This is simple, If Names of List are found in List B then Replace:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a &amp;lt;- list(x = 1, y = TRUE, z = &amp;quot;a&amp;quot;)
b &amp;lt;- list(x = 2, z = &amp;quot;b&amp;quot;)
expected &amp;lt;- list(x = 2, y = TRUE, z = &amp;quot;b&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-solution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Solution&lt;/h2&gt;
&lt;p&gt;It was hard to think in something simple, because the problem is not as complicated, it is just List is a complicated object to deal with, but I came with this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a &amp;lt;- list(x = 1, y = TRUE, z = &amp;quot;a&amp;quot;)
b &amp;lt;- list(x = 2, z = &amp;quot;b&amp;quot;)

val_names &amp;lt;- names(a) %in% names(b) %&amp;gt;% names(a)[.]
a[val_names] &amp;lt;- b[val_names]

a&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $x
## [1] 2
## 
## $y
## [1] TRUE
## 
## $z
## [1] &amp;quot;b&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Ugly Untied Dataset</title>
      <link>/publication/problem-4/</link>
      <pubDate>Sat, 19 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/publication/problem-4/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/pagedtable/css/pagedtable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/pagedtable/js/pagedtable.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Even though the Data looks messy and an Intruitive solution didn´t pop up inmediately, It was relatively short to fix.&lt;/p&gt;
&lt;div id=&#34;the-problem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Problem&lt;/h2&gt;
&lt;p&gt;I want to save some words so I’ll go to the source&lt;/p&gt;
&lt;center&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Hey &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; peeps. I have ~36 tables like this extracted from the LCMM 📦 results. &lt;br&gt;I need to tidy it. &lt;br&gt;I want 5 rows with the values for intercept and sofa_study_day in individual columns. &lt;br&gt;&lt;br&gt;Suggestions? &lt;a href=&#34;https://t.co/1rFku6T0qt&#34;&gt;pic.twitter.com/1rFku6T0qt&lt;/a&gt;&lt;/p&gt;&amp;mdash; jsonpott (@jsonpott) &lt;a href=&#34;https://twitter.com/jsonpott/status/1185527244569174017?ref_src=twsrc%5Etfw&#34;&gt;October 19, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;/center&gt;
&lt;p&gt;So I just replicated the data and did the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Made data longer eliminating all the NAs that showed up.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Divided into Intercept and sofa_study_day.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Joining both together to obtain the 5 records.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The code looks like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data &amp;lt;- tibble::tribble(
  ~Sofa.time.point,     ~Se,   ~Wald, ~p.value,      ~x1,      ~x2,      ~x3,      ~x4,      ~x5,
       &amp;quot;intercept&amp;quot;, 0.12395, -24.333,        0,       NA,       NA,       NA, -3.01592,       NA,
       &amp;quot;intercept&amp;quot;, 0.13165, -40.045,        0,       NA,       NA,       NA,       NA, -5.27211,
       &amp;quot;intercept&amp;quot;, 0.21603,  -7.372,        0,       NA, -1.59253,       NA,       NA,       NA,
       &amp;quot;intercept&amp;quot;, 0.23614,  -5.085,        0,       NA,       NA, -1.20082,       NA,       NA,
       &amp;quot;intercept&amp;quot;,      NA,      NA,        0,        0,       NA,       NA,       NA,       NA,
  &amp;quot;sofa_study_day&amp;quot;, 0.00411, -14.669,        0,       NA,       NA,       NA,       NA, -0.06028,
  &amp;quot;sofa_study_day&amp;quot;, 0.00479, -34.798,        0,       NA,       NA,       NA, -0.16685,       NA,
  &amp;quot;sofa_study_day&amp;quot;, 0.00615, -39.744,        0, -0.24443,       NA,       NA,       NA,       NA,
  &amp;quot;sofa_study_day&amp;quot;, 0.00756,  -9.975,        0,       NA,       NA, -0.07543,       NA,       NA,
  &amp;quot;sofa_study_day&amp;quot;, 0.02224, -24.673,        0,       NA,  -0.5488,       NA,       NA,       NA
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy_data &amp;lt;- data %&amp;gt;%
  pivot_longer(
    # keeping columns from &amp;quot;Sofa.time.point&amp;quot; to &amp;quot;p.value&amp;quot;
    -(Sofa.time.point:p.value),
    # transform x columns into just one column
    names_to = &amp;quot;x&amp;quot;,
    # populate with values
    values_to = &amp;quot;values&amp;quot;,
    # dropping NAs
    values_drop_na = TRUE
  )

# &amp;quot;intercept&amp;quot; data
tidy_data %&amp;gt;%
  filter(Sofa.time.point == &amp;quot;intercept&amp;quot;) %&amp;gt;%
  left_join(
    #joined with &amp;quot;sofa_study_day&amp;quot;
    tidy_data %&amp;gt;%
      filter(Sofa.time.point == &amp;quot;sofa_study_day&amp;quot;),
    # joining by &amp;quot;x&amp;quot;
    by = &amp;quot;x&amp;quot;,
    # adding identifiers to columns having the same name
    suffix = c(&amp;quot;.intercept&amp;quot;, &amp;quot;.sofa&amp;quot;)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
&lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;Sofa.time.point.intercept&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;Se.intercept&#34;],&#34;name&#34;:[2],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;Wald.intercept&#34;],&#34;name&#34;:[3],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;p.value.intercept&#34;],&#34;name&#34;:[4],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;x&#34;],&#34;name&#34;:[5],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;values.intercept&#34;],&#34;name&#34;:[6],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;Sofa.time.point.sofa&#34;],&#34;name&#34;:[7],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;Se.sofa&#34;],&#34;name&#34;:[8],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;Wald.sofa&#34;],&#34;name&#34;:[9],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;p.value.sofa&#34;],&#34;name&#34;:[10],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;values.sofa&#34;],&#34;name&#34;:[11],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]}],&#34;data&#34;:[{&#34;1&#34;:&#34;intercept&#34;,&#34;2&#34;:&#34;0.12395&#34;,&#34;3&#34;:&#34;-24.333&#34;,&#34;4&#34;:&#34;0&#34;,&#34;5&#34;:&#34;x4&#34;,&#34;6&#34;:&#34;-3.01592&#34;,&#34;7&#34;:&#34;sofa_study_day&#34;,&#34;8&#34;:&#34;0.00479&#34;,&#34;9&#34;:&#34;-34.798&#34;,&#34;10&#34;:&#34;0&#34;,&#34;11&#34;:&#34;-0.16685&#34;},{&#34;1&#34;:&#34;intercept&#34;,&#34;2&#34;:&#34;0.13165&#34;,&#34;3&#34;:&#34;-40.045&#34;,&#34;4&#34;:&#34;0&#34;,&#34;5&#34;:&#34;x5&#34;,&#34;6&#34;:&#34;-5.27211&#34;,&#34;7&#34;:&#34;sofa_study_day&#34;,&#34;8&#34;:&#34;0.00411&#34;,&#34;9&#34;:&#34;-14.669&#34;,&#34;10&#34;:&#34;0&#34;,&#34;11&#34;:&#34;-0.06028&#34;},{&#34;1&#34;:&#34;intercept&#34;,&#34;2&#34;:&#34;0.21603&#34;,&#34;3&#34;:&#34;-7.372&#34;,&#34;4&#34;:&#34;0&#34;,&#34;5&#34;:&#34;x2&#34;,&#34;6&#34;:&#34;-1.59253&#34;,&#34;7&#34;:&#34;sofa_study_day&#34;,&#34;8&#34;:&#34;0.02224&#34;,&#34;9&#34;:&#34;-24.673&#34;,&#34;10&#34;:&#34;0&#34;,&#34;11&#34;:&#34;-0.54880&#34;},{&#34;1&#34;:&#34;intercept&#34;,&#34;2&#34;:&#34;0.23614&#34;,&#34;3&#34;:&#34;-5.085&#34;,&#34;4&#34;:&#34;0&#34;,&#34;5&#34;:&#34;x3&#34;,&#34;6&#34;:&#34;-1.20082&#34;,&#34;7&#34;:&#34;sofa_study_day&#34;,&#34;8&#34;:&#34;0.00756&#34;,&#34;9&#34;:&#34;-9.975&#34;,&#34;10&#34;:&#34;0&#34;,&#34;11&#34;:&#34;-0.07543&#34;},{&#34;1&#34;:&#34;intercept&#34;,&#34;2&#34;:&#34;NA&#34;,&#34;3&#34;:&#34;NA&#34;,&#34;4&#34;:&#34;0&#34;,&#34;5&#34;:&#34;x1&#34;,&#34;6&#34;:&#34;0.00000&#34;,&#34;7&#34;:&#34;sofa_study_day&#34;,&#34;8&#34;:&#34;0.00615&#34;,&#34;9&#34;:&#34;-39.744&#34;,&#34;10&#34;:&#34;0&#34;,&#34;11&#34;:&#34;-0.24443&#34;}],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Missing Value Imputation</title>
      <link>/publication/problem-1/</link>
      <pubDate>Wed, 16 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/publication/problem-1/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/pagedtable/css/pagedtable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/pagedtable/js/pagedtable.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Surfing at Stack Overflow I noticed a problem that I found interesting to solve:
The following Data was presented:&lt;/p&gt;
&lt;div id=&#34;the-problem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Problem&lt;/h2&gt;
&lt;p&gt;The following Data is presented:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sample &amp;lt;-
  structure(
    list(
      `Country Name` = c(
        &amp;quot;Aruba&amp;quot;,
        &amp;quot;Afghanistan&amp;quot;,
        &amp;quot;Angola&amp;quot;,
        &amp;quot;Albania&amp;quot;,
        &amp;quot;Andorra&amp;quot;,
        &amp;quot;Arab World&amp;quot;,
        &amp;quot;United Arab Emirates&amp;quot;,
        &amp;quot;Argentina&amp;quot;,
        &amp;quot;Armenia&amp;quot;,
        &amp;quot;American Samoa&amp;quot;,
        &amp;quot;Antigua and Barbuda&amp;quot;,
        &amp;quot;Australia&amp;quot;
      ),
      `Country Code` = c(
        &amp;quot;ABW&amp;quot;,
        &amp;quot;AFG&amp;quot;,
        &amp;quot;AGO&amp;quot;,
        &amp;quot;ALB&amp;quot;,
        &amp;quot;AND&amp;quot;,
        &amp;quot;ARB&amp;quot;,
        &amp;quot;ARE&amp;quot;,
        &amp;quot;ARG&amp;quot;,
        &amp;quot;ARM&amp;quot;,
        &amp;quot;ASM&amp;quot;,
        &amp;quot;ATG&amp;quot;,
        &amp;quot;AUS&amp;quot;
      ),
      `2007` = c(
        5.39162036843645,
        8.68057078513406,
        12.2514974459487,
        2.93268248162318,
        NA,
        4.74356585295154,
        NA,
        NA,
        NA,
        NA,
        1.41605259409743,
        NA
      ),
      `2008` = c(
        8.95722105296535,
        26.4186641547444,
        12.4758291326398,
        3.36313757366391,
        NA,
        NA,
        12.2504202448139,
        NA,
        8.94995335353386,
        NA,
        5.33380639820232,
        NA
      ),
      `2009` = c(
        -2.13630037272305,-6.81116108898995,
        13.7302839288409,
        2.23139683475865,
        NA,
        2.92089711805365,
        1.55980098148558,
        NA,
        3.40676682683799,
        NA,
        -0.550159995508869,
        NA
      ),
      `2010` = c(
        2.07773902027782,
        2.1785375238942,
        14.4696564932574,
        3.61538461538463,
        NA,
        3.91106195534027,
        0.879216764156813,
        NA,
        8.17636138473956,
        NA,
        3.3700254022015,
        2.91834002677376
      ),
      `2011` = c(
        4.31633194082721,
        11.8041858089129,
        13.4824679218511,
        3.44283593170005,
        NA,
        4.75316388885632,
        NA,
        NA,
        7.6500080785929,
        NA,
        3.45674967234599,
        3.30385015608744
      ),
      `2012` = c(
        0.627927921638161,
        6.44121280934118,
        10.2779049218839,
        2.03642235579081,
        NA,
        4.61184432206646,
        0.662268900269082,
        NA,
        2.55802007757907,
        NA,
        3.37688044338879,
        1.76278015613193
      ),
      `2013` = c(
        -2.37226328015073,
        7.38577178397857,
        8.77781429332619,
        1.92544399507649,
        NA,
        3.23423783752364,
        1.10111836375706,
        NA,
        5.78966778544654,
        NA,
        1.05949782356168,
        2.44988864142539
      ),
      `2014` = c(
        0.421637771012246,
        4.67399603536339,
        7.28038730361125,
        1.61304235314414,
        NA,
        2.77261158414198,
        2.34626865671643,
        NA,
        2.98130868933673,
        NA,
        1.08944157435363,
        2.48792270531403
      )
    ),
    class = c(&amp;quot;tbl_df&amp;quot;, &amp;quot;tbl&amp;quot;, &amp;quot;data.frame&amp;quot;),
    row.names = c(NA,-12L)
  )

sample&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
&lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;Country Name&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;Country Code&#34;],&#34;name&#34;:[2],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;2007&#34;],&#34;name&#34;:[3],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;2008&#34;],&#34;name&#34;:[4],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;2009&#34;],&#34;name&#34;:[5],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;2010&#34;],&#34;name&#34;:[6],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;2011&#34;],&#34;name&#34;:[7],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;2012&#34;],&#34;name&#34;:[8],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;2013&#34;],&#34;name&#34;:[9],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;2014&#34;],&#34;name&#34;:[10],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]}],&#34;data&#34;:[{&#34;1&#34;:&#34;Aruba&#34;,&#34;2&#34;:&#34;ABW&#34;,&#34;3&#34;:&#34;5.391620&#34;,&#34;4&#34;:&#34;8.957221&#34;,&#34;5&#34;:&#34;-2.136300&#34;,&#34;6&#34;:&#34;2.0777390&#34;,&#34;7&#34;:&#34;4.316332&#34;,&#34;8&#34;:&#34;0.6279279&#34;,&#34;9&#34;:&#34;-2.372263&#34;,&#34;10&#34;:&#34;0.4216378&#34;},{&#34;1&#34;:&#34;Afghanistan&#34;,&#34;2&#34;:&#34;AFG&#34;,&#34;3&#34;:&#34;8.680571&#34;,&#34;4&#34;:&#34;26.418664&#34;,&#34;5&#34;:&#34;-6.811161&#34;,&#34;6&#34;:&#34;2.1785375&#34;,&#34;7&#34;:&#34;11.804186&#34;,&#34;8&#34;:&#34;6.4412128&#34;,&#34;9&#34;:&#34;7.385772&#34;,&#34;10&#34;:&#34;4.6739960&#34;},{&#34;1&#34;:&#34;Angola&#34;,&#34;2&#34;:&#34;AGO&#34;,&#34;3&#34;:&#34;12.251497&#34;,&#34;4&#34;:&#34;12.475829&#34;,&#34;5&#34;:&#34;13.730284&#34;,&#34;6&#34;:&#34;14.4696565&#34;,&#34;7&#34;:&#34;13.482468&#34;,&#34;8&#34;:&#34;10.2779049&#34;,&#34;9&#34;:&#34;8.777814&#34;,&#34;10&#34;:&#34;7.2803873&#34;},{&#34;1&#34;:&#34;Albania&#34;,&#34;2&#34;:&#34;ALB&#34;,&#34;3&#34;:&#34;2.932682&#34;,&#34;4&#34;:&#34;3.363138&#34;,&#34;5&#34;:&#34;2.231397&#34;,&#34;6&#34;:&#34;3.6153846&#34;,&#34;7&#34;:&#34;3.442836&#34;,&#34;8&#34;:&#34;2.0364224&#34;,&#34;9&#34;:&#34;1.925444&#34;,&#34;10&#34;:&#34;1.6130424&#34;},{&#34;1&#34;:&#34;Andorra&#34;,&#34;2&#34;:&#34;AND&#34;,&#34;3&#34;:&#34;NA&#34;,&#34;4&#34;:&#34;NA&#34;,&#34;5&#34;:&#34;NA&#34;,&#34;6&#34;:&#34;NA&#34;,&#34;7&#34;:&#34;NA&#34;,&#34;8&#34;:&#34;NA&#34;,&#34;9&#34;:&#34;NA&#34;,&#34;10&#34;:&#34;NA&#34;},{&#34;1&#34;:&#34;Arab World&#34;,&#34;2&#34;:&#34;ARB&#34;,&#34;3&#34;:&#34;4.743566&#34;,&#34;4&#34;:&#34;NA&#34;,&#34;5&#34;:&#34;2.920897&#34;,&#34;6&#34;:&#34;3.9110620&#34;,&#34;7&#34;:&#34;4.753164&#34;,&#34;8&#34;:&#34;4.6118443&#34;,&#34;9&#34;:&#34;3.234238&#34;,&#34;10&#34;:&#34;2.7726116&#34;},{&#34;1&#34;:&#34;United Arab Emirates&#34;,&#34;2&#34;:&#34;ARE&#34;,&#34;3&#34;:&#34;NA&#34;,&#34;4&#34;:&#34;12.250420&#34;,&#34;5&#34;:&#34;1.559801&#34;,&#34;6&#34;:&#34;0.8792168&#34;,&#34;7&#34;:&#34;NA&#34;,&#34;8&#34;:&#34;0.6622689&#34;,&#34;9&#34;:&#34;1.101118&#34;,&#34;10&#34;:&#34;2.3462687&#34;},{&#34;1&#34;:&#34;Argentina&#34;,&#34;2&#34;:&#34;ARG&#34;,&#34;3&#34;:&#34;NA&#34;,&#34;4&#34;:&#34;NA&#34;,&#34;5&#34;:&#34;NA&#34;,&#34;6&#34;:&#34;NA&#34;,&#34;7&#34;:&#34;NA&#34;,&#34;8&#34;:&#34;NA&#34;,&#34;9&#34;:&#34;NA&#34;,&#34;10&#34;:&#34;NA&#34;},{&#34;1&#34;:&#34;Armenia&#34;,&#34;2&#34;:&#34;ARM&#34;,&#34;3&#34;:&#34;NA&#34;,&#34;4&#34;:&#34;8.949953&#34;,&#34;5&#34;:&#34;3.406767&#34;,&#34;6&#34;:&#34;8.1763614&#34;,&#34;7&#34;:&#34;7.650008&#34;,&#34;8&#34;:&#34;2.5580201&#34;,&#34;9&#34;:&#34;5.789668&#34;,&#34;10&#34;:&#34;2.9813087&#34;},{&#34;1&#34;:&#34;American Samoa&#34;,&#34;2&#34;:&#34;ASM&#34;,&#34;3&#34;:&#34;NA&#34;,&#34;4&#34;:&#34;NA&#34;,&#34;5&#34;:&#34;NA&#34;,&#34;6&#34;:&#34;NA&#34;,&#34;7&#34;:&#34;NA&#34;,&#34;8&#34;:&#34;NA&#34;,&#34;9&#34;:&#34;NA&#34;,&#34;10&#34;:&#34;NA&#34;},{&#34;1&#34;:&#34;Antigua and Barbuda&#34;,&#34;2&#34;:&#34;ATG&#34;,&#34;3&#34;:&#34;1.416053&#34;,&#34;4&#34;:&#34;5.333806&#34;,&#34;5&#34;:&#34;-0.550160&#34;,&#34;6&#34;:&#34;3.3700254&#34;,&#34;7&#34;:&#34;3.456750&#34;,&#34;8&#34;:&#34;3.3768804&#34;,&#34;9&#34;:&#34;1.059498&#34;,&#34;10&#34;:&#34;1.0894416&#34;},{&#34;1&#34;:&#34;Australia&#34;,&#34;2&#34;:&#34;AUS&#34;,&#34;3&#34;:&#34;NA&#34;,&#34;4&#34;:&#34;NA&#34;,&#34;5&#34;:&#34;NA&#34;,&#34;6&#34;:&#34;2.9183400&#34;,&#34;7&#34;:&#34;3.303850&#34;,&#34;8&#34;:&#34;1.7627802&#34;,&#34;9&#34;:&#34;2.449889&#34;,&#34;10&#34;:&#34;2.4879227&#34;}],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;p&gt;The idea is to input Missing Values following some rules:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Some countries have NAs for all 8 years (columns 3:10), and in that case I want to replace all NAs with the column mean.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Other countries only have NAs in some columns, in which case I want to replace NA with the previous year’s value.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The final condition is that, if the NA is in the first year (2007), I want to replace it with the 2007 column mean instead of the next year (2008 was the financial crisis so all the inflation rates went nuts).&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Of course this can be easily programmed using Regular Programming Rules using For loops and If Statements, but the idea is to do it in a tidy way using the Tidyverse.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr, warn.conflicts = FALSE)
library(tidyr)
library(janitor)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;janitor&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:stats&amp;#39;:
## 
##     chisq.test, fisher.test&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Getting the Column Means to Replace according to Condition 1 and 3. 
(replacement &amp;lt;- sample %&amp;gt;%
    select_if(is.numeric) %&amp;gt;%
    summarize_all( ~ mean(., na.rm = TRUE)) %&amp;gt;%
    #Transformed to List since it is a requirement for tidyr::replace_na()
    as.list())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $`2007`
## [1] 5.902665
## 
## $`2008`
## [1] 11.107
## 
## $`2009`
## [1] 1.793941
## 
## $`2010`
## [1] 4.621814
## 
## $`2011`
## [1] 6.526199
## 
## $`2012`
## [1] 3.595029
## 
## $`2013`
## [1] 3.261242
## 
## $`2014`
## [1] 2.851846&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-solution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The solution&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sample %&amp;gt;%
  pivot_longer(`2007`:`2014`, names_to = &amp;quot;year&amp;quot;, values_to = &amp;quot;int_rate&amp;quot;) %&amp;gt;%
  group_by(`Country Name`) %&amp;gt;%
  summarize(na_num = is.na(int_rate) %&amp;gt;% sum) %&amp;gt;%
  #Joining the number of NAs na_num as a new column
  left_join(sample, by = &amp;quot;Country Name&amp;quot;) %&amp;gt;%
  #Replacing 2007 missing as a first value. Condition 3.
  mutate(`2007` = if_else(between(na_num, 1, 7) &amp;amp;
                            is.na(`2007`), replacement[[1]] , `2007`)) %&amp;gt;%
  #Making dataset wider 
  pivot_longer(`2007`:`2014`, names_to = &amp;quot;year&amp;quot;, values_to = &amp;quot;int_rate&amp;quot;) %&amp;gt;%
  group_by(`Country Name`) %&amp;gt;%
  #Using fill to impute NAs with the previous one. Condition 2.
  fill(int_rate) %&amp;gt;%
  pivot_wider(names_from = year, values_from = int_rate) %&amp;gt;%
  #Replacing Values when all values are missing. Condition 1.
  replace_na(replace = replacement) &lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
&lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;Country Name&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;na_num&#34;],&#34;name&#34;:[2],&#34;type&#34;:[&#34;int&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;Country Code&#34;],&#34;name&#34;:[3],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;2007&#34;],&#34;name&#34;:[4],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;2008&#34;],&#34;name&#34;:[5],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;2009&#34;],&#34;name&#34;:[6],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;2010&#34;],&#34;name&#34;:[7],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;2011&#34;],&#34;name&#34;:[8],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;2012&#34;],&#34;name&#34;:[9],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;2013&#34;],&#34;name&#34;:[10],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;2014&#34;],&#34;name&#34;:[11],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]}],&#34;data&#34;:[{&#34;1&#34;:&#34;Afghanistan&#34;,&#34;2&#34;:&#34;0&#34;,&#34;3&#34;:&#34;AFG&#34;,&#34;4&#34;:&#34;8.680571&#34;,&#34;5&#34;:&#34;26.418664&#34;,&#34;6&#34;:&#34;-6.811161&#34;,&#34;7&#34;:&#34;2.1785375&#34;,&#34;8&#34;:&#34;11.8041858&#34;,&#34;9&#34;:&#34;6.4412128&#34;,&#34;10&#34;:&#34;7.385772&#34;,&#34;11&#34;:&#34;4.6739960&#34;},{&#34;1&#34;:&#34;Albania&#34;,&#34;2&#34;:&#34;0&#34;,&#34;3&#34;:&#34;ALB&#34;,&#34;4&#34;:&#34;2.932682&#34;,&#34;5&#34;:&#34;3.363138&#34;,&#34;6&#34;:&#34;2.231397&#34;,&#34;7&#34;:&#34;3.6153846&#34;,&#34;8&#34;:&#34;3.4428359&#34;,&#34;9&#34;:&#34;2.0364224&#34;,&#34;10&#34;:&#34;1.925444&#34;,&#34;11&#34;:&#34;1.6130424&#34;},{&#34;1&#34;:&#34;American Samoa&#34;,&#34;2&#34;:&#34;8&#34;,&#34;3&#34;:&#34;ASM&#34;,&#34;4&#34;:&#34;5.902665&#34;,&#34;5&#34;:&#34;11.107005&#34;,&#34;6&#34;:&#34;1.793941&#34;,&#34;7&#34;:&#34;4.6218137&#34;,&#34;8&#34;:&#34;6.5261992&#34;,&#34;9&#34;:&#34;3.5950291&#34;,&#34;10&#34;:&#34;3.261242&#34;,&#34;11&#34;:&#34;2.8518463&#34;},{&#34;1&#34;:&#34;Andorra&#34;,&#34;2&#34;:&#34;8&#34;,&#34;3&#34;:&#34;AND&#34;,&#34;4&#34;:&#34;5.902665&#34;,&#34;5&#34;:&#34;11.107005&#34;,&#34;6&#34;:&#34;1.793941&#34;,&#34;7&#34;:&#34;4.6218137&#34;,&#34;8&#34;:&#34;6.5261992&#34;,&#34;9&#34;:&#34;3.5950291&#34;,&#34;10&#34;:&#34;3.261242&#34;,&#34;11&#34;:&#34;2.8518463&#34;},{&#34;1&#34;:&#34;Angola&#34;,&#34;2&#34;:&#34;0&#34;,&#34;3&#34;:&#34;AGO&#34;,&#34;4&#34;:&#34;12.251497&#34;,&#34;5&#34;:&#34;12.475829&#34;,&#34;6&#34;:&#34;13.730284&#34;,&#34;7&#34;:&#34;14.4696565&#34;,&#34;8&#34;:&#34;13.4824679&#34;,&#34;9&#34;:&#34;10.2779049&#34;,&#34;10&#34;:&#34;8.777814&#34;,&#34;11&#34;:&#34;7.2803873&#34;},{&#34;1&#34;:&#34;Antigua and Barbuda&#34;,&#34;2&#34;:&#34;0&#34;,&#34;3&#34;:&#34;ATG&#34;,&#34;4&#34;:&#34;1.416053&#34;,&#34;5&#34;:&#34;5.333806&#34;,&#34;6&#34;:&#34;-0.550160&#34;,&#34;7&#34;:&#34;3.3700254&#34;,&#34;8&#34;:&#34;3.4567497&#34;,&#34;9&#34;:&#34;3.3768804&#34;,&#34;10&#34;:&#34;1.059498&#34;,&#34;11&#34;:&#34;1.0894416&#34;},{&#34;1&#34;:&#34;Arab World&#34;,&#34;2&#34;:&#34;1&#34;,&#34;3&#34;:&#34;ARB&#34;,&#34;4&#34;:&#34;4.743566&#34;,&#34;5&#34;:&#34;4.743566&#34;,&#34;6&#34;:&#34;2.920897&#34;,&#34;7&#34;:&#34;3.9110620&#34;,&#34;8&#34;:&#34;4.7531639&#34;,&#34;9&#34;:&#34;4.6118443&#34;,&#34;10&#34;:&#34;3.234238&#34;,&#34;11&#34;:&#34;2.7726116&#34;},{&#34;1&#34;:&#34;Argentina&#34;,&#34;2&#34;:&#34;8&#34;,&#34;3&#34;:&#34;ARG&#34;,&#34;4&#34;:&#34;5.902665&#34;,&#34;5&#34;:&#34;11.107005&#34;,&#34;6&#34;:&#34;1.793941&#34;,&#34;7&#34;:&#34;4.6218137&#34;,&#34;8&#34;:&#34;6.5261992&#34;,&#34;9&#34;:&#34;3.5950291&#34;,&#34;10&#34;:&#34;3.261242&#34;,&#34;11&#34;:&#34;2.8518463&#34;},{&#34;1&#34;:&#34;Armenia&#34;,&#34;2&#34;:&#34;1&#34;,&#34;3&#34;:&#34;ARM&#34;,&#34;4&#34;:&#34;5.902665&#34;,&#34;5&#34;:&#34;8.949953&#34;,&#34;6&#34;:&#34;3.406767&#34;,&#34;7&#34;:&#34;8.1763614&#34;,&#34;8&#34;:&#34;7.6500081&#34;,&#34;9&#34;:&#34;2.5580201&#34;,&#34;10&#34;:&#34;5.789668&#34;,&#34;11&#34;:&#34;2.9813087&#34;},{&#34;1&#34;:&#34;Aruba&#34;,&#34;2&#34;:&#34;0&#34;,&#34;3&#34;:&#34;ABW&#34;,&#34;4&#34;:&#34;5.391620&#34;,&#34;5&#34;:&#34;8.957221&#34;,&#34;6&#34;:&#34;-2.136300&#34;,&#34;7&#34;:&#34;2.0777390&#34;,&#34;8&#34;:&#34;4.3163319&#34;,&#34;9&#34;:&#34;0.6279279&#34;,&#34;10&#34;:&#34;-2.372263&#34;,&#34;11&#34;:&#34;0.4216378&#34;},{&#34;1&#34;:&#34;Australia&#34;,&#34;2&#34;:&#34;3&#34;,&#34;3&#34;:&#34;AUS&#34;,&#34;4&#34;:&#34;5.902665&#34;,&#34;5&#34;:&#34;5.902665&#34;,&#34;6&#34;:&#34;5.902665&#34;,&#34;7&#34;:&#34;2.9183400&#34;,&#34;8&#34;:&#34;3.3038502&#34;,&#34;9&#34;:&#34;1.7627802&#34;,&#34;10&#34;:&#34;2.449889&#34;,&#34;11&#34;:&#34;2.4879227&#34;},{&#34;1&#34;:&#34;United Arab Emirates&#34;,&#34;2&#34;:&#34;2&#34;,&#34;3&#34;:&#34;ARE&#34;,&#34;4&#34;:&#34;5.902665&#34;,&#34;5&#34;:&#34;12.250420&#34;,&#34;6&#34;:&#34;1.559801&#34;,&#34;7&#34;:&#34;0.8792168&#34;,&#34;8&#34;:&#34;0.8792168&#34;,&#34;9&#34;:&#34;0.6622689&#34;,&#34;10&#34;:&#34;1.101118&#34;,&#34;11&#34;:&#34;2.3462687&#34;}],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Tidy Evaluation</title>
      <link>/publication/problem-2/</link>
      <pubDate>Wed, 16 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/publication/problem-2/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/pagedtable/css/pagedtable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/pagedtable/js/pagedtable.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Navigating Twitter I found this other Problem:&lt;/p&gt;
&lt;div id=&#34;the-problem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Problem&lt;/h2&gt;
&lt;p&gt;The following dummy_function is presented:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
#&amp;gt; 
a &amp;lt;- sample(letters[1:5], 500, rep = TRUE)
b &amp;lt;- sample(1:10, 500, rep = TRUE)
df1 &amp;lt;- data.frame(a, b)
 
dummy_function &amp;lt;- function(data, var1, var2){
  # Creating summary statistics
  df &amp;lt;- data %&amp;gt;%
    group_by(var1, var2) %&amp;gt;%
    summarise(n=n()) %&amp;gt;%
    group_by(var1) %&amp;gt;%
    mutate(perc=100*n/sum(n))
    
  df
}
dummy_function(df1, a, b)
#&amp;gt; Error: Column `var1` is unknown&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;
Created by the &lt;a href=&#34;https://reprex.tidyverse.org&#34;&gt;reprex package&lt;/a&gt; (v0.3.0)
&lt;/p&gt;
&lt;p&gt;This is a typical problem caused by one of the coolest things provided by the tidyverse: the Non-Standard Evaluation.&lt;/p&gt;
&lt;p&gt;Non-Standard Evaluation is the ability that some R functions have (mainly in the tidyverse and all the packages following a tidy approach) when you can pass a variable within the data without quoting:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;iris %&amp;gt;% 
  select(Species) %&amp;gt;%
  head(10)&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
&lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;&#34;],&#34;name&#34;:[&#34;_rn_&#34;],&#34;type&#34;:[&#34;&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;Species&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;fctr&#34;],&#34;align&#34;:[&#34;left&#34;]}],&#34;data&#34;:[{&#34;1&#34;:&#34;setosa&#34;,&#34;_rn_&#34;:&#34;1&#34;},{&#34;1&#34;:&#34;setosa&#34;,&#34;_rn_&#34;:&#34;2&#34;},{&#34;1&#34;:&#34;setosa&#34;,&#34;_rn_&#34;:&#34;3&#34;},{&#34;1&#34;:&#34;setosa&#34;,&#34;_rn_&#34;:&#34;4&#34;},{&#34;1&#34;:&#34;setosa&#34;,&#34;_rn_&#34;:&#34;5&#34;},{&#34;1&#34;:&#34;setosa&#34;,&#34;_rn_&#34;:&#34;6&#34;},{&#34;1&#34;:&#34;setosa&#34;,&#34;_rn_&#34;:&#34;7&#34;},{&#34;1&#34;:&#34;setosa&#34;,&#34;_rn_&#34;:&#34;8&#34;},{&#34;1&#34;:&#34;setosa&#34;,&#34;_rn_&#34;:&#34;9&#34;},{&#34;1&#34;:&#34;setosa&#34;,&#34;_rn_&#34;:&#34;10&#34;}],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;p&gt;As you may see, you don´t need to quote Species, but R is not recognizing Species as an R object but as an existing variable within iris dataset. If you would like to do the same thing using “&lt;em&gt;Standard Evaluation&lt;/em&gt;” you´d have to code something like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(iris[&amp;quot;Species&amp;quot;], 10)&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
&lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;&#34;],&#34;name&#34;:[&#34;_rn_&#34;],&#34;type&#34;:[&#34;&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;Species&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;fctr&#34;],&#34;align&#34;:[&#34;left&#34;]}],&#34;data&#34;:[{&#34;1&#34;:&#34;setosa&#34;,&#34;_rn_&#34;:&#34;1&#34;},{&#34;1&#34;:&#34;setosa&#34;,&#34;_rn_&#34;:&#34;2&#34;},{&#34;1&#34;:&#34;setosa&#34;,&#34;_rn_&#34;:&#34;3&#34;},{&#34;1&#34;:&#34;setosa&#34;,&#34;_rn_&#34;:&#34;4&#34;},{&#34;1&#34;:&#34;setosa&#34;,&#34;_rn_&#34;:&#34;5&#34;},{&#34;1&#34;:&#34;setosa&#34;,&#34;_rn_&#34;:&#34;6&#34;},{&#34;1&#34;:&#34;setosa&#34;,&#34;_rn_&#34;:&#34;7&#34;},{&#34;1&#34;:&#34;setosa&#34;,&#34;_rn_&#34;:&#34;8&#34;},{&#34;1&#34;:&#34;setosa&#34;,&#34;_rn_&#34;:&#34;9&#34;},{&#34;1&#34;:&#34;setosa&#34;,&#34;_rn_&#34;:&#34;10&#34;}],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;p&gt;In this case you see Species is not an object but a quoted string that is passed as the Variable name for object Iris.&lt;/p&gt;
&lt;p&gt;The error then pops up because in the dummy_function() you have group_by() that uses NSE having var1, var2 as arguments and var1 and var2 objects are not variables of data. What you actually want is to pass var1 and var2 values as the grouping variables.&lt;/p&gt;
&lt;p&gt;Definitely NSE is a great addition and saves typing, but when it comes to create functions it used to be a nightmare. rlang package handled this using something called quosures, and the bang-bang operator. If you want to know about this Hadley teaches it in 5 minutes:&lt;/p&gt;
&lt;center&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/nERXS3ssntw&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;/center&gt;
&lt;/div&gt;
&lt;div id=&#34;the-solution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The solution&lt;/h2&gt;
&lt;p&gt;Fortunately, Hadley’s explanation is helpful to understand the problem but the solution now is super easy with the new version of rlang. You just need to wrap var1 and var2 in the new curly-curly operator to embrace the values of var1 and var2 and pass them along the group_by() function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a &amp;lt;- sample(letters[1:5], 500, rep = TRUE)
b &amp;lt;- sample(1:10, 500, rep = TRUE)
df1 &amp;lt;- data.frame(a, b)

library(rlang)
dummy_function &amp;lt;- function(data, var1, var2){
  # Creating summary statistics
  df &amp;lt;- data %&amp;gt;%
    group_by({{var1}}, {{var2}}) %&amp;gt;%
    summarise(n=n()) %&amp;gt;%
    group_by({{var1}}) %&amp;gt;%
    mutate(perc=100*n/sum(n))
  
  df
}
dummy_function(df1, a, b)&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
&lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;a&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;fctr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;b&#34;],&#34;name&#34;:[2],&#34;type&#34;:[&#34;int&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;n&#34;],&#34;name&#34;:[3],&#34;type&#34;:[&#34;int&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;perc&#34;],&#34;name&#34;:[4],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]}],&#34;data&#34;:[{&#34;1&#34;:&#34;a&#34;,&#34;2&#34;:&#34;1&#34;,&#34;3&#34;:&#34;13&#34;,&#34;4&#34;:&#34;11.403509&#34;},{&#34;1&#34;:&#34;a&#34;,&#34;2&#34;:&#34;2&#34;,&#34;3&#34;:&#34;14&#34;,&#34;4&#34;:&#34;12.280702&#34;},{&#34;1&#34;:&#34;a&#34;,&#34;2&#34;:&#34;3&#34;,&#34;3&#34;:&#34;16&#34;,&#34;4&#34;:&#34;14.035088&#34;},{&#34;1&#34;:&#34;a&#34;,&#34;2&#34;:&#34;4&#34;,&#34;3&#34;:&#34;3&#34;,&#34;4&#34;:&#34;2.631579&#34;},{&#34;1&#34;:&#34;a&#34;,&#34;2&#34;:&#34;5&#34;,&#34;3&#34;:&#34;10&#34;,&#34;4&#34;:&#34;8.771930&#34;},{&#34;1&#34;:&#34;a&#34;,&#34;2&#34;:&#34;6&#34;,&#34;3&#34;:&#34;8&#34;,&#34;4&#34;:&#34;7.017544&#34;},{&#34;1&#34;:&#34;a&#34;,&#34;2&#34;:&#34;7&#34;,&#34;3&#34;:&#34;13&#34;,&#34;4&#34;:&#34;11.403509&#34;},{&#34;1&#34;:&#34;a&#34;,&#34;2&#34;:&#34;8&#34;,&#34;3&#34;:&#34;15&#34;,&#34;4&#34;:&#34;13.157895&#34;},{&#34;1&#34;:&#34;a&#34;,&#34;2&#34;:&#34;9&#34;,&#34;3&#34;:&#34;14&#34;,&#34;4&#34;:&#34;12.280702&#34;},{&#34;1&#34;:&#34;a&#34;,&#34;2&#34;:&#34;10&#34;,&#34;3&#34;:&#34;8&#34;,&#34;4&#34;:&#34;7.017544&#34;},{&#34;1&#34;:&#34;b&#34;,&#34;2&#34;:&#34;1&#34;,&#34;3&#34;:&#34;10&#34;,&#34;4&#34;:&#34;10.416667&#34;},{&#34;1&#34;:&#34;b&#34;,&#34;2&#34;:&#34;2&#34;,&#34;3&#34;:&#34;8&#34;,&#34;4&#34;:&#34;8.333333&#34;},{&#34;1&#34;:&#34;b&#34;,&#34;2&#34;:&#34;3&#34;,&#34;3&#34;:&#34;5&#34;,&#34;4&#34;:&#34;5.208333&#34;},{&#34;1&#34;:&#34;b&#34;,&#34;2&#34;:&#34;4&#34;,&#34;3&#34;:&#34;8&#34;,&#34;4&#34;:&#34;8.333333&#34;},{&#34;1&#34;:&#34;b&#34;,&#34;2&#34;:&#34;5&#34;,&#34;3&#34;:&#34;9&#34;,&#34;4&#34;:&#34;9.375000&#34;},{&#34;1&#34;:&#34;b&#34;,&#34;2&#34;:&#34;6&#34;,&#34;3&#34;:&#34;12&#34;,&#34;4&#34;:&#34;12.500000&#34;},{&#34;1&#34;:&#34;b&#34;,&#34;2&#34;:&#34;7&#34;,&#34;3&#34;:&#34;11&#34;,&#34;4&#34;:&#34;11.458333&#34;},{&#34;1&#34;:&#34;b&#34;,&#34;2&#34;:&#34;8&#34;,&#34;3&#34;:&#34;12&#34;,&#34;4&#34;:&#34;12.500000&#34;},{&#34;1&#34;:&#34;b&#34;,&#34;2&#34;:&#34;9&#34;,&#34;3&#34;:&#34;9&#34;,&#34;4&#34;:&#34;9.375000&#34;},{&#34;1&#34;:&#34;b&#34;,&#34;2&#34;:&#34;10&#34;,&#34;3&#34;:&#34;12&#34;,&#34;4&#34;:&#34;12.500000&#34;},{&#34;1&#34;:&#34;c&#34;,&#34;2&#34;:&#34;1&#34;,&#34;3&#34;:&#34;9&#34;,&#34;4&#34;:&#34;9.890110&#34;},{&#34;1&#34;:&#34;c&#34;,&#34;2&#34;:&#34;2&#34;,&#34;3&#34;:&#34;7&#34;,&#34;4&#34;:&#34;7.692308&#34;},{&#34;1&#34;:&#34;c&#34;,&#34;2&#34;:&#34;3&#34;,&#34;3&#34;:&#34;10&#34;,&#34;4&#34;:&#34;10.989011&#34;},{&#34;1&#34;:&#34;c&#34;,&#34;2&#34;:&#34;4&#34;,&#34;3&#34;:&#34;9&#34;,&#34;4&#34;:&#34;9.890110&#34;},{&#34;1&#34;:&#34;c&#34;,&#34;2&#34;:&#34;5&#34;,&#34;3&#34;:&#34;11&#34;,&#34;4&#34;:&#34;12.087912&#34;},{&#34;1&#34;:&#34;c&#34;,&#34;2&#34;:&#34;6&#34;,&#34;3&#34;:&#34;10&#34;,&#34;4&#34;:&#34;10.989011&#34;},{&#34;1&#34;:&#34;c&#34;,&#34;2&#34;:&#34;7&#34;,&#34;3&#34;:&#34;9&#34;,&#34;4&#34;:&#34;9.890110&#34;},{&#34;1&#34;:&#34;c&#34;,&#34;2&#34;:&#34;8&#34;,&#34;3&#34;:&#34;8&#34;,&#34;4&#34;:&#34;8.791209&#34;},{&#34;1&#34;:&#34;c&#34;,&#34;2&#34;:&#34;9&#34;,&#34;3&#34;:&#34;7&#34;,&#34;4&#34;:&#34;7.692308&#34;},{&#34;1&#34;:&#34;c&#34;,&#34;2&#34;:&#34;10&#34;,&#34;3&#34;:&#34;11&#34;,&#34;4&#34;:&#34;12.087912&#34;},{&#34;1&#34;:&#34;d&#34;,&#34;2&#34;:&#34;1&#34;,&#34;3&#34;:&#34;10&#34;,&#34;4&#34;:&#34;10.869565&#34;},{&#34;1&#34;:&#34;d&#34;,&#34;2&#34;:&#34;2&#34;,&#34;3&#34;:&#34;14&#34;,&#34;4&#34;:&#34;15.217391&#34;},{&#34;1&#34;:&#34;d&#34;,&#34;2&#34;:&#34;3&#34;,&#34;3&#34;:&#34;10&#34;,&#34;4&#34;:&#34;10.869565&#34;},{&#34;1&#34;:&#34;d&#34;,&#34;2&#34;:&#34;4&#34;,&#34;3&#34;:&#34;12&#34;,&#34;4&#34;:&#34;13.043478&#34;},{&#34;1&#34;:&#34;d&#34;,&#34;2&#34;:&#34;5&#34;,&#34;3&#34;:&#34;9&#34;,&#34;4&#34;:&#34;9.782609&#34;},{&#34;1&#34;:&#34;d&#34;,&#34;2&#34;:&#34;6&#34;,&#34;3&#34;:&#34;8&#34;,&#34;4&#34;:&#34;8.695652&#34;},{&#34;1&#34;:&#34;d&#34;,&#34;2&#34;:&#34;7&#34;,&#34;3&#34;:&#34;5&#34;,&#34;4&#34;:&#34;5.434783&#34;},{&#34;1&#34;:&#34;d&#34;,&#34;2&#34;:&#34;8&#34;,&#34;3&#34;:&#34;8&#34;,&#34;4&#34;:&#34;8.695652&#34;},{&#34;1&#34;:&#34;d&#34;,&#34;2&#34;:&#34;9&#34;,&#34;3&#34;:&#34;7&#34;,&#34;4&#34;:&#34;7.608696&#34;},{&#34;1&#34;:&#34;d&#34;,&#34;2&#34;:&#34;10&#34;,&#34;3&#34;:&#34;9&#34;,&#34;4&#34;:&#34;9.782609&#34;},{&#34;1&#34;:&#34;e&#34;,&#34;2&#34;:&#34;1&#34;,&#34;3&#34;:&#34;9&#34;,&#34;4&#34;:&#34;8.411215&#34;},{&#34;1&#34;:&#34;e&#34;,&#34;2&#34;:&#34;2&#34;,&#34;3&#34;:&#34;17&#34;,&#34;4&#34;:&#34;15.887850&#34;},{&#34;1&#34;:&#34;e&#34;,&#34;2&#34;:&#34;3&#34;,&#34;3&#34;:&#34;6&#34;,&#34;4&#34;:&#34;5.607477&#34;},{&#34;1&#34;:&#34;e&#34;,&#34;2&#34;:&#34;4&#34;,&#34;3&#34;:&#34;12&#34;,&#34;4&#34;:&#34;11.214953&#34;},{&#34;1&#34;:&#34;e&#34;,&#34;2&#34;:&#34;5&#34;,&#34;3&#34;:&#34;8&#34;,&#34;4&#34;:&#34;7.476636&#34;},{&#34;1&#34;:&#34;e&#34;,&#34;2&#34;:&#34;6&#34;,&#34;3&#34;:&#34;6&#34;,&#34;4&#34;:&#34;5.607477&#34;},{&#34;1&#34;:&#34;e&#34;,&#34;2&#34;:&#34;7&#34;,&#34;3&#34;:&#34;10&#34;,&#34;4&#34;:&#34;9.345794&#34;},{&#34;1&#34;:&#34;e&#34;,&#34;2&#34;:&#34;8&#34;,&#34;3&#34;:&#34;9&#34;,&#34;4&#34;:&#34;8.411215&#34;},{&#34;1&#34;:&#34;e&#34;,&#34;2&#34;:&#34;9&#34;,&#34;3&#34;:&#34;15&#34;,&#34;4&#34;:&#34;14.018692&#34;},{&#34;1&#34;:&#34;e&#34;,&#34;2&#34;:&#34;10&#34;,&#34;3&#34;:&#34;15&#34;,&#34;4&#34;:&#34;14.018692&#34;}],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Why this is failing?</title>
      <link>/publication/problem-3/</link>
      <pubDate>Wed, 16 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/publication/problem-3/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/pagedtable/css/pagedtable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/pagedtable/js/pagedtable.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;This is a pretty typical issue. Specially when you have dealing with data a long time you just stop seeing obvious things, and you just can´t find solution to inexistant problems. For instance:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars %&amp;gt;%
  filter(cyl &amp;lt; 4)&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
&lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;mpg&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;cyl&#34;],&#34;name&#34;:[2],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;disp&#34;],&#34;name&#34;:[3],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;hp&#34;],&#34;name&#34;:[4],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;drat&#34;],&#34;name&#34;:[5],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;wt&#34;],&#34;name&#34;:[6],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;qsec&#34;],&#34;name&#34;:[7],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;vs&#34;],&#34;name&#34;:[8],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;am&#34;],&#34;name&#34;:[9],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;gear&#34;],&#34;name&#34;:[10],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;carb&#34;],&#34;name&#34;:[11],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]}],&#34;data&#34;:[],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;p&gt;You want to get the rows having cyl less or equal to 4 and for quite a while you keep getting 0 results.
Obviously something is wrong with the code but you just can´t notice it.&lt;/p&gt;
&lt;div id=&#34;the-solution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Solution&lt;/h2&gt;
&lt;p&gt;Well tidylog can give you an idea. Just load tidylog and watch:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#loading the package this way to avoid verbose messages
library(tidylog)
mtcars %&amp;gt;%
  filter(cyl &amp;lt; 4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## filter: removed all rows (100%)&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
&lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;mpg&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;cyl&#34;],&#34;name&#34;:[2],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;disp&#34;],&#34;name&#34;:[3],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;hp&#34;],&#34;name&#34;:[4],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;drat&#34;],&#34;name&#34;:[5],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;wt&#34;],&#34;name&#34;:[6],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;qsec&#34;],&#34;name&#34;:[7],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;vs&#34;],&#34;name&#34;:[8],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;am&#34;],&#34;name&#34;:[9],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;gear&#34;],&#34;name&#34;:[10],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;carb&#34;],&#34;name&#34;:[11],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]}],&#34;data&#34;:[],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;p&gt;Tidylog produces short log messages for dplyr and tidyr operations that help you understand what is happening with the data. Here definitely filter is incorrect, not producing an error but removing the 100% of the data, that is not what I was looking for.&lt;/p&gt;
&lt;p&gt;Everytime you build a pipeline, tidylog will tell what is happening:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars %&amp;gt;%
  filter(cyl &amp;gt; 4) %&amp;gt;%
  select(-disp) %&amp;gt;%
  mutate( overall = rowMeans(.)) %&amp;gt;%
  summarize_all( ~ mean(.))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## filter: removed 11 rows (34%), 21 rows remaining&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## select: dropped one variable (disp)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## mutate: new variable &amp;#39;overall&amp;#39; with 21 unique values and 0% NA&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## summarize_all: now one row and 11 columns, ungrouped&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
&lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;mpg&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;cyl&#34;],&#34;name&#34;:[2],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;hp&#34;],&#34;name&#34;:[3],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;drat&#34;],&#34;name&#34;:[4],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;wt&#34;],&#34;name&#34;:[5],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;qsec&#34;],&#34;name&#34;:[6],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;vs&#34;],&#34;name&#34;:[7],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;am&#34;],&#34;name&#34;:[8],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;gear&#34;],&#34;name&#34;:[9],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;carb&#34;],&#34;name&#34;:[10],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;overall&#34;],&#34;name&#34;:[11],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]}],&#34;data&#34;:[{&#34;1&#34;:&#34;16.64762&#34;,&#34;2&#34;:&#34;7.333333&#34;,&#34;3&#34;:&#34;180.2381&#34;,&#34;4&#34;:&#34;3.348095&#34;,&#34;5&#34;:&#34;3.70519&#34;,&#34;6&#34;:&#34;17.17381&#34;,&#34;7&#34;:&#34;0.1904762&#34;,&#34;8&#34;:&#34;0.2380952&#34;,&#34;9&#34;:&#34;3.47619&#34;,&#34;10&#34;:&#34;3.47619&#34;,&#34;11&#34;:&#34;23.58271&#34;}],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
