[{"authors":["admin"],"categories":null,"content":"I´m a Civil Engineer, but I got to know Data Science and I just loved it. I´ve worked the last 5 years as an R and Python developer, involved in almost every aspect of the Data Science Workflow.\nI love teaching and I aspire to eventually teach I\u0026rsquo;m currently teaching Intro to Python, Data Science Fundamentals and Machine Learning at Academia Desafio Latam to help Data Scientist around the world in Chile to get better.\nIn my spare time I love Drumming, playing Table Tennis, solving the Rubik´s Cube and spending time with my beautiful wife Valentina.\n","date":1595219758,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1595219758,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":"I´m a Civil Engineer, but I got to know Data Science and I just loved it. I´ve worked the last 5 years as an R and Python developer, involved in almost every aspect of the Data Science Workflow.\nI love teaching and I aspire to eventually teach I\u0026rsquo;m currently teaching Intro to Python, Data Science Fundamentals and Machine Learning at Academia Desafio Latam to help Data Scientist around the world in Chile to get better.","tags":null,"title":"Alfonso Tobar","type":"author"},{"authors":null,"categories":null,"content":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.main]] menu links to it in the config.toml.\n","date":1536462000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1553404091,"objectID":"c3224f3a64174f08aaf31e1f1d16ffd3","permalink":"/tutorial/","publishdate":"2018-09-09T00:00:00-03:00","relpermalink":"/tutorial/","section":"tutorial","summary":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.","tags":null,"title":"Overview","type":"docs"},{"authors":["Alfonso Tobar"],"categories":["Quick Solves"],"content":" Naive bayes The other day I had to prepare a class showing the benefits of using Naive Bayes. I have to say this is not a super powerful model, mainly because it makes assumptions that are most of the time not true. Nevertheless, I noticed this can be an excellent way to create a baseline model. It is easy, not very complicated to implement and the best thing is that is super fast.\nAs you may know, this model is based on the Bayes Theorem, namely:\n$$P[B/A] = \\frac{P[A / B] \\cdot P[B]}{P[A]}$$\nI´m not gonna demonstrate why this happens (basically because I don´t know how), but this models is based on probabilities that are calculated out of the dataset itself. In order to assign a class the class is calculated as:\n$$y = k = argmax\\, P[y = k] \\cdot \\prod{}_{i = 1}^p P[X_i/y = k]$$\nWhere the class is denoted by the maximum probability (between the different classes) of the product of the a priori probability of y and the different likelihood of Variables $X_i$ given y = k.\nThis example will be implemented using a lyrics dataset that I found here. Shoutouts to Hitesh Yalamanchili for making this data available.\nSo this naive Bayes model will be implemented in Python trying to Predict what genre a song belongs to by using its lyrics. So the implementation in Python looks like this:\nImporting Data When trying to import the data I noticed this has the following form:\nFor some reason there is a duplicated Index. In order to avoid a weird Unnamed: 0 column I had to use names argument in pd.read_csv() to declare the actual column names to import. Even by doing that the DataFrame was imported as a double Index dataset so I had to remove one of the index using .reset_index().\n Note: In order to make the dataset manageable for demonstration purposes only I decided to use only four genres: Rock, Pop, Hip-Hop and Metal.\n %%time import pandas as pd df = pd.read_csv('english_cleaned_lyrics.csv', header = 0, names = ['song','year','artist','genre', 'lyrics'], index_col = None).reset_index(level = 1, drop = True) df.query('genre in [\u0026quot;Rock\u0026quot;,\u0026quot;Pop\u0026quot;,\u0026quot;Hip-Hop\u0026quot;,\u0026quot;Metal\u0026quot;]', inplace = True) df  Wall time: 3.32 s   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    song year artist genre lyrics     0 ego-remix 2009 beyonce-knowles Pop Oh baby how you doing You know I'm gonna cut r...   1 then-tell-me 2009 beyonce-knowles Pop playin everything so easy it's like you seem s...   2 honesty 2009 beyonce-knowles Pop If you search For tenderness It isn't hard to ...   3 you-are-my-rock 2009 beyonce-knowles Pop Oh oh oh I oh oh oh I If I wrote a book about ...   4 black-culture 2009 beyonce-knowles Pop Party the people the people the party it's pop...   ... ... ... ... ... ...   362210 photographs-you-are-taking-now 2014 damon-albarn Pop When the photographs you're taking now Are tak...   362211 you-and-me 2014 damon-albarn Pop I met Moko jumbie He walks on stilts through a...   362212 hollow-ponds 2014 damon-albarn Pop Chill on the hollow ponds Set sail by a kid In...   362213 the-selfish-giant 2014 damon-albarn Pop Celebrate the passing drugs Put them on the ba...   362214 hostiles 2014 damon-albarn Pop When the serve is done And the parish shuffled...    178054 rows × 5 columns\n Feature Extraction In this step, we´ll use the CountVectorizer() class to provide a Occurrence Matrix. In this Matrix every row will be a Document, in this case a song, whereas every column is a Word. If a word ocurrs in the Document the is denoted by a 1. The only processing to the data is stopwords removal, that is removing all the words that are too common that end up adding noise to the analysis.\nWe\u0026rsquo;ll then use the word ocurrences as predictors for the genre. The predictor will look like this:\n%%time from sklearn.feature_extraction.text import CountVectorizer c_vec = CountVectorizer(stop_words = 'english', max_features = 20000) ## I´m removing english stopwords, and setting the max number of predictors to 20000 to avoid my computer to crush. vectorizer = c_vec.fit_transform(df['lyrics']) # Transform output into pandas Df for visualization pd.DataFrame(vectorizer.toarray(), columns = c_vec.get_feature_names())  Wall time: 35.6 s   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    00 000 02 03 05 06 07 09 10 100 ... zones zonin zoo zoom zoomin zoovie zoovier zoowap zu zulu     0 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0   1 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0   2 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0   3 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0   4 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0   ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ...   178049 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0   178050 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0   178051 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0   178052 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0   178053 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0    178054 rows × 20000 columns\n Setting up the Model The model is usper easy to set up. We just need to import MultinomialNB since this is a multiclass Prediction Model. Additionally, we´ll import train_test_split() to split the data into train and test, Pipeline() to create the Model Pipeline (the steps to come up with the model) and classification_report() to measure model performance.\nfrom sklearn.naive_bayes import MultinomialNB from sklearn.model_selection import train_test_split from sklearn.pipeline import Pipeline from sklearn.metrics import confusion_matrix, classification_report  # X will be song lyrics and y is the genre. We´ll split the data using 40% for test purposes. X_train, X_test, y_train, y_test = train_test_split(df['lyrics'], df['genre'], test_size = 0.4, random_state = 123)  Then a pipeline will be set using 2 steps. The first one being the CountVectorizer() named \u0026lsquo;cv\u0026rsquo; and the MultinomialNB() model named \u0026lsquo;nb\u0026rsquo;:\n%%time text_clf = Pipeline(steps = [ ('cv', CountVectorizer(stop_words = 'english', max_features = 20000)), ('nb', MultinomialNB(alpha = 0.1)) ]) text_clf.fit(X_train, y_train) # fitting the Pipeline #predicting using the Test Set to measure performance y_pred = text_clf.predict(X_test)  Wall time: 26.4 s  The first thing to notice is that even having 178K rows and 20000 predictors the model fits in under 30 seconds. FAST!\nNow when it comes to results, it is not a terrible model, it has a 63% of accuracy and the Macro F1 is 62%. Not bad for just using a couple of lines of code.\nprint(classification_report(y_test,y_pred))   precision recall f1-score support Hip-Hop 0.72 0.77 0.74 9062 Metal 0.48 0.75 0.59 8551 Pop 0.42 0.53 0.47 13582 Rock 0.78 0.60 0.68 40027 accuracy 0.63 71222 macro avg 0.60 0.66 0.62 71222 weighted avg 0.66 0.63 0.63 71222  Improving the Model In order to improve the model, we could run a GridSearch trying to play around with the alpha smoothing parameter that NB has. By adjusting this parameter correctly we could easily improve a bit the model without too much effort.\nIn this case we´ll run a Grid using values from 0 to 1, as shown below. Additionally we´ll use the \u0026lsquo;f1_macro\u0026rsquo; as the metric to choose the best model using a 5-Fold Cross Validation.\n%%time from sklearn import set_config from sklearn.model_selection import GridSearchCV from sklearn.metrics import precision_score, make_scorer #Grilla de parámetros a buscar parameters = {'nb__alpha': [0, 0.001, 0.01, 0.1, 0.5, 1] } text_clf = Pipeline(steps = [ ('cv', CountVectorizer(stop_words = 'english')), ('nb', MultinomialNB()) ]) searchCV = GridSearchCV(text_clf, parameters, n_jobs = -1, scoring = 'f1_macro', cv = 5) # 5 Fold CV optimizando el modelo por f1 macro searchCV.fit(X_train, y_train)  Wall time: 3min 27s GridSearchCV(cv=5, estimator=Pipeline(steps=[('cv', CountVectorizer(stop_words='english')), ('nb', MultinomialNB())]), n_jobs=-1, param_grid={'nb__alpha': [0, 0.001, 0.01, 0.1, 0.5, 1]}, scoring='f1_macro')  The GridSearch takes around 3 minutes to run 6 models using 5-Fold CV. And we can inmediately notice, small improvements for the best model:\nbest_nb = searchCV.best_estimator_ # Extracting Best Model y_pred = best_nb.predict(X_test) # Predicting the Test Set print(classification_report(y_test,y_pred))   precision recall f1-score support Hip-Hop 0.73 0.77 0.75 9062 Metal 0.56 0.70 0.62 8551 Pop 0.45 0.49 0.47 13582 Rock 0.76 0.69 0.73 40027 accuracy 0.66 71222 macro avg 0.63 0.66 0.64 71222 weighted avg 0.67 0.66 0.67 71222  We can inmediately see that:\n Overall Accuracy improves 3%. F1 macro average improved 2%. The Rock category is the one that improves the most from 68 to 73%. There is a trade off, even though some classess improve we can see that Metal decreases, whereas Pop keep the same results.  Finally we can check that the best is achieved when using alpha equals to 1.\nbest_nb.named_steps.nb.get_params()  {'alpha': 1, 'class_prior': None, 'fit_prior': True}  This is just a short example on how to set up a baseline model. Hopefully this can be useful for you.\nSee you next time!!\n","date":1597798200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595219758,"objectID":"8583218b81922aab1466535fb8e2d579","permalink":"/publication/problem-11/","publishdate":"2020-08-19T00:50:00Z","relpermalink":"/publication/problem-11/","section":"publication","summary":"This is a short demo on how to implement a NB model in SKlearn","tags":["Naive Bayes","sklearn","Machine Learning Learning"],"title":"Using Naive Bayes as a Baseline Model","type":"publication"},{"authors":["Alfonso Tobar"],"categories":["Blog"],"content":" How to use MLflow I have to say that I love modeling, but at the same time it demands the best of me in terms of being organized. I´m a mess, and that definitely is not helpful when modeling. So I had to put lots of efforts on creating my own folder system that helps me to organize my code, my data and my outputs. But even having all of that there is something I couldn\u0026rsquo;t managed to organize and those are my experiments.\nWhen trying to come up with a model, is all about trial and error. You never know priorly what type of model, what type of preprocessing, feature engineering or whatever other hyperparameter will be the ones that will provide the best performing model. The thing is, how to organize your notebooks, and know exactly what combination of hyperparameters you used to get the best results.\nHere is where MLflow comes into play, providing a way to have everything organized into a nice UI. I have to say, I have never used MLflow before and at the same time I´ve never found a good tutorial that helps me understand in detail how this thing works.\nMy idea is to test some of its functionalities by myself and with time come up with a confortable workflow that help to organize my code and my models in a better way.\nSo first things first. Installing this thing was quite easy by using pip install mlflow. Then, by running mlflow ui into conda you will serve a UI in the following link: http://localhost:5000 that looks like this:\nI have no idea how to use it. So I will keep playing around with it. In order to test out this thing, I will again use the titanic dataset.\nimport pandas as pd import numpy as np df = pd.read_csv('train.csv') df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked Signing_date     0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 1911-05-17   1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.0 1 0 PC 17599 71.2833 C85 C 1911-07-23   2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 1911-09-08   3 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 0 113803 53.1000 C123 S 1911-06-26   4 5 0 3 Allen, Mr. William Henry male 35.0 0 0 373450 8.0500 NaN S 1911-10-25     Initially I will only Impute Nulls and will encode categories as ordinal numbers. This approach is simplistic and not necessarily the best but I just want to make a model work with this data.\ndf.isnull().sum()  PassengerId 0 Survived 0 Pclass 0 Name 0 Sex 0 Age 177 SibSp 0 Parch 0 Ticket 0 Fare 0 Cabin 687 Embarked 2 Signing_date 0 dtype: int64  According to this, I will impute Age with its mean. I will drop Cabin and Signing_date, and Impute Embarked with its Mode.\n Note: Signing_date is a fake variable I created for other side project I have. Please disregard.\n mean_age = df.Age.mean() mode_embarked = df.Embarked.mode() mean_fare = df.Fare.mean() import category_encoders as ce def make_data_ready(data): result = (data.fillna(value = {'Age': mean_age, 'Embarked': mode_embarked, 'Fare': mean_fare}) .drop(columns = ['Cabin','Signing_date'], errors = 'ignore') .set_index('PassengerId') ) ord = ce.OrdinalEncoder() out = ord.fit_transform(result) return out df = make_data_ready(df) df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    Survived Pclass Name Sex Age SibSp Parch Ticket Fare Embarked   PassengerId               1 0 3 1 1 22.0 1 0 1 7.2500 1   2 1 1 2 2 38.0 1 0 2 71.2833 2   3 1 3 3 2 26.0 0 0 3 7.9250 1   4 1 1 4 2 35.0 1 0 4 53.1000 1   5 0 3 5 1 35.0 0 0 5 8.0500 1     Then, the only thing that´s left is splitting the data into Train and validation. Again, simple approach just to show more MLflow features.\nfrom sklearn.model_selection import train_test_split X_train, X_val, y_train, y_val = train_test_split(df.drop(columns = 'Survived'), df.Survived, test_size = 0.25, random_state = 123)  Time to Model After spending more than 3 hours reading the documentation because some things didn´t work as expected (There is a high chance that I´m not smart enough and I couldn\u0026rsquo;t understand the really well organized documentation I managed to figure out how to make this thing to work. So the first recommended thing is to create an experiment. This can be done directly in the UI by clicing the + sign in the top left corner, which is nice:\nOr it can be done by commands, that is my prefered choice:\nimport mlflow mlflow.set_experiment(experiment_name = 'New Experiment')  INFO: 'New Experiment' does not exist. Creating a new experiment  This command is quite nice, because it will create a experiment in case it doesn\u0026rsquo;t exist, or it will set the experiment as your active experiment in case it exists. So even though there is a .create_experiment() function, I prefer this.\nThen the logic MLflow has is super straight forward. Once you have an experiment you open a Run, the best way to this is using mlflow.start_run() as a context manager. That way once the indented block is done the run automatically closes, no need to call mlflow.end_run().\n mlflow is the high level API which helps making easier, but some of tits functionalities can be little bit cumbersome when you have no experience, like in my case. The pros about this API is that all of the IDs will be created automatically, which is good since you\u0026rsquo;ll not be overwriting things if you forgot to change an ID, but the cons are that IDs are extremely not human readable and hard to access. In case you´ll want to control everything manually you\u0026rsquo;ll have to go to the mlflow.tracking which is low level.\n Ok, so once this is clear the logic is easy, in an open run you can log things, such as:\n parameters: Model Hyperparameters or anything you want to track from your current notebook. Normally these things are given. metrics: This is more related to the model and are values that are measurable, such as performance metrics. artifacts: It can be any file you want to attach to your model. It can be your data, it can be a chart, images, etc. models: This is the model as such, that will be serialized into a .pkl file. This functionality will have a separate API for every model flavor such as: mlflow.sklearn for sklearn models, mlflow.xgboost for xgboost models and you get the idea.   In this example, I\u0026rsquo;ll run a simple Logistic Regression, in which I will like to save some parameters such as: solver, C, and max_iter. I will open 3 Runs:\nfrom sklearn.linear_model import LogisticRegression import mlflow import mlflow.sklearn C = 1.5 max_iter = 1000 name = 'Run 1' with mlflow.start_run(run_name = name) as run: lr = LogisticRegression(solver = 'lbfgs', random_state = 123, max_iter = max_iter, C = C) lr.fit(X_train,y_train) acc = lr.score(X_val,y_val) mlflow.log_param(\u0026quot;run_id\u0026quot;, run.info.run_id) mlflow.log_param(\u0026quot;solver\u0026quot;, 'lbfgs') mlflow.log_param(\u0026quot;max_iter\u0026quot;, max_iter) mlflow.log_param(\u0026quot;C\u0026quot;, C) mlflow.log_metric(\u0026quot;Accuracy\u0026quot;, acc) mlflow.sklearn.log_model(lr, name)  The code above was run several times, and the results can be seen in the UI as follows:\n4 Runs were run. Every run automatically logs the Startting time. Right next to it there is a green or red icon indicating if the run was succesful or not. Also the Run can be logged with a Run Name which is optional, but I recommend it, because it is the only way to recognize what is your Run about. You will see that all the parameters using the .log_param() are there as well as the metrics logged with .log_metric(). One thing to note is that the Run name can be repeated because the unique identifier of every run is the run_id which is automatically created.\nI found this run_id is particularly difficult to get with the current API, and define it manually carries some other issues I don\u0026rsquo;t want to deal with. That is why is super important that when running the Run you add mlflow.log_param(\u0026quot;run_id\u0026quot;, run.info.run_id) to store the run id. This will be really helpful to get access to other functionalities of the UI.\nNow, if you click in one of the Runs you go to another View like this:\nIn this first part you\u0026rsquo;ll get the logged parameters plus Info regarding the start run, the duration of the run, which is quite nice to avoid the %%time magic commands, and the status.\nAnd there is another interesting part that shows the metrics and the artifacts. In our case we have only logged the model that as you can see is stored as a .pkl file.\nThere are some other commands that are quite handy to get access to the logged objects. The first one is mlflow.get_experiment_by_name(). I think this is important because It will provide the most important info about your experiment, namely the experiment_id. The format of this information is super complicated to deal with so I recommmend to convert it into a dictionary like this:\ndict(mlflow.get_experiment_by_name('New Experiment'))  {'artifact_location': 'file:///C:/Users/fata2810/OneDrive%20-%20Retail%20Financiero%20Scotiabank%20Cencosud/Clases%20Python/mlruns/0', 'experiment_id': '0', 'lifecycle_stage': 'active', 'name': 'New Experiment', 'tags': {}}  Some other important commands are based in the mlflow.tracking API such as .get_run() that will provide the info about the runs and .list_run_infos that will retrieve basically all the run_ids but in a really ugly way.\n Note: Always remember the experiment_id is a string, even if they look as an integer.\n th mlflow.tracking API works like this:\nfrom mlflow.tracking import MlflowClient client = MlflowClient() client.list_run_infos(experiment_id = '0')  [\u0026lt;RunInfo: artifact_uri='file:///C:/Users/fata2810/OneDrive%20-%20Retail%20Financiero%20Scotiabank%20Cencosud/Clases%20Python/mlruns/0/86ba898ad44049edb55203166cfab227/artifacts', end_time=1594022373954, experiment_id='0', lifecycle_stage='active', run_id='86ba898ad44049edb55203166cfab227', run_uuid='86ba898ad44049edb55203166cfab227', start_time=1594022373922, status='FAILED', user_id='FATA2810'\u0026gt;, \u0026lt;RunInfo: artifact_uri='file:///C:/Users/fata2810/OneDrive%20-%20Retail%20Financiero%20Scotiabank%20Cencosud/Clases%20Python/mlruns/0/cf34867bb1ee45bc9755446eba6e073e/artifacts', end_time=1594022347517, experiment_id='0', lifecycle_stage='active', run_id='cf34867bb1ee45bc9755446eba6e073e', run_uuid='cf34867bb1ee45bc9755446eba6e073e', start_time=1594022347331, status='FINISHED', user_id='FATA2810'\u0026gt;, \u0026lt;RunInfo: artifact_uri='file:///C:/Users/fata2810/OneDrive%20-%20Retail%20Financiero%20Scotiabank%20Cencosud/Clases%20Python/mlruns/0/d1534e5faf7642faa0f028bdaf68a6bd/artifacts', end_time=1594022336377, experiment_id='0', lifecycle_stage='active', run_id='d1534e5faf7642faa0f028bdaf68a6bd', run_uuid='d1534e5faf7642faa0f028bdaf68a6bd', start_time=1594022336288, status='FINISHED', user_id='FATA2810'\u0026gt;, \u0026lt;RunInfo: artifact_uri='file:///C:/Users/fata2810/OneDrive%20-%20Retail%20Financiero%20Scotiabank%20Cencosud/Clases%20Python/mlruns/0/c55159dcfe884e21b8c35f18168fdcde/artifacts', end_time=1594022312153, experiment_id='0', lifecycle_stage='active', run_id='c55159dcfe884e21b8c35f18168fdcde', run_uuid='c55159dcfe884e21b8c35f18168fdcde', start_time=1594022311962, status='FINISHED', user_id='FATA2810'\u0026gt;]  client.get_run(run_id = 'c55159dcfe884e21b8c35f18168fdcde')  \u0026lt;Run: data=\u0026lt;RunData: metrics={'Accuracy': 0.7937219730941704}, params={'C': '1.5', 'max_iter': '1000', 'run_id': 'c55159dcfe884e21b8c35f18168fdcde', 'solver': 'lbfgs'}, tags={'mlflow.log-model.history': '[{\u0026quot;run_id\u0026quot;: \u0026quot;c55159dcfe884e21b8c35f18168fdcde\u0026quot;, ' '\u0026quot;artifact_path\u0026quot;: \u0026quot;Run 1\u0026quot;, \u0026quot;utc_time_created\u0026quot;: ' '\u0026quot;2020-07-06 07:58:32.125378\u0026quot;, \u0026quot;flavors\u0026quot;: ' '{\u0026quot;python_function\u0026quot;: {\u0026quot;loader_module\u0026quot;: ' '\u0026quot;mlflow.sklearn\u0026quot;, \u0026quot;python_version\u0026quot;: \u0026quot;3.7.7\u0026quot;, ' '\u0026quot;data\u0026quot;: \u0026quot;model.pkl\u0026quot;, \u0026quot;env\u0026quot;: \u0026quot;conda.yaml\u0026quot;}, ' '\u0026quot;sklearn\u0026quot;: {\u0026quot;pickled_model\u0026quot;: \u0026quot;model.pkl\u0026quot;, ' '\u0026quot;sklearn_version\u0026quot;: \u0026quot;0.22.2.post1\u0026quot;, ' '\u0026quot;serialization_format\u0026quot;: \u0026quot;cloudpickle\u0026quot;}}}]', 'mlflow.runName': 'Run 1', 'mlflow.source.name': 'C:\\\\Users\\\\fata2810\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\envs\\\\MLprojects\\\\lib\\\\site-packages\\\\ipykernel_launcher.py', 'mlflow.source.type': 'LOCAL', 'mlflow.user': 'FATA2810'}\u0026gt;, info=\u0026lt;RunInfo: artifact_uri='file:///C:/Users/fata2810/OneDrive%20-%20Retail%20Financiero%20Scotiabank%20Cencosud/Clases%20Python/mlruns/0/c55159dcfe884e21b8c35f18168fdcde/artifacts', end_time=1594022312153, experiment_id='0', lifecycle_stage='active', run_id='c55159dcfe884e21b8c35f18168fdcde', run_uuid='c55159dcfe884e21b8c35f18168fdcde', start_time=1594022311962, status='FINISHED', user_id='FATA2810'\u0026gt;\u0026gt;  I think the most important feature that mlflow provides is the ability to store models as .pkl files and then be able to retrieve them. I think this is not very well explained in the documentation, and I wasted a lot of time understanding the correct way to this properly, so here it goes:\nFirst you need to import the flavor of your model, in my case a sklearn model, hence import mlflow.sklearn. Then the documentation describes the usage of mlflow.sklearn.load_models() with a URI in the following form:\n\u0026lsquo;runs://relative_path_to_models\u0026rsquo;.\nPut into easy words the URI works like this:\n The run id can be obtained from the UI or using the commands shown above. The relative path will be the information provided in the second argument of mlflow.sklearn.log_model(lr, name). This argument name, will create a folder with the same \u0026ldquo;name\u0026rdquo;. Remember that the variable name took the same value as the Run Name.  import mlflow.sklearn model_lr = mlflow.sklearn.load_model(f'runs:/c55159dcfe884e21b8c35f18168fdcde/Run 1') #Run 1 is the name of the first experiment model_lr  LogisticRegression(C=1.5, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=1000, multi_class='auto', n_jobs=None, penalty='l2', random_state=123, solver='lbfgs', tol=0.0001, verbose=0, warm_start=False)  As you can see, now the LR model was retrieved and can be used normally since it is loaded into the Python environment.\nThis was a brief introduction to MLflow. Even though the tool is super intuitive and easy to use it was really difficult to understand how to make it work because there are not many tutorials out there, and the docs although they are super well organized doesn\u0026rsquo;t provide code examples that for me are the best way to understand how this works.\nI will be uploading some other example with more advanced functionalities if I manage to understand them.\nBest,\n","date":1596761400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595219758,"objectID":"3f8bb77e7b143ae5eb74a836d49ebc46","permalink":"/post/mlflow/","publishdate":"2020-08-07T00:50:00Z","relpermalink":"/post/mlflow/","section":"post","summary":"This is a short tutorial on my deep dive on MLflow","tags":["MLflow","Machine Learning Learning"],"title":"How to use MLflow","type":"post"},{"authors":["Alfonso Tobar"],"categories":["Quick Solves"],"content":" Creating reusable plotting Functions In my new job I´ve noticed they like to explain variables impact into some target in the following way:\nNormally we have a natural Rate of an Event happening shown as this TN dashed line. And you have a particular variable that is splitted into Categories showing what is the specific Rate of the Event by Category.\nSince this is happening so often, I decided to build a simple function to avoid all the work behind the scenes. In order to show this, I will use the well-known Titanic dataset from Kaggle, which can be downloaded from here.\nSo first of all. let´s import the data:\nimport pandas as pd df = pd.read_csv('train.csv') df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked Signing_date     0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 1911-05-17   1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.0 1 0 PC 17599 71.2833 C85 C 1911-07-23   2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 1911-09-08   3 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 0 113803 53.1000 C123 S 1911-06-26   4 5 0 3 Allen, Mr. William Henry male 35.0 0 0 373450 8.0500 NaN S 1911-10-25     This dataset is intended to find survival rate of passengers depending of the features associated to them. In this case the Target variable is Survived. If we want to understand the Surviving rate we can just simply use .value_counts() to get those numbers.\ndf.Survived.value_counts(normalize = True)  0 0.616162 1 0.383838 Name: Survived, dtype: float64  We can notice that 61% of passengers did not survived, but is there any difference when we take subsets of the data? For example does the survival rate change when analizing by sex? We expect so:\ndf.groupby(['Sex']).Survived.value_counts(normalize = True)  Sex Survived female 1 0.742038 0 0.257962 male 0 0.811092 1 0.188908 Name: Survived, dtype: float64  Actually, when splitting by Sex we notice that 74% of women survived whereas only 18% of men survived. So is there a good way to plot this? So our aim will be to show only survival rate (because death rate is just the complement) and show how that compares to the Natural Survival Rate:\nThe Trick The first trick is that since Survived is a binary variable, it is possible to get the same survival rates, but only for the \u0026ldquo;1\u0026rdquo; doing the following:\ndf.groupby('Sex').Survived.mean()  Sex female 0.742038 male 0.188908 Name: Survived, dtype: float64  This can be easily plot using pandas.plot()\nimport matplotlib.pyplot as plt df.groupby('Sex').Survived.mean().plot(kind = 'bar') plt.show()  I will create the plot_rate_by() in order to make this interesting and be very flexible. Also this will add some other functionalities such as adding titles and allow this for any dataset:\ndef plot_rate_by(data,by,Target, TN, title, x, y, x_label = None, rot = 0): TN *=100 # converts to percentage # plots ading title, and optional label rotation ax = (data.groupby(by)[Target].mean()*100).plot(kind = 'bar', title = title, rot = rot) plt.axhline(TN, color = 'r', linestyle = '--') # adds dashed line # adds the red text box, in coordinates x and y to avoid overlapping plt.text(x,y,f'TN = {TN}%',bbox=dict(facecolor='red', alpha=0.5)) ax.set_xlabel(x_label) # optional Label for the x Axis return plt.show()  So now, it is just matter of tun the function with the parameters and that\u0026rsquo;s it.\nimport numpy as np # Natural Rate for survivors tn = np.round(df.Survived.value_counts(normalize = True).loc[1],3) plot_rate_by(df, by = 'Sex', Target = 'Survived', TN = tn, title = 'Survivors by Sex', x = 0.4, y = 40, x_label = None)  And it can be done for any categorical variable, for example, Pclass:\nplot_rate_by(df, by = 'Pclass', Target = 'Survived', TN = tn, title = 'Survivors by Pclass', x = 1.6, y = 40, x_label = None)  Another thing that normally happens at my job is that they want to do this with variables that are continous. This cannot be done directly, so it is necessary to create some binning. This can be done easily. Let´s say we want to use Fare. So what is the Survival rate for people paying cheaper tickets, let´s say =300. In order to this I will use numpy.select.\nI think numpy select is not very self explanatory. Basically works as a case when in which you have to define a condition list, boolean masks, and a choice list with vallues when condition is met.\ncondlist = [df.Fare \u0026lt; 10, df.Fare \u0026lt; 100, df.Fare \u0026lt; 300, df.Fare \u0026gt;= 300] # list of conditions choicelist = ['\u0026lt;10','\u0026lt;100','\u0026lt;300','\u0026gt;=300'] df['Fare_binning'] = np.select(condlist, choicelist) df[['Fare','Fare_binning']]   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    Fare Fare_binning     0 7.2500 \u0026lt;10   1 71.2833 \u0026lt;100   2 7.9250 \u0026lt;10   3 53.1000 \u0026lt;100   4 8.0500 \u0026lt;10   ... ... ...   886 13.0000 \u0026lt;100   887 30.0000 \u0026lt;100   888 23.4500 \u0026lt;100   889 30.0000 \u0026lt;100   890 7.7500 \u0026lt;10    891 rows × 2 columns\n Well normally you don\u0026rsquo;t know priorly what are good binnings to show your data so this type of operation needs to be extremely flexible. How can this transform into a function? Let´s say we have a list of tha boundary values:\nvals_l = [10,100,300]  A for loop can be used to use this values into conditions using .lt() method and creating the choices:\ncondlist = [] choicelist = [] #Doing this operation for every boundary for v in vals_l: condlist.append(df['Fare'].lt(v)) # data['Fare'].lt(v) is equivalent to data.Fare \u0026lt; v choicelist.append('\u0026lt;'+str(v)) #concatening boundary and the \u0026lt; sign  Additionally, it is necessary to add the last category for the \u0026gt;=. This can be done as:\ncondlist.append(df['Fare'].ge(vals_l[-1])) choicelist.append('\u0026gt;='+str(vals_l[-1])) choicelist  ['\u0026lt;10', '\u0026lt;100', '\u0026lt;300', '\u0026gt;=300']  It can be seen that all of the categories has been correctly created. Now putting all together into a function will look like this:\ndef convert_to_range(data, field, vals_l): vals_l = vals_l condlist = [] choicelist = [] for v in vals_l: condlist.append(data[field].lt(v)) choicelist.append('\u0026lt;'+str(v)) condlist.append(data[field].ge(vals_l[-1])) choicelist.append('\u0026gt;='+str(vals_l[-1])) return pd.Categorical(np.select(condlist, choicelist), categories=choicelist, ordered = True) #converts everything into a pandas categorical variable  df['binning_function'] = convert_to_range(df,'Fare',vals_l) df[['Fare_binning','binning_function']]   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    Fare_binning binning_function     0 \u0026lt;10 \u0026lt;10   1 \u0026lt;100 \u0026lt;100   2 \u0026lt;10 \u0026lt;10   3 \u0026lt;100 \u0026lt;100   4 \u0026lt;10 \u0026lt;10   ... ... ...   886 \u0026lt;100 \u0026lt;100   887 \u0026lt;100 \u0026lt;100   888 \u0026lt;100 \u0026lt;100   889 \u0026lt;100 \u0026lt;100   890 \u0026lt;10 \u0026lt;10    891 rows × 2 columns\n We can check that results are identical, and now this can be easily applied to any continuos variable. The lesson here is, I´m lazy I don´t want to things everytime, so I do it well once and then I recycle the functions.\nNow the plot_rate_by() function can be used:\nplot_rate_by(df, by = 'binning_function', Target = 'Survived', TN = tn, title = 'Survivors by Fare Categories', x = -0.3, y = 42, x_label = None)  Hope you like this. And can be useful to understand that being lazy is good if this makes you a recycler. Not sure if this sounds OK but anyways.\n","date":1593910200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593927393,"objectID":"5d26d97fc1b169d91b69f66c868ebb6c","permalink":"/publication/problem-10/","publishdate":"2020-07-05T00:50:00Z","relpermalink":"/publication/problem-10/","section":"publication","summary":"Creating Plotting functions for my daily work","tags":["Quick Solves","Pandas","Plotting"],"title":"Reusable Plotting Functions in Python","type":"publication"},{"authors":["Alfonso Tobar"],"categories":["Quick Solves"],"content":"\r\rThe Problem\rWe have an Intern working on his Thesis Project in our office. He needed to calculate Customer recency, meaning he needed to know the amount of months since the last time the Customer made a Purchase. This was quite intriguing to me because it needs to combine some windows scoped functions with group by and some other things.\nThis is the problem with the expected solution:\n(data \u0026lt;- tibble::tribble(\r~Client_ID, ~Date_ID, ~Purchase_Amount, ~Recency,\r1, 1, 2344, 0,\r1, 2, 0, 1,\r1, 3, 0, 2,\r1, 4, 5676, 0,\r1, 5, 4587, 0,\r1, 6, 0, 1,\r1, 7, 0, 2,\r1, 8, 0, 3,\r2, 1, 2500, 0,\r2, 2, 2634, 0,\r2, 3, 0, 1,\r2, 4, 0, 2,\r2, 5, 0, 3,\r2, 6, 4578, 0,\r2, 7, 4562, 0,\r2, 8, 0, 1\r)\r)\r{\"columns\":[{\"label\":[\"Client_ID\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Date_ID\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Purchase_Amount\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Recency\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1\",\"2\":\"1\",\"3\":\"2344\",\"4\":\"0\"},{\"1\":\"1\",\"2\":\"2\",\"3\":\"0\",\"4\":\"1\"},{\"1\":\"1\",\"2\":\"3\",\"3\":\"0\",\"4\":\"2\"},{\"1\":\"1\",\"2\":\"4\",\"3\":\"5676\",\"4\":\"0\"},{\"1\":\"1\",\"2\":\"5\",\"3\":\"4587\",\"4\":\"0\"},{\"1\":\"1\",\"2\":\"6\",\"3\":\"0\",\"4\":\"1\"},{\"1\":\"1\",\"2\":\"7\",\"3\":\"0\",\"4\":\"2\"},{\"1\":\"1\",\"2\":\"8\",\"3\":\"0\",\"4\":\"3\"},{\"1\":\"2\",\"2\":\"1\",\"3\":\"2500\",\"4\":\"0\"},{\"1\":\"2\",\"2\":\"2\",\"3\":\"2634\",\"4\":\"0\"},{\"1\":\"2\",\"2\":\"3\",\"3\":\"0\",\"4\":\"1\"},{\"1\":\"2\",\"2\":\"4\",\"3\":\"0\",\"4\":\"2\"},{\"1\":\"2\",\"2\":\"5\",\"3\":\"0\",\"4\":\"3\"},{\"1\":\"2\",\"2\":\"6\",\"3\":\"4578\",\"4\":\"0\"},{\"1\":\"2\",\"2\":\"7\",\"3\":\"4562\",\"4\":\"0\"},{\"1\":\"2\",\"2\":\"8\",\"3\":\"0\",\"4\":\"1\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\rAs you may see every time I have a Purchase the counter needs to restart at 0 and then start counting how many dates have passed since the last purchase. Aditionally the counter needs to restart for new Customers.\n\rThe solution\rIn this particular case I will detail the different steps of the solution because it can be tricky to get.\nFirst I will create an auxiliary variable called has_purchased and a date_group. These variables need to be created at the client level, in order to make this easier I will use Client_ID 1 for demonstration purposes:\ndata %\u0026gt;%\rfilter(Client_ID == 1) %\u0026gt;% mutate(has_purchased = as.numeric(Purchase_Amount \u0026gt; 0),\rdate_group = cumsum(has_purchased))\r{\"columns\":[{\"label\":[\"Client_ID\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Date_ID\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Purchase_Amount\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Recency\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"has_purchased\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"date_group\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1\",\"2\":\"1\",\"3\":\"2344\",\"4\":\"0\",\"5\":\"1\",\"6\":\"1\"},{\"1\":\"1\",\"2\":\"2\",\"3\":\"0\",\"4\":\"1\",\"5\":\"0\",\"6\":\"1\"},{\"1\":\"1\",\"2\":\"3\",\"3\":\"0\",\"4\":\"2\",\"5\":\"0\",\"6\":\"1\"},{\"1\":\"1\",\"2\":\"4\",\"3\":\"5676\",\"4\":\"0\",\"5\":\"1\",\"6\":\"2\"},{\"1\":\"1\",\"2\":\"5\",\"3\":\"4587\",\"4\":\"0\",\"5\":\"1\",\"6\":\"3\"},{\"1\":\"1\",\"2\":\"6\",\"3\":\"0\",\"4\":\"1\",\"5\":\"0\",\"6\":\"3\"},{\"1\":\"1\",\"2\":\"7\",\"3\":\"0\",\"4\":\"2\",\"5\":\"0\",\"6\":\"3\"},{\"1\":\"1\",\"2\":\"8\",\"3\":\"0\",\"4\":\"3\",\"5\":\"0\",\"6\":\"3\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\rNoticed that using the cumsum() function we can create groups in which the recency needs to restart. Everytime we change the date_group recency needs to come back to 0.\nThen we can calculate the row_number by group and substract 1 and that’s it.\ndata %\u0026gt;%\rfilter(Client_ID == 1) %\u0026gt;%\rmutate(\rhas_purchased = as.numeric(Purchase_Amount \u0026gt; 0),\rdate_group = cumsum(has_purchased)\r) %\u0026gt;%\rgroup_by(date_group) %\u0026gt;%\rmutate(calculated_recency = row_number() - 1)\r{\"columns\":[{\"label\":[\"Client_ID\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Date_ID\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Purchase_Amount\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Recency\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"has_purchased\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"date_group\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"calculated_recency\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1\",\"2\":\"1\",\"3\":\"2344\",\"4\":\"0\",\"5\":\"1\",\"6\":\"1\",\"7\":\"0\"},{\"1\":\"1\",\"2\":\"2\",\"3\":\"0\",\"4\":\"1\",\"5\":\"0\",\"6\":\"1\",\"7\":\"1\"},{\"1\":\"1\",\"2\":\"3\",\"3\":\"0\",\"4\":\"2\",\"5\":\"0\",\"6\":\"1\",\"7\":\"2\"},{\"1\":\"1\",\"2\":\"4\",\"3\":\"5676\",\"4\":\"0\",\"5\":\"1\",\"6\":\"2\",\"7\":\"0\"},{\"1\":\"1\",\"2\":\"5\",\"3\":\"4587\",\"4\":\"0\",\"5\":\"1\",\"6\":\"3\",\"7\":\"0\"},{\"1\":\"1\",\"2\":\"6\",\"3\":\"0\",\"4\":\"1\",\"5\":\"0\",\"6\":\"3\",\"7\":\"1\"},{\"1\":\"1\",\"2\":\"7\",\"3\":\"0\",\"4\":\"2\",\"5\":\"0\",\"6\":\"3\",\"7\":\"2\"},{\"1\":\"1\",\"2\":\"8\",\"3\":\"0\",\"4\":\"3\",\"5\":\"0\",\"6\":\"3\",\"7\":\"3\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\rNow, in order to make this computationally efficient and generalize this solution to all of the customers we need to apply this by Client_ID. The way in which we’ll do this is by using the group_modify() function.\rThis function works very similarly to purrr’s maps but applied to grouped data. The final solution looks like this:\ndata %\u0026gt;%\r#grouped by client\rgroup_by(Client_ID) %\u0026gt;%\rgroup_modify(\r#This is the same pipeline showed before but applied to element .x that represents each group\r~ .x %\u0026gt;%\rmutate(\rhas_purchased = as.numeric(Purchase_Amount \u0026gt; 0),\rdate_group = cumsum(has_purchased)\r) %\u0026gt;%\rgroup_by(date_group) %\u0026gt;%\rmutate(calculated_recency = row_number() - 1)\r) %\u0026gt;%\rselect(-has_purchased, -date_group)\r{\"columns\":[{\"label\":[\"Client_ID\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Date_ID\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Purchase_Amount\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Recency\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"calculated_recency\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1\",\"2\":\"1\",\"3\":\"2344\",\"4\":\"0\",\"5\":\"0\"},{\"1\":\"1\",\"2\":\"2\",\"3\":\"0\",\"4\":\"1\",\"5\":\"1\"},{\"1\":\"1\",\"2\":\"3\",\"3\":\"0\",\"4\":\"2\",\"5\":\"2\"},{\"1\":\"1\",\"2\":\"4\",\"3\":\"5676\",\"4\":\"0\",\"5\":\"0\"},{\"1\":\"1\",\"2\":\"5\",\"3\":\"4587\",\"4\":\"0\",\"5\":\"0\"},{\"1\":\"1\",\"2\":\"6\",\"3\":\"0\",\"4\":\"1\",\"5\":\"1\"},{\"1\":\"1\",\"2\":\"7\",\"3\":\"0\",\"4\":\"2\",\"5\":\"2\"},{\"1\":\"1\",\"2\":\"8\",\"3\":\"0\",\"4\":\"3\",\"5\":\"3\"},{\"1\":\"2\",\"2\":\"1\",\"3\":\"2500\",\"4\":\"0\",\"5\":\"0\"},{\"1\":\"2\",\"2\":\"2\",\"3\":\"2634\",\"4\":\"0\",\"5\":\"0\"},{\"1\":\"2\",\"2\":\"3\",\"3\":\"0\",\"4\":\"1\",\"5\":\"1\"},{\"1\":\"2\",\"2\":\"4\",\"3\":\"0\",\"4\":\"2\",\"5\":\"2\"},{\"1\":\"2\",\"2\":\"5\",\"3\":\"0\",\"4\":\"3\",\"5\":\"3\"},{\"1\":\"2\",\"2\":\"6\",\"3\":\"4578\",\"4\":\"0\",\"5\":\"0\"},{\"1\":\"2\",\"2\":\"7\",\"3\":\"4562\",\"4\":\"0\",\"5\":\"0\"},{\"1\":\"2\",\"2\":\"8\",\"3\":\"0\",\"4\":\"1\",\"5\":\"1\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\rYou’ll notice calculated_recency matches with the expected results proving our solution works and a complicated calculation can be easily done using some group_by statements.\nI think the beauty of this solution is that we only used vectorized functions without applying any loop to run through the data by Client and by date_group which normally would take 2 nested for loops.\n\r","date":1580774400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580786696,"objectID":"42ba9dd37e826bcd5dd00adfd7107b2c","permalink":"/publication/problem-9/","publishdate":"2020-02-04T00:00:00Z","relpermalink":"/publication/problem-9/","section":"publication","summary":"The idea here is to leverage dplyr power to calculate Customer recency.","tags":["Quick Solves","dplyr","Recency"],"title":"Calculating Recency using dplyr","type":"publication"},{"authors":["Alfonso Tobar"],"categories":["Blog","Rambling"],"content":"\rDuring last blog post I shared my bad experiences with R in these late days. This doesn’t mean I hate R at all, but it does mean that I’m eagerly learning Python to supply some of the R deficencies.\nIn this 3 weeks of intensive Python learning I’ve learned a lot and here are some things on why moving to Python.\nThe main advantage of the Python Data science environment is that you can find everything condesnsed into the Scipy stack. With just 5 packages you can perform most of the data scientist tasks out there.\nScikit - Learn\rRemember that I had my favorite package in R? Well I have to say this is my favorite in Python. I have never worked before with such a complete package. It is so well organized and documented. I have to say that just by reading its documentation and the API I learned tons of Machine Learning.\nIt is true that R is quite biased towards statistics. This bias is quite noticeable when you discover a lot of new techniques that I never found in any other R package. Sklearn includes everything: Data spliting functions, tons of models or estimators (using sklearn terminology), a lot of validation strategies, Grid Search strategies, metrics, pre-processing, calibration helpers, unsupervised algorithms, ensembles algorithms and so many complementary packages that makes this framework the best one among all of the Machine Learning languages.\nI think one advantage that Python has in here is that the creators of this package took this so seriously, they even have acceptation criteria to add new models into the framework that makes this package incredible stable and the state of art when it comes to modeling. Compared to R that has all of the models segregated into many different packages, plus, not having an stable unified interface and not having a lot of support for unsupervised algorithms and ensembles, I have to say Python here is way superior.\nI have to say that in this kind of problems the object oriented programming shines and makes Python really delightful. Cool things I have noticed:\n\rThe parallel interface using n_jobs combined with joblib is so smooth and well implemented.\n\rThe memory management of Python makes it so suitable for Production and to run large models in limited memory environments.\n\rThe creators are really focused in the package scope and they are not trying to aim to everything out there, but I would say 90% of the funtions implemented in sklearn work excellent.\n\rThe package is implemented on top of Numpy that I think is the most powerful package in Data Science because of its performance and ease to be used.\n\rThe documentation is just beautiful and easy to get access.\n\rTons of tutorials and examples to get up and running with your model super fast.\n\rReady to run on top of a cluster and be combine with Spark to be scaled up.\n\r\rDoes this thing have any cons?\rOf course, even though it is easy to learn, at the beginning you run into errors all the time. And I would like to say it is because you need to get used to the API.\nSome cons that I have found so far:\n\rIt is quite documentation dependant. You need to work with sklearn.\n\rSince Python it is not a community of statisticians a lot of people makes opinions about how they use some of the sklearn features that some times you can get confused and you just don’t know who to trust to.\n\rEven though Pipelines are a very powerful tool to create models, I still prefer the recipes API, it is way cleaner and easy to learn. The pipeline funtions tends to be messy when combining list and tuples with the normal function parenthesis.\n\r\r\rOther than that, I think sklearn is perfect.\n\r\r\rMatplotlib\rI have to say I was very reluctant to use this library, because ggplot is powerful and easy to use. But once you get into this library you notice that is as easy to use as ggplot, but the graphics quality and the color palette are aesthetically superior. Something that I never liked about ggplot is the collor palette and the resolution, something is way resolved in matpotlib, and in my opinion when combined with pandas plotting becomes way easier.\nCharts comparison\rTODO\nBesides that, I like that matplotlib, pandas and numpy are fully integrated, so you don’t have compatibility issues, something that you do have when trying to combine matrices and ggplot in R for example.\n\rNot too much more to say about this but, pretty, simple and full compatible.\n\r\r\rpandas\rFor me this is the weakest link in the scipy stack. There is no any doubt about pandas capability and performance, but I feel (this is a personal opinion) that pandas doesn’t have an own identity. Pandas is always trying to emulate what dplyr does, and sometimes it is superior (except for the syntax) and even it has some properties that makes it unique like dealing with time series, the usage of index and the ability to transpose any dataframe, I accept it, I respet it, I use it but I don’t love it (yet).\nIn my opinion the biggest weakness is the excessive use of the apostrophe (’). I have to say I really miss R’s non-standard evaluation, and an equivalent for the filter function. I just don’t like to use boolean masks within square brackets to filter out some data.\n\rnumpy\rI think is the responsible of Python’s popularity. It can be that you don’t use it all the time. Actually I’m still learning how to use it. But its speed is abosolutely uncomparable, and all of the scipy stack is built on top of this library.\nI have to say my first Python’s impression were not good at all. I remember that we had a trainee implementing an algorithm in Python and we made all of the worst practice mistakes, making Python ridiculously bad performant. But after using the combination of all of these packages in the late weeks has been a pleasure.\nI won’t refer to scipy or seaborn that are some other popular packages that are complementary to these ones, basically because I haven’t used them.\nSince I’m using Python more regularly I will start uploading some use cases and issues I have been encountering the same way I do with R.\nMore to come, in the next weeks!!\nSee ya.\n\r","date":1580688000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580783839,"objectID":"c51c22fa5a25e21afc902244ab97f409","permalink":"/post/r-vs-python-ii/","publishdate":"2020-02-03T00:00:00Z","relpermalink":"/post/r-vs-python-ii/","section":"post","summary":"This is the great debate in Data Science, who wins?","tags":["R","Python","Data Science"],"title":"R vs Python, Part II","type":"post"},{"authors":["Alfonso Tobar"],"categories":["Blog","Rambling"],"content":"\rCatching up about me\rI’ve been working in my new job for the last two months, I’ve had some pros and some cons but that will be another day’s discussion. The reason I’m writing about this is because in my new role I have to create a lot of ML models and sadly, and I really mean sadly, R is not suited for that. And it seems Python is.\nThis is something that breaks my heart, and makes me wonder what tool is better suited for data science, R or Python? And I want to add my two cents based on my experience and trying to be as objective as possible.\nAditionally, I run into this updated DataCamp Infographic that I think is one of the most accurate and less biased comparisons I have found so far.\nRecap, my background and How I use to approach R vs Python\rDisclaimer\r\rI’ve been coding with R by 5 five years now, and heavy coding with Python the last 3 weeks.\n\rIf you know me (probably not) I’m an R fan, I’ve been programming with R since 2015 I’ve done everything: Reports, ETLs, Data cleaning, Shiny Apps, Machine Learning and Deep Learning (My Thesis) and a long etc.\nAnd I have to say I’ve been really reluctant to learn Python, I just couldn’t understand why people prefer it and why they consider it to be superior to R. I really get upset when I notice that Google releases some really cool APIs for something that I really want and natively is there for Python and I have to wait for some genius R hacker to create the API for R.\n2 weeks ago I attended to a Microsoft conference and I was in a data science Talk when the keynote asked who in the audience prefered Python, and 95% of the room raised their hand. My boss stared at me saying: “Ouch”. That hurted, but at the same time opened my eyes.\nNormally when I watch R vs Python things I notice a lot of Bias towards Python. I can’t find a founded reason of why you should go for Python instead of R.\nWhen you check these R vs Python entries the reason a lot of people allege about R not being a good option is because “… it is not suited for Production”. But what Production means?\n\rActually Production could be whatever form your data Product could be ready to be consumed, it could be a flat file, a Database, an online dashboard, a PDF report, an API…whatever your want.\n\rUnder that definition, R has a lot of different options to meet those needs:\n\rWhen it comes to exporting to files\r\r{readr} can export to any flat file out there.\n\r{arrow} can export to feather and parquet.\n\r{haven} can export to SAS, SPSS files.\n\r{jsonlite} can export to JSON.\n\ra long etc.\r\rIf it is about DBs, you can definitely need to check out the {DBI}, {odbc}, {dbplyr} combo. WIth those three packages you can connect to almost any DB type.\n\rAn online dashboard, well you have {Shiny} and a really long list of extensions to create really professional dashboards using the most cutting edges web frameworks such as Bootstrap, Bulma, AdminLTE, Semantic-UI, etc.\n\r\rA PDF report, you can use the whole –down stack of packages:\n\rReports {Rmarkdown},\rwrite your own book {bookdown},\ryour website (suck as this) {blogdown},\rposters {posterdown} and {pagedown},\rPDFs, Words, Powerpoints {Rmarkdown}, {xaringan}, {pagedown},\rScientific articles {distill}, {Rmarkdown},\rA really long etc. again.\r\rAn API, well {plumber} is definitely the easiest way to make your own API.\n\rWhen someone says R is not production ready, I would say I’m NOT SURE!!!\n\rR has all the capabilities to be suited for Production.\nAnother common issue is that R is slow. Of course it is slow if you use for loops with no previously defined length.\nI think there is enough evidence to show that R is not slow. You can check:\n\rH2o benchmarks here showing speed for data manipulation.\n\rYou can check some Deep Learning frameworks comparisons here and you will see that R can achieve pretty good times. (Take into account that R has no native DL framework and even with the translation to Python overhead can beat some well established Frameworks in some tasks, not all of them)\n\r\r\rSo again, speed is not an issue, if you want full speed of course you’ll want to go to C++, Scala or Java.\n\rAnother thing is the syntax. This is something that really gets me upset. Because comparisons are just not fair.\nPython prides itself to have “Among its most important characteristics the use of elegant syntax, which allows the users to read program code easily”.\nSorry guys but this:\ndf.loc[(df[\u0026#39;var_1\u0026#39;]\u0026gt;3) \u0026amp; df[\u0026#39;var_2\u0026#39;]\u0026lt;5,[\u0026#39;var_3\u0026#39;,\u0026#39;var_4\u0026#39;]].apply(lambda x: (x+3)**2, axis=0)\r\rDoes not read as plain english and it is not elegant.\n\rOf course, there are ways to write that code in a more readable way, but that code is quite compliant with Python Standards and I just can read it at first sight.\ndf %\u0026gt;%\rfilter(var_1 \u0026gt; 3 \u0026amp; var_2 \u0026lt; 5) %\u0026gt;%\rselect(var_3, var_4) %\u0026gt;%\rmap_df(~ (.x + 3)^2)\r\rSorry but, this is plain english and elegant.\n\rNormally websites says pandas can do something really powerful like sort values with:\ndf.sort_values(\u0026#39;var_1\u0026#39;, ascending = False)\rwhich is actually pretty powerful but they compare it to:\ndf[-order(df[\u0026#39;var_1\u0026#39;]),]\rwhich is unreadable, and nobody uses, and if you do, please STOP doing it.\nTidyverse allows to do this just by:\ndf %\u0026gt;%\rarrange(var_1)\rA fair comparison between Python and R needs to incorporate their main packages. The problem here is that Python have everything concentrated into the Scipy stack (Numpy, pandas, Scipy, matplotlib and Scikit-Learn). With those 5 packages you can do almost anything related to Data Science in Python.\nIn R, just by using the Tidyverse you have around 20 to 30 packages and you have a lot of small specialized packages to improve productivity.\nBut all of these things do not cover the MAIN reason of why people prefer Python over R.\nAnother popular reason described in these comparisons is that R is more suited for Statisticians while Python is more suited for programmers. This is partially True. I would say that Python looks more familiar for people with a Computer Science background, while R is more friendly for people that have never programmed in their life (During Latin R I was gladly surprised to notice that a lot of R programmers were not related at all with Data Science but with other fields that leverage data).\nThat explains a lot about things that R is being discriminated for:\n\rR has a messy syntax: Not necessarily, But a lot of people with no previous coding experience use R and they don´t care about following best practices, they want to get things done, no matter if you use base R, or tidyverse, or pipe, or data.table, all in the same script and alternating with no prior notice.\n\rR is slow: Again since most of the people don´t have coding experience they are not worried about imporving code performance.\n\rR has too many packages: This is true, and you can get lost here. But if you need to deal with data, normally you should go for the tidyverse, and following tidy principals all of the problems described above should be solved.\n\r\r\r\r\rWhy I prefer R\rThis is really personal, and the main reason is the tidyverse and the pipe friendly syntax. When I code I put a lot of effort to be able to understand it at first sight. Code readability is for me the most important thing.\nThen is code performance, if the code is not performant I modify syntax slightly to improve performance not affecting readability.\r{\rThen is productivity and complementary packages that I normally I use a lot, packages like {mufflr}, {glue} or {remedy} to make coding easier and more fun are crucial for me.\nFinally my favorite packages, and with this I refer to packages that have no comparison:\n\r{dplyr} (and {tidyr}) have absolutely no comparison. It is just the most beautiful syntax to deal with data, the function names, the functionality (specially things like mutate and scope variants like *_at, *_if and *_all) are just the best thing to work with.\n\r{ggplot2} Even though I´m not a chart fan, again the syntax and the ease to make really complete charts (I don´t want to say beautiful because I really hate ggplot’s default color palette) is priceless.\n\r{recipes} and the tidymodels API (not the documentation, that I have to say is quite messy sometimes). When recipes was released I just didn’t get it and I was so confused. Once I understood how it worked my life changed. There is no easier way to apply preprocessing steps like recipes. And this became my favorite package. The when the rest of packages started to be released I fell in love with tidymodels. I use to use caret and the package was so huge that I usually got lost. With this new framework everything was so organized that I really enjoyed creating Machine Learning workflows.\n\r\rThese 3 packages, namely dplyr, ggplot and recipes, have no comparison in my humble opinion.\n\rThe break up\rWhen you have your favorite packages and it fails, it just break your heart, and that happened with {recipes}.\nI built a simple Random Forest model with around 1M of rows, and the prepper object was 40GB. The {ranger} model object was 5GB and after the resample I got this lovely message:\n“Vector size X GB cannot be allocated”\nI moved to a cloud server, having 240GB of RAM and I used {furrr} to parallelize my code when R just stopped working running out of memory several times leaving incomplete processes (as many as threads could be run) running in the Task Manager, blocking memory to be used for some others processes.\nAfter carefully investigate what was happening I noticed that everytime I ran something less RAM was available, and the dissapointment arrived. Once the model finished I was expecting Memory to be released but that never happened.\n\rAfter doing some research I found the right term: Memory Leakage.\n\rMemory leakage refers when the OS is not releasing Memory once a process allocating this memory finishes. Plus, I didn´t understand why having 50GB objects used up my whole memory. The only way to free up this memory was restarting R (Ctrl + Shift + F10)\nAnd here is the real reason why R is not a top choice for data Science, and for some reason this is something that nobody mentions but it is slightly touched in the DataCamp Infographic. R has a poor memory management, and Hadley knows it. He mentions this Memory Leakage issues in Advance R first edition (for some reason it is not detailed in the 2nd Edition).\nReal Data Science, and not just the small examples we use to demonstrate the power and usage of a package, depends a lot on memory usage, and you cannot mount a production system knowing that R will use up your full memory available and you cannot release it.\nProbably this is something R developers are also facing and Hadley and the Rstudio guys know, but it is something intrinsic from R.\nPlus, R is not natively installed in a hosting service or a Linux server as Python is, and has not parallelism “almost natively” implemented. Making Python an easier choice to go.\n\rOk, here we explained why R is not a top choice option but we haven’t talked anything about Python…\n\rWhy Python? This will be covered in a next Post.\n\r","date":1580256000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580783839,"objectID":"35d52a12851b9a03a690daf332b3f46e","permalink":"/post/r-vs-python/","publishdate":"2020-01-29T00:00:00Z","relpermalink":"/post/r-vs-python/","section":"post","summary":"This is the great debate in Data Science, who wins?","tags":["R","Python","Data Science"],"title":"R vs Python","type":"post"},{"authors":["Alfonso Tobar"],"categories":["Blog","Rambling"],"content":"\rMy new Job\rMy working relationship with Evalueserve ended the past June and I just dedicated to finish my Thesis. On the first days of October I just started to apply to different Jobs, I have to say my first option was trying to get a Job at H2o, but I´m not there yet.\nThe thing is that after the first Interview I knew this would be my new Company, I felt some kind of feeling saying “you will work here” (although I had some hesitation). So I will be a Senior Data Scientist working at Scotiabank Cencosud, a Retail Company, and I´ll be part of the Advanced Analytics Team.\nIt seems there is a lot of interesting opportunities for my carreer. So let´s break them down:\n\rThis is a real Modeling Position.\n\rI will be primarily involved in developing Machine Learning in Production. This is something super exciting because I think I have learned and practiced a lot to be involved into this. I feel super prepared and eager to start applying the knowledge learned in the POCs I´ve worked in EVS and in the ML Diploma.\n\rWe´ll have a real Data Lake with plenty of Data\n\rThis is important, I don´t remember where I read about the things I needed to pay attention when applying to a new Job, and of course if we are doing Data Science we need Data. Well Cencosud is a Company that is positioning itself as an IDO (Insight Driven organization) and its primary focus is to make Data Driven Decisions.\nI think, I will have the chance to work with large amounts of Data and hopefully use Spark, this is something I really want to incorporate into my skill set.\n\rThere´s space for Innovative Research\n\rAt least in my conversations with the High Management during Interviews they guaranteed some space to innovate and experiment. This is something really fun to me and that I was looking for. So I expect to have the opportunity to implement new models, or algorithms that i haven´t work with before.\n\rI will use R\n\rThis was a main thing to me. I applied to several position where R was not an option, but I just couldn´t stand it. I want to work with R, and here I will have the opportunity to use it from end to end. I even twitted about it 😜 and I got one 😁.\n\r{{\u0026lt; tweet 1183107543113093120 \u0026gt;}}\r\rMany thanks to all the people that provided assitance and help to find job opportunities.\n\rI´m well paid and with plenty of Benefits\n\rOf course I will not disclose my salary (to avoid jealousy ), but I feel happy with the Offering. This was really an issue in my previous job, so I expect good things to happen in here.\nMore to come after the first month, I think, but at least for now. I´m happy.\n\r","date":1572998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580783839,"objectID":"5b5bc2e728609a713fc3f1bc2fa255a3","permalink":"/post/new-job/","publishdate":"2019-11-06T00:00:00Z","relpermalink":"/post/new-job/","section":"post","summary":"I just want to tell you about my new job","tags":["R","New job","Data Science"],"title":"I got a new job","type":"post"},{"authors":["Alfonso Tobar"],"categories":["Quick Solves"],"content":"\r\rThe Problem\rHere is the challenge:\nCalculate the time difference between Max and Min Dates found in a date vector.\nlibrary(tidyverse)\rdate_vec \u0026lt;- c(\u0026quot;2019/10/24 10:00:00\u0026quot;,\u0026quot;2019/10/23 11:00:00\u0026quot;,\u0026quot;2019/10/25 12:00:00\u0026quot;) \r\rThe Solution\rThe thing is super easy to get, but the idea is to create a pipeline that can calculate this in just a series of steps:\nlibrary(lubridate)\rdate_vec %\u0026gt;%\r#Transforming characters into dates using ymd for dates and hms for time\rymd_hms() %\u0026gt;%\r#range() retrieves max and min date\rrange() %\u0026gt;%\r#Calculate the time difference\rdiff() %\u0026gt;%\r#Transform into lubridate duration object %\u0026gt;%\ras.duration() \r## [1] \u0026quot;176400s (~2.04 days)\u0026quot;\r\r","date":1572393600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572494298,"objectID":"cdafdbb924d4e22d9e1567a8d8a98941","permalink":"/publication/problem-7/","publishdate":"2019-10-30T00:00:00Z","relpermalink":"/publication/problem-7/","section":"publication","summary":"Combining lubridate with Base R to create time difference calculations","tags":["Quick Solves","lubridate","Base R"],"title":"Dealing with dates","type":"publication"},{"authors":["Alfonso Tobar"],"categories":["Quick Solves"],"content":"\r\rThe Problem\rAnother Twitter Challenge:\n\r#rstats I\u0026#39;m sure there\u0026#39;s an elegant solution that I\u0026#39;m just totally missing. How do I create a unique episode_ID that increases by 1 for instances where episode_flag == \u0026quot;new\u0026quot; but just repeats the value from the row above when episode_flag == \u0026quot;same\u0026quot;? pic.twitter.com/Dl5ZtAiE7J\n\u0026mdash; Jessica Streeter (@phillynerd) October 30, 2019  \r\rThe Solution\rIt is almost there, I just added a couple of lines to get the expected output elegantly:\n{\"columns\":[{\"label\":[\"member\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"appt\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"episode_flag\"],\"name\":[3],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"a\",\"2\":\"1\",\"3\":\"new\"},{\"1\":\"a\",\"2\":\"2\",\"3\":\"same\"},{\"1\":\"b\",\"2\":\"1\",\"3\":\"new\"},{\"1\":\"b\",\"2\":\"2\",\"3\":\"same\"},{\"1\":\"b\",\"2\":\"3\",\"3\":\"same\"},{\"1\":\"b\",\"2\":\"1\",\"3\":\"new\"},{\"1\":\"c\",\"2\":\"1\",\"3\":\"new\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\rdf %\u0026gt;%\rgroup_by(episode_flag) %\u0026gt;%\rmutate(episode_ID = ifelse(episode_flag ==\u0026quot;new\u0026quot;, row_number(), NA)) %\u0026gt;%\r# Eliminating groups to apply next function\rungroup() %\u0026gt;%\r# Filling NAs with previous non-NA values\rfill(episode_ID)\r{\"columns\":[{\"label\":[\"member\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"appt\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"episode_flag\"],\"name\":[3],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"episode_ID\"],\"name\":[4],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"a\",\"2\":\"1\",\"3\":\"new\",\"4\":\"1\"},{\"1\":\"a\",\"2\":\"2\",\"3\":\"same\",\"4\":\"1\"},{\"1\":\"b\",\"2\":\"1\",\"3\":\"new\",\"4\":\"2\"},{\"1\":\"b\",\"2\":\"2\",\"3\":\"same\",\"4\":\"2\"},{\"1\":\"b\",\"2\":\"3\",\"3\":\"same\",\"4\":\"2\"},{\"1\":\"b\",\"2\":\"1\",\"3\":\"new\",\"4\":\"3\"},{\"1\":\"c\",\"2\":\"1\",\"3\":\"new\",\"4\":\"4\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\r\r","date":1572393600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572494298,"objectID":"ad011ed77167d189f5137771bbef9253","permalink":"/publication/problem-8/","publishdate":"2019-10-30T00:00:00Z","relpermalink":"/publication/problem-8/","section":"publication","summary":"Using TidyR to create create a complicated Unique ID","tags":["Quick Solves","tidyR","Unique ID"],"title":"Unique Id Challenge","type":"publication"},{"authors":["Alfonso Tobar"],"categories":["Quick Solves"],"content":"\r\rThe Problem\rHere is the challenge:\ndata \u0026lt;- data.frame(\r\u0026quot;id\u0026quot; = c(901, 902, 903, \u0026quot;age\u0026quot;, \u0026quot;gender\u0026quot;, \u0026quot;language\u0026quot;),\r\u0026quot;rater1\u0026quot; = c(7, 9, 9, 21, 1, 1),\r\u0026quot;rater2\u0026quot; = c(9, 9, 9, 39, 2, 2),\r\u0026quot;rater3\u0026quot; = c(9, 9, 9, 38, 2, 1),\r\u0026quot;rater4\u0026quot; = c(9, 9, 9, 33, 2, 1),\r\u0026quot;rater5\u0026quot; = c(2, 9, 9, 21, 2, 1)\r)\rFilter all the ratings with gender 1, or language 1, or gender 1 AND language 1.\n\rThe Solution\rThe thing is super easy, we need to transpose, the thing transposition is not a valid operation when it comes to data frames, how can we apply this in a data frame using tidyverse.\nSo in order to understand what happens I will run the solution by parts.\ndata %\u0026gt;%\rtranspose()\r## [[1]]\r## [[1]]$id\r## [1] 1\r## ## [[1]]$rater1\r## [1] 7\r## ## [[1]]$rater2\r## [1] 9\r## ## [[1]]$rater3\r## [1] 9\r## ## [[1]]$rater4\r## [1] 9\r## ## [[1]]$rater5\r## [1] 2\r## ## ## [[2]]\r## [[2]]$id\r## [1] 2\r## ## [[2]]$rater1\r## [1] 9\r## ## [[2]]$rater2\r## [1] 9\r## ## [[2]]$rater3\r## [1] 9\r## ## [[2]]$rater4\r## [1] 9\r## ## [[2]]$rater5\r## [1] 9\r## ## ## [[3]]\r## [[3]]$id\r## [1] 3\r## ## [[3]]$rater1\r## [1] 9\r## ## [[3]]$rater2\r## [1] 9\r## ## [[3]]$rater3\r## [1] 9\r## ## [[3]]$rater4\r## [1] 9\r## ## [[3]]$rater5\r## [1] 9\r## ## ## [[4]]\r## [[4]]$id\r## [1] 4\r## ## [[4]]$rater1\r## [1] 21\r## ## [[4]]$rater2\r## [1] 39\r## ## [[4]]$rater3\r## [1] 38\r## ## [[4]]$rater4\r## [1] 33\r## ## [[4]]$rater5\r## [1] 21\r## ## ## [[5]]\r## [[5]]$id\r## [1] 5\r## ## [[5]]$rater1\r## [1] 1\r## ## [[5]]$rater2\r## [1] 2\r## ## [[5]]$rater3\r## [1] 2\r## ## [[5]]$rater4\r## [1] 2\r## ## [[5]]$rater5\r## [1] 2\r## ## ## [[6]]\r## [[6]]$id\r## [1] 6\r## ## [[6]]$rater1\r## [1] 1\r## ## [[6]]$rater2\r## [1] 2\r## ## [[6]]$rater3\r## [1] 1\r## ## [[6]]$rater4\r## [1] 1\r## ## [[6]]$rater5\r## [1] 1\rThe problem using transpose is that the results is a list of lists, so it´s necessary to transform inner list into vectors:\ndata %\u0026gt;% #select raters\rselect(contains(\u0026quot;rater\u0026quot;)) %\u0026gt;%\r#transpose, the problem is that this transform data into lists of lists.\rtranspose() %\u0026gt;%\r#unlisting into double vectors\rmap(flatten_dbl)\r## [[1]]\r## rater1 rater2 rater3 rater4 rater5 ## 7 9 9 9 2 ## ## [[2]]\r## rater1 rater2 rater3 rater4 rater5 ## 9 9 9 9 9 ## ## [[3]]\r## rater1 rater2 rater3 rater4 rater5 ## 9 9 9 9 9 ## ## [[4]]\r## rater1 rater2 rater3 rater4 rater5 ## 21 39 38 33 21 ## ## [[5]]\r## rater1 rater2 rater3 rater4 rater5 ## 1 2 2 2 2 ## ## [[6]]\r## rater1 rater2 rater3 rater4 rater5 ## 1 2 1 1 1\rNow every list slot can be renamed with the corresponding id:\ndata %\u0026gt;% #select raters\rselect(contains(\u0026quot;rater\u0026quot;)) %\u0026gt;%\r#transpose, the problem is that this transform data into lists of lists.\rtranspose() %\u0026gt;%\r#unlisting map(flatten_dbl) %\u0026gt;%\rset_names(data$id)\r## $`901`\r## rater1 rater2 rater3 rater4 rater5 ## 7 9 9 9 2 ## ## $`902`\r## rater1 rater2 rater3 rater4 rater5 ## 9 9 9 9 9 ## ## $`903`\r## rater1 rater2 rater3 rater4 rater5 ## 9 9 9 9 9 ## ## $age\r## rater1 rater2 rater3 rater4 rater5 ## 21 39 38 33 21 ## ## $gender\r## rater1 rater2 rater3 rater4 rater5 ## 1 2 2 2 2 ## ## $language\r## rater1 rater2 rater3 rater4 rater5 ## 1 2 1 1 1\rFinally we can reorganize using map_dfc() function that reorder the data into dataframes by column:\n(\rnew_data \u0026lt;- data %\u0026gt;%\r#select raters\rselect(contains(\u0026quot;rater\u0026quot;)) %\u0026gt;%\r#transpose, the problem is that this transform data into lists of lists.\rtranspose() %\u0026gt;%\r#unlisting\rmap(flatten_dbl) %\u0026gt;%\rset_names(data$id) %\u0026gt;%\rmap_dfc( ~ .x)\r)\r{\"columns\":[{\"label\":[\"901\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"902\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"903\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"age\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"gender\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"language\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"7\",\"2\":\"9\",\"3\":\"9\",\"4\":\"21\",\"5\":\"1\",\"6\":\"1\",\"_row\":\"rater1\"},{\"1\":\"9\",\"2\":\"9\",\"3\":\"9\",\"4\":\"39\",\"5\":\"2\",\"6\":\"2\",\"_row\":\"rater2\"},{\"1\":\"9\",\"2\":\"9\",\"3\":\"9\",\"4\":\"38\",\"5\":\"2\",\"6\":\"1\",\"_row\":\"rater3\"},{\"1\":\"9\",\"2\":\"9\",\"3\":\"9\",\"4\":\"33\",\"5\":\"2\",\"6\":\"1\",\"_row\":\"rater4\"},{\"1\":\"2\",\"2\":\"9\",\"3\":\"9\",\"4\":\"21\",\"5\":\"2\",\"6\":\"1\",\"_row\":\"rater5\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\rNow we can filter accordingly the requested filterings:\nnew_data %\u0026gt;%\rfilter(gender == 1)\r{\"columns\":[{\"label\":[\"901\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"902\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"903\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"age\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"gender\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"language\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"7\",\"2\":\"9\",\"3\":\"9\",\"4\":\"21\",\"5\":\"1\",\"6\":\"1\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\rnew_data %\u0026gt;%\rfilter(language == 1)\r{\"columns\":[{\"label\":[\"901\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"902\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"903\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"age\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"gender\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"language\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"7\",\"2\":\"9\",\"3\":\"9\",\"4\":\"21\",\"5\":\"1\",\"6\":\"1\"},{\"1\":\"9\",\"2\":\"9\",\"3\":\"9\",\"4\":\"38\",\"5\":\"2\",\"6\":\"1\"},{\"1\":\"9\",\"2\":\"9\",\"3\":\"9\",\"4\":\"33\",\"5\":\"2\",\"6\":\"1\"},{\"1\":\"2\",\"2\":\"9\",\"3\":\"9\",\"4\":\"21\",\"5\":\"2\",\"6\":\"1\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\rnew_data %\u0026gt;%\rfilter(gender == 1 \u0026amp; language == 1)\r{\"columns\":[{\"label\":[\"901\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"902\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"903\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"age\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"gender\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"language\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"7\",\"2\":\"9\",\"3\":\"9\",\"4\":\"21\",\"5\":\"1\",\"6\":\"1\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\r\r","date":1571875200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572494298,"objectID":"7cd65f23310c8c67b16d19aa3cf78394","permalink":"/publication/problem-6/","publishdate":"2019-10-24T00:00:00Z","relpermalink":"/publication/problem-6/","section":"publication","summary":"Using purrr to transpose a dataframe","tags":["Quick Solves","purrr","transposing"],"title":"Transposing a dataframe","type":"publication"},{"authors":["Alfonso Tobar"],"categories":["Quick Solves"],"content":"\r\rThe Problem\rThis is simple, If Names of List are found in List B then Replace:\na \u0026lt;- list(x = 1, y = TRUE, z = \u0026quot;a\u0026quot;)\rb \u0026lt;- list(x = 2, z = \u0026quot;b\u0026quot;)\rexpected \u0026lt;- list(x = 2, y = TRUE, z = \u0026quot;b\u0026quot;)\r\rThe Solution\rIt was hard to think in something simple, because the problem is not as complicated, it is just List is a complicated object to deal with, but I came with this:\na \u0026lt;- list(x = 1, y = TRUE, z = \u0026quot;a\u0026quot;)\rb \u0026lt;- list(x = 2, z = \u0026quot;b\u0026quot;)\rval_names \u0026lt;- names(a) %in% names(b) %\u0026gt;% names(a)[.]\ra[val_names] \u0026lt;- b[val_names]\ra\r## $x\r## [1] 2\r## ## $y\r## [1] TRUE\r## ## $z\r## [1] \u0026quot;b\u0026quot;\r\r","date":1571443200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572494298,"objectID":"919a5ddd48b74f97cfee1d71c91ee4b4","permalink":"/publication/problem-5/","publishdate":"2019-10-19T00:00:00Z","relpermalink":"/publication/problem-5/","section":"publication","summary":"Using R to Replace Element in One List if the same name found in Other List","tags":["Quick Solves","List","Challenge"],"title":"List Challenge","type":"publication"},{"authors":["Alfonso Tobar"],"categories":["Quick Solves"],"content":"\r\rEven though the Data looks messy and an Intruitive solution didn´t pop up inmediately, It was relatively short to fix.\nThe Problem\rI want to save some words so I’ll go to the source\n\rHey #rstats peeps. I have ~36 tables like this extracted from the LCMM 📦 results. I need to tidy it. I want 5 rows with the values for intercept and sofa_study_day in individual columns. Suggestions? pic.twitter.com/1rFku6T0qt\n\u0026mdash; jsonpott (@jsonpott) October 19, 2019  \rSo I just replicated the data and did the following:\n\rMade data longer eliminating all the NAs that showed up.\n\rDivided into Intercept and sofa_study_day.\n\rJoining both together to obtain the 5 records.\n\r\rThe code looks like this:\ndata \u0026lt;- tibble::tribble(\r~Sofa.time.point, ~Se, ~Wald, ~p.value, ~x1, ~x2, ~x3, ~x4, ~x5,\r\u0026quot;intercept\u0026quot;, 0.12395, -24.333, 0, NA, NA, NA, -3.01592, NA,\r\u0026quot;intercept\u0026quot;, 0.13165, -40.045, 0, NA, NA, NA, NA, -5.27211,\r\u0026quot;intercept\u0026quot;, 0.21603, -7.372, 0, NA, -1.59253, NA, NA, NA,\r\u0026quot;intercept\u0026quot;, 0.23614, -5.085, 0, NA, NA, -1.20082, NA, NA,\r\u0026quot;intercept\u0026quot;, NA, NA, 0, 0, NA, NA, NA, NA,\r\u0026quot;sofa_study_day\u0026quot;, 0.00411, -14.669, 0, NA, NA, NA, NA, -0.06028,\r\u0026quot;sofa_study_day\u0026quot;, 0.00479, -34.798, 0, NA, NA, NA, -0.16685, NA,\r\u0026quot;sofa_study_day\u0026quot;, 0.00615, -39.744, 0, -0.24443, NA, NA, NA, NA,\r\u0026quot;sofa_study_day\u0026quot;, 0.00756, -9.975, 0, NA, NA, -0.07543, NA, NA,\r\u0026quot;sofa_study_day\u0026quot;, 0.02224, -24.673, 0, NA, -0.5488, NA, NA, NA\r)\rtidy_data \u0026lt;- data %\u0026gt;%\rpivot_longer(\r# keeping columns from \u0026quot;Sofa.time.point\u0026quot; to \u0026quot;p.value\u0026quot;\r-(Sofa.time.point:p.value),\r# transform x columns into just one column\rnames_to = \u0026quot;x\u0026quot;,\r# populate with values\rvalues_to = \u0026quot;values\u0026quot;,\r# dropping NAs\rvalues_drop_na = TRUE\r)\r# \u0026quot;intercept\u0026quot; data\rtidy_data %\u0026gt;%\rfilter(Sofa.time.point == \u0026quot;intercept\u0026quot;) %\u0026gt;%\rleft_join(\r#joined with \u0026quot;sofa_study_day\u0026quot;\rtidy_data %\u0026gt;%\rfilter(Sofa.time.point == \u0026quot;sofa_study_day\u0026quot;),\r# joining by \u0026quot;x\u0026quot;\rby = \u0026quot;x\u0026quot;,\r# adding identifiers to columns having the same name\rsuffix = c(\u0026quot;.intercept\u0026quot;, \u0026quot;.sofa\u0026quot;)\r)\r{\"columns\":[{\"label\":[\"Sofa.time.point.intercept\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"Se.intercept\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Wald.intercept\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"p.value.intercept\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"x\"],\"name\":[5],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"values.intercept\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Sofa.time.point.sofa\"],\"name\":[7],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"Se.sofa\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Wald.sofa\"],\"name\":[9],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"p.value.sofa\"],\"name\":[10],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"values.sofa\"],\"name\":[11],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"intercept\",\"2\":\"0.12395\",\"3\":\"-24.333\",\"4\":\"0\",\"5\":\"x4\",\"6\":\"-3.01592\",\"7\":\"sofa_study_day\",\"8\":\"0.00479\",\"9\":\"-34.798\",\"10\":\"0\",\"11\":\"-0.16685\"},{\"1\":\"intercept\",\"2\":\"0.13165\",\"3\":\"-40.045\",\"4\":\"0\",\"5\":\"x5\",\"6\":\"-5.27211\",\"7\":\"sofa_study_day\",\"8\":\"0.00411\",\"9\":\"-14.669\",\"10\":\"0\",\"11\":\"-0.06028\"},{\"1\":\"intercept\",\"2\":\"0.21603\",\"3\":\"-7.372\",\"4\":\"0\",\"5\":\"x2\",\"6\":\"-1.59253\",\"7\":\"sofa_study_day\",\"8\":\"0.02224\",\"9\":\"-24.673\",\"10\":\"0\",\"11\":\"-0.54880\"},{\"1\":\"intercept\",\"2\":\"0.23614\",\"3\":\"-5.085\",\"4\":\"0\",\"5\":\"x3\",\"6\":\"-1.20082\",\"7\":\"sofa_study_day\",\"8\":\"0.00756\",\"9\":\"-9.975\",\"10\":\"0\",\"11\":\"-0.07543\"},{\"1\":\"intercept\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"0\",\"5\":\"x1\",\"6\":\"0.00000\",\"7\":\"sofa_study_day\",\"8\":\"0.00615\",\"9\":\"-39.744\",\"10\":\"0\",\"11\":\"-0.24443\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\r\r","date":1571443200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572494298,"objectID":"96c08d2950c3c758c27084f279510239","permalink":"/publication/problem-4/","publishdate":"2019-10-19T00:00:00Z","relpermalink":"/publication/problem-4/","section":"publication","summary":"Using TidyR and dplyr Joins to fix some really ugly Data","tags":["Quick Solves","tidyR","left_join()","ugly data"],"title":"Ugly Untied Dataset","type":"publication"},{"authors":["Alfonso Tobar"],"categories":["Quick Solves"],"content":"\r\rSurfing at Stack Overflow I noticed a problem that I found interesting to solve:\rThe following Data was presented:\nThe Problem\rThe following Data is presented:\nsample \u0026lt;-\rstructure(\rlist(\r`Country Name` = c(\r\u0026quot;Aruba\u0026quot;,\r\u0026quot;Afghanistan\u0026quot;,\r\u0026quot;Angola\u0026quot;,\r\u0026quot;Albania\u0026quot;,\r\u0026quot;Andorra\u0026quot;,\r\u0026quot;Arab World\u0026quot;,\r\u0026quot;United Arab Emirates\u0026quot;,\r\u0026quot;Argentina\u0026quot;,\r\u0026quot;Armenia\u0026quot;,\r\u0026quot;American Samoa\u0026quot;,\r\u0026quot;Antigua and Barbuda\u0026quot;,\r\u0026quot;Australia\u0026quot;\r),\r`Country Code` = c(\r\u0026quot;ABW\u0026quot;,\r\u0026quot;AFG\u0026quot;,\r\u0026quot;AGO\u0026quot;,\r\u0026quot;ALB\u0026quot;,\r\u0026quot;AND\u0026quot;,\r\u0026quot;ARB\u0026quot;,\r\u0026quot;ARE\u0026quot;,\r\u0026quot;ARG\u0026quot;,\r\u0026quot;ARM\u0026quot;,\r\u0026quot;ASM\u0026quot;,\r\u0026quot;ATG\u0026quot;,\r\u0026quot;AUS\u0026quot;\r),\r`2007` = c(\r5.39162036843645,\r8.68057078513406,\r12.2514974459487,\r2.93268248162318,\rNA,\r4.74356585295154,\rNA,\rNA,\rNA,\rNA,\r1.41605259409743,\rNA\r),\r`2008` = c(\r8.95722105296535,\r26.4186641547444,\r12.4758291326398,\r3.36313757366391,\rNA,\rNA,\r12.2504202448139,\rNA,\r8.94995335353386,\rNA,\r5.33380639820232,\rNA\r),\r`2009` = c(\r-2.13630037272305,-6.81116108898995,\r13.7302839288409,\r2.23139683475865,\rNA,\r2.92089711805365,\r1.55980098148558,\rNA,\r3.40676682683799,\rNA,\r-0.550159995508869,\rNA\r),\r`2010` = c(\r2.07773902027782,\r2.1785375238942,\r14.4696564932574,\r3.61538461538463,\rNA,\r3.91106195534027,\r0.879216764156813,\rNA,\r8.17636138473956,\rNA,\r3.3700254022015,\r2.91834002677376\r),\r`2011` = c(\r4.31633194082721,\r11.8041858089129,\r13.4824679218511,\r3.44283593170005,\rNA,\r4.75316388885632,\rNA,\rNA,\r7.6500080785929,\rNA,\r3.45674967234599,\r3.30385015608744\r),\r`2012` = c(\r0.627927921638161,\r6.44121280934118,\r10.2779049218839,\r2.03642235579081,\rNA,\r4.61184432206646,\r0.662268900269082,\rNA,\r2.55802007757907,\rNA,\r3.37688044338879,\r1.76278015613193\r),\r`2013` = c(\r-2.37226328015073,\r7.38577178397857,\r8.77781429332619,\r1.92544399507649,\rNA,\r3.23423783752364,\r1.10111836375706,\rNA,\r5.78966778544654,\rNA,\r1.05949782356168,\r2.44988864142539\r),\r`2014` = c(\r0.421637771012246,\r4.67399603536339,\r7.28038730361125,\r1.61304235314414,\rNA,\r2.77261158414198,\r2.34626865671643,\rNA,\r2.98130868933673,\rNA,\r1.08944157435363,\r2.48792270531403\r)\r),\rclass = c(\u0026quot;tbl_df\u0026quot;, \u0026quot;tbl\u0026quot;, \u0026quot;data.frame\u0026quot;),\rrow.names = c(NA,-12L)\r)\rsample\r{\"columns\":[{\"label\":[\"Country Name\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"Country Code\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"2007\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"2008\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"2009\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"2010\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"2011\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"2012\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"2013\"],\"name\":[9],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"2014\"],\"name\":[10],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"Aruba\",\"2\":\"ABW\",\"3\":\"5.391620\",\"4\":\"8.957221\",\"5\":\"-2.136300\",\"6\":\"2.0777390\",\"7\":\"4.316332\",\"8\":\"0.6279279\",\"9\":\"-2.372263\",\"10\":\"0.4216378\"},{\"1\":\"Afghanistan\",\"2\":\"AFG\",\"3\":\"8.680571\",\"4\":\"26.418664\",\"5\":\"-6.811161\",\"6\":\"2.1785375\",\"7\":\"11.804186\",\"8\":\"6.4412128\",\"9\":\"7.385772\",\"10\":\"4.6739960\"},{\"1\":\"Angola\",\"2\":\"AGO\",\"3\":\"12.251497\",\"4\":\"12.475829\",\"5\":\"13.730284\",\"6\":\"14.4696565\",\"7\":\"13.482468\",\"8\":\"10.2779049\",\"9\":\"8.777814\",\"10\":\"7.2803873\"},{\"1\":\"Albania\",\"2\":\"ALB\",\"3\":\"2.932682\",\"4\":\"3.363138\",\"5\":\"2.231397\",\"6\":\"3.6153846\",\"7\":\"3.442836\",\"8\":\"2.0364224\",\"9\":\"1.925444\",\"10\":\"1.6130424\"},{\"1\":\"Andorra\",\"2\":\"AND\",\"3\":\"NA\",\"4\":\"NA\",\"5\":\"NA\",\"6\":\"NA\",\"7\":\"NA\",\"8\":\"NA\",\"9\":\"NA\",\"10\":\"NA\"},{\"1\":\"Arab World\",\"2\":\"ARB\",\"3\":\"4.743566\",\"4\":\"NA\",\"5\":\"2.920897\",\"6\":\"3.9110620\",\"7\":\"4.753164\",\"8\":\"4.6118443\",\"9\":\"3.234238\",\"10\":\"2.7726116\"},{\"1\":\"United Arab Emirates\",\"2\":\"ARE\",\"3\":\"NA\",\"4\":\"12.250420\",\"5\":\"1.559801\",\"6\":\"0.8792168\",\"7\":\"NA\",\"8\":\"0.6622689\",\"9\":\"1.101118\",\"10\":\"2.3462687\"},{\"1\":\"Argentina\",\"2\":\"ARG\",\"3\":\"NA\",\"4\":\"NA\",\"5\":\"NA\",\"6\":\"NA\",\"7\":\"NA\",\"8\":\"NA\",\"9\":\"NA\",\"10\":\"NA\"},{\"1\":\"Armenia\",\"2\":\"ARM\",\"3\":\"NA\",\"4\":\"8.949953\",\"5\":\"3.406767\",\"6\":\"8.1763614\",\"7\":\"7.650008\",\"8\":\"2.5580201\",\"9\":\"5.789668\",\"10\":\"2.9813087\"},{\"1\":\"American Samoa\",\"2\":\"ASM\",\"3\":\"NA\",\"4\":\"NA\",\"5\":\"NA\",\"6\":\"NA\",\"7\":\"NA\",\"8\":\"NA\",\"9\":\"NA\",\"10\":\"NA\"},{\"1\":\"Antigua and Barbuda\",\"2\":\"ATG\",\"3\":\"1.416053\",\"4\":\"5.333806\",\"5\":\"-0.550160\",\"6\":\"3.3700254\",\"7\":\"3.456750\",\"8\":\"3.3768804\",\"9\":\"1.059498\",\"10\":\"1.0894416\"},{\"1\":\"Australia\",\"2\":\"AUS\",\"3\":\"NA\",\"4\":\"NA\",\"5\":\"NA\",\"6\":\"2.9183400\",\"7\":\"3.303850\",\"8\":\"1.7627802\",\"9\":\"2.449889\",\"10\":\"2.4879227\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\rThe idea is to input Missing Values following some rules:\nSome countries have NAs for all 8 years (columns 3:10), and in that case I want to replace all NAs with the column mean.\n\rOther countries only have NAs in some columns, in which case I want to replace NA with the previous year’s value.\n\rThe final condition is that, if the NA is in the first year (2007), I want to replace it with the 2007 column mean instead of the next year (2008 was the financial crisis so all the inflation rates went nuts).\n\r\rOf course this can be easily programmed using Regular Programming Rules using For loops and If Statements, but the idea is to do it in a tidy way using the Tidyverse.\nlibrary(dplyr, warn.conflicts = FALSE)\rlibrary(tidyr)\rlibrary(janitor)\r## ## Attaching package: \u0026#39;janitor\u0026#39;\r## The following objects are masked from \u0026#39;package:stats\u0026#39;:\r## ## chisq.test, fisher.test\r# Getting the Column Means to Replace according to Condition 1 and 3. (replacement \u0026lt;- sample %\u0026gt;%\rselect_if(is.numeric) %\u0026gt;%\rsummarize_all( ~ mean(., na.rm = TRUE)) %\u0026gt;%\r#Transformed to List since it is a requirement for tidyr::replace_na()\ras.list())\r## $`2007`\r## [1] 5.902665\r## ## $`2008`\r## [1] 11.107\r## ## $`2009`\r## [1] 1.793941\r## ## $`2010`\r## [1] 4.621814\r## ## $`2011`\r## [1] 6.526199\r## ## $`2012`\r## [1] 3.595029\r## ## $`2013`\r## [1] 3.261242\r## ## $`2014`\r## [1] 2.851846\r\rThe solution\rsample %\u0026gt;%\rpivot_longer(`2007`:`2014`, names_to = \u0026quot;year\u0026quot;, values_to = \u0026quot;int_rate\u0026quot;) %\u0026gt;%\rgroup_by(`Country Name`) %\u0026gt;%\rsummarize(na_num = is.na(int_rate) %\u0026gt;% sum) %\u0026gt;%\r#Joining the number of NAs na_num as a new column\rleft_join(sample, by = \u0026quot;Country Name\u0026quot;) %\u0026gt;%\r#Replacing 2007 missing as a first value. Condition 3.\rmutate(`2007` = if_else(between(na_num, 1, 7) \u0026amp;\ris.na(`2007`), replacement[[1]] , `2007`)) %\u0026gt;%\r#Making dataset wider pivot_longer(`2007`:`2014`, names_to = \u0026quot;year\u0026quot;, values_to = \u0026quot;int_rate\u0026quot;) %\u0026gt;%\rgroup_by(`Country Name`) %\u0026gt;%\r#Using fill to impute NAs with the previous one. Condition 2.\rfill(int_rate) %\u0026gt;%\rpivot_wider(names_from = year, values_from = int_rate) %\u0026gt;%\r#Replacing Values when all values are missing. Condition 1.\rreplace_na(replace = replacement) \r{\"columns\":[{\"label\":[\"Country Name\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"na_num\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Country Code\"],\"name\":[3],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"2007\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"2008\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"2009\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"2010\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"2011\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"2012\"],\"name\":[9],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"2013\"],\"name\":[10],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"2014\"],\"name\":[11],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"Afghanistan\",\"2\":\"0\",\"3\":\"AFG\",\"4\":\"8.680571\",\"5\":\"26.418664\",\"6\":\"-6.811161\",\"7\":\"2.1785375\",\"8\":\"11.8041858\",\"9\":\"6.4412128\",\"10\":\"7.385772\",\"11\":\"4.6739960\"},{\"1\":\"Albania\",\"2\":\"0\",\"3\":\"ALB\",\"4\":\"2.932682\",\"5\":\"3.363138\",\"6\":\"2.231397\",\"7\":\"3.6153846\",\"8\":\"3.4428359\",\"9\":\"2.0364224\",\"10\":\"1.925444\",\"11\":\"1.6130424\"},{\"1\":\"American Samoa\",\"2\":\"8\",\"3\":\"ASM\",\"4\":\"5.902665\",\"5\":\"11.107005\",\"6\":\"1.793941\",\"7\":\"4.6218137\",\"8\":\"6.5261992\",\"9\":\"3.5950291\",\"10\":\"3.261242\",\"11\":\"2.8518463\"},{\"1\":\"Andorra\",\"2\":\"8\",\"3\":\"AND\",\"4\":\"5.902665\",\"5\":\"11.107005\",\"6\":\"1.793941\",\"7\":\"4.6218137\",\"8\":\"6.5261992\",\"9\":\"3.5950291\",\"10\":\"3.261242\",\"11\":\"2.8518463\"},{\"1\":\"Angola\",\"2\":\"0\",\"3\":\"AGO\",\"4\":\"12.251497\",\"5\":\"12.475829\",\"6\":\"13.730284\",\"7\":\"14.4696565\",\"8\":\"13.4824679\",\"9\":\"10.2779049\",\"10\":\"8.777814\",\"11\":\"7.2803873\"},{\"1\":\"Antigua and Barbuda\",\"2\":\"0\",\"3\":\"ATG\",\"4\":\"1.416053\",\"5\":\"5.333806\",\"6\":\"-0.550160\",\"7\":\"3.3700254\",\"8\":\"3.4567497\",\"9\":\"3.3768804\",\"10\":\"1.059498\",\"11\":\"1.0894416\"},{\"1\":\"Arab World\",\"2\":\"1\",\"3\":\"ARB\",\"4\":\"4.743566\",\"5\":\"4.743566\",\"6\":\"2.920897\",\"7\":\"3.9110620\",\"8\":\"4.7531639\",\"9\":\"4.6118443\",\"10\":\"3.234238\",\"11\":\"2.7726116\"},{\"1\":\"Argentina\",\"2\":\"8\",\"3\":\"ARG\",\"4\":\"5.902665\",\"5\":\"11.107005\",\"6\":\"1.793941\",\"7\":\"4.6218137\",\"8\":\"6.5261992\",\"9\":\"3.5950291\",\"10\":\"3.261242\",\"11\":\"2.8518463\"},{\"1\":\"Armenia\",\"2\":\"1\",\"3\":\"ARM\",\"4\":\"5.902665\",\"5\":\"8.949953\",\"6\":\"3.406767\",\"7\":\"8.1763614\",\"8\":\"7.6500081\",\"9\":\"2.5580201\",\"10\":\"5.789668\",\"11\":\"2.9813087\"},{\"1\":\"Aruba\",\"2\":\"0\",\"3\":\"ABW\",\"4\":\"5.391620\",\"5\":\"8.957221\",\"6\":\"-2.136300\",\"7\":\"2.0777390\",\"8\":\"4.3163319\",\"9\":\"0.6279279\",\"10\":\"-2.372263\",\"11\":\"0.4216378\"},{\"1\":\"Australia\",\"2\":\"3\",\"3\":\"AUS\",\"4\":\"5.902665\",\"5\":\"5.902665\",\"6\":\"5.902665\",\"7\":\"2.9183400\",\"8\":\"3.3038502\",\"9\":\"1.7627802\",\"10\":\"2.449889\",\"11\":\"2.4879227\"},{\"1\":\"United Arab Emirates\",\"2\":\"2\",\"3\":\"ARE\",\"4\":\"5.902665\",\"5\":\"12.250420\",\"6\":\"1.559801\",\"7\":\"0.8792168\",\"8\":\"0.8792168\",\"9\":\"0.6622689\",\"10\":\"1.101118\",\"11\":\"2.3462687\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\r\r","date":1571184000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572494298,"objectID":"49c47c25dfdd4f04a2b5084ecb94dbcf","permalink":"/publication/problem-1/","publishdate":"2019-10-16T00:00:00Z","relpermalink":"/publication/problem-1/","section":"publication","summary":"I´ll be solving an Missing Value Imputation problem.","tags":["Quick Solves","Imputing","Replacing NAs"],"title":"Missing Value Imputation","type":"publication"},{"authors":["Alfonso Tobar"],"categories":["Quick Solves"],"content":"\r\rNavigating Twitter I found this other Problem:\nThe Problem\rThe following dummy_function is presented:\nlibrary(dplyr)\r#\u0026gt; a \u0026lt;- sample(letters[1:5], 500, rep = TRUE)\rb \u0026lt;- sample(1:10, 500, rep = TRUE)\rdf1 \u0026lt;- data.frame(a, b)\rdummy_function \u0026lt;- function(data, var1, var2){\r# Creating summary statistics\rdf \u0026lt;- data %\u0026gt;%\rgroup_by(var1, var2) %\u0026gt;%\rsummarise(n=n()) %\u0026gt;%\rgroup_by(var1) %\u0026gt;%\rmutate(perc=100*n/sum(n))\rdf\r}\rdummy_function(df1, a, b)\r#\u0026gt; Error: Column `var1` is unknown\r\rCreated by the reprex package (v0.3.0)\rThis is a typical problem caused by one of the coolest things provided by the tidyverse: the Non-Standard Evaluation.\nNon-Standard Evaluation is the ability that some R functions have (mainly in the tidyverse and all the packages following a tidy approach) when you can pass a variable within the data without quoting:\niris %\u0026gt;% select(Species) %\u0026gt;%\rhead(10)\r{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"Species\"],\"name\":[1],\"type\":[\"fctr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"setosa\",\"_rn_\":\"1\"},{\"1\":\"setosa\",\"_rn_\":\"2\"},{\"1\":\"setosa\",\"_rn_\":\"3\"},{\"1\":\"setosa\",\"_rn_\":\"4\"},{\"1\":\"setosa\",\"_rn_\":\"5\"},{\"1\":\"setosa\",\"_rn_\":\"6\"},{\"1\":\"setosa\",\"_rn_\":\"7\"},{\"1\":\"setosa\",\"_rn_\":\"8\"},{\"1\":\"setosa\",\"_rn_\":\"9\"},{\"1\":\"setosa\",\"_rn_\":\"10\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\rAs you may see, you don´t need to quote Species, but R is not recognizing Species as an R object but as an existing variable within iris dataset. If you would like to do the same thing using “Standard Evaluation” you´d have to code something like this:\nhead(iris[\u0026quot;Species\u0026quot;], 10)\r{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"Species\"],\"name\":[1],\"type\":[\"fctr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"setosa\",\"_rn_\":\"1\"},{\"1\":\"setosa\",\"_rn_\":\"2\"},{\"1\":\"setosa\",\"_rn_\":\"3\"},{\"1\":\"setosa\",\"_rn_\":\"4\"},{\"1\":\"setosa\",\"_rn_\":\"5\"},{\"1\":\"setosa\",\"_rn_\":\"6\"},{\"1\":\"setosa\",\"_rn_\":\"7\"},{\"1\":\"setosa\",\"_rn_\":\"8\"},{\"1\":\"setosa\",\"_rn_\":\"9\"},{\"1\":\"setosa\",\"_rn_\":\"10\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\rIn this case you see Species is not an object but a quoted string that is passed as the Variable name for object Iris.\nThe error then pops up because in the dummy_function() you have group_by() that uses NSE having var1, var2 as arguments and var1 and var2 objects are not variables of data. What you actually want is to pass var1 and var2 values as the grouping variables.\nDefinitely NSE is a great addition and saves typing, but when it comes to create functions it used to be a nightmare. rlang package handled this using something called quosures, and the bang-bang operator. If you want to know about this Hadley teaches it in 5 minutes:\n\r  \r\rThe solution\rFortunately, Hadley’s explanation is helpful to understand the problem but the solution now is super easy with the new version of rlang. You just need to wrap var1 and var2 in the new curly-curly operator to embrace the values of var1 and var2 and pass them along the group_by() function.\na \u0026lt;- sample(letters[1:5], 500, rep = TRUE)\rb \u0026lt;- sample(1:10, 500, rep = TRUE)\rdf1 \u0026lt;- data.frame(a, b)\rlibrary(rlang)\rdummy_function \u0026lt;- function(data, var1, var2){\r# Creating summary statistics\rdf \u0026lt;- data %\u0026gt;%\rgroup_by({{var1}}, {{var2}}) %\u0026gt;%\rsummarise(n=n()) %\u0026gt;%\rgroup_by({{var1}}) %\u0026gt;%\rmutate(perc=100*n/sum(n))\rdf\r}\rdummy_function(df1, a, b)\r{\"columns\":[{\"label\":[\"a\"],\"name\":[1],\"type\":[\"fctr\"],\"align\":[\"left\"]},{\"label\":[\"b\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"n\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"perc\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"a\",\"2\":\"1\",\"3\":\"13\",\"4\":\"11.403509\"},{\"1\":\"a\",\"2\":\"2\",\"3\":\"14\",\"4\":\"12.280702\"},{\"1\":\"a\",\"2\":\"3\",\"3\":\"16\",\"4\":\"14.035088\"},{\"1\":\"a\",\"2\":\"4\",\"3\":\"3\",\"4\":\"2.631579\"},{\"1\":\"a\",\"2\":\"5\",\"3\":\"10\",\"4\":\"8.771930\"},{\"1\":\"a\",\"2\":\"6\",\"3\":\"8\",\"4\":\"7.017544\"},{\"1\":\"a\",\"2\":\"7\",\"3\":\"13\",\"4\":\"11.403509\"},{\"1\":\"a\",\"2\":\"8\",\"3\":\"15\",\"4\":\"13.157895\"},{\"1\":\"a\",\"2\":\"9\",\"3\":\"14\",\"4\":\"12.280702\"},{\"1\":\"a\",\"2\":\"10\",\"3\":\"8\",\"4\":\"7.017544\"},{\"1\":\"b\",\"2\":\"1\",\"3\":\"10\",\"4\":\"10.416667\"},{\"1\":\"b\",\"2\":\"2\",\"3\":\"8\",\"4\":\"8.333333\"},{\"1\":\"b\",\"2\":\"3\",\"3\":\"5\",\"4\":\"5.208333\"},{\"1\":\"b\",\"2\":\"4\",\"3\":\"8\",\"4\":\"8.333333\"},{\"1\":\"b\",\"2\":\"5\",\"3\":\"9\",\"4\":\"9.375000\"},{\"1\":\"b\",\"2\":\"6\",\"3\":\"12\",\"4\":\"12.500000\"},{\"1\":\"b\",\"2\":\"7\",\"3\":\"11\",\"4\":\"11.458333\"},{\"1\":\"b\",\"2\":\"8\",\"3\":\"12\",\"4\":\"12.500000\"},{\"1\":\"b\",\"2\":\"9\",\"3\":\"9\",\"4\":\"9.375000\"},{\"1\":\"b\",\"2\":\"10\",\"3\":\"12\",\"4\":\"12.500000\"},{\"1\":\"c\",\"2\":\"1\",\"3\":\"9\",\"4\":\"9.890110\"},{\"1\":\"c\",\"2\":\"2\",\"3\":\"7\",\"4\":\"7.692308\"},{\"1\":\"c\",\"2\":\"3\",\"3\":\"10\",\"4\":\"10.989011\"},{\"1\":\"c\",\"2\":\"4\",\"3\":\"9\",\"4\":\"9.890110\"},{\"1\":\"c\",\"2\":\"5\",\"3\":\"11\",\"4\":\"12.087912\"},{\"1\":\"c\",\"2\":\"6\",\"3\":\"10\",\"4\":\"10.989011\"},{\"1\":\"c\",\"2\":\"7\",\"3\":\"9\",\"4\":\"9.890110\"},{\"1\":\"c\",\"2\":\"8\",\"3\":\"8\",\"4\":\"8.791209\"},{\"1\":\"c\",\"2\":\"9\",\"3\":\"7\",\"4\":\"7.692308\"},{\"1\":\"c\",\"2\":\"10\",\"3\":\"11\",\"4\":\"12.087912\"},{\"1\":\"d\",\"2\":\"1\",\"3\":\"10\",\"4\":\"10.869565\"},{\"1\":\"d\",\"2\":\"2\",\"3\":\"14\",\"4\":\"15.217391\"},{\"1\":\"d\",\"2\":\"3\",\"3\":\"10\",\"4\":\"10.869565\"},{\"1\":\"d\",\"2\":\"4\",\"3\":\"12\",\"4\":\"13.043478\"},{\"1\":\"d\",\"2\":\"5\",\"3\":\"9\",\"4\":\"9.782609\"},{\"1\":\"d\",\"2\":\"6\",\"3\":\"8\",\"4\":\"8.695652\"},{\"1\":\"d\",\"2\":\"7\",\"3\":\"5\",\"4\":\"5.434783\"},{\"1\":\"d\",\"2\":\"8\",\"3\":\"8\",\"4\":\"8.695652\"},{\"1\":\"d\",\"2\":\"9\",\"3\":\"7\",\"4\":\"7.608696\"},{\"1\":\"d\",\"2\":\"10\",\"3\":\"9\",\"4\":\"9.782609\"},{\"1\":\"e\",\"2\":\"1\",\"3\":\"9\",\"4\":\"8.411215\"},{\"1\":\"e\",\"2\":\"2\",\"3\":\"17\",\"4\":\"15.887850\"},{\"1\":\"e\",\"2\":\"3\",\"3\":\"6\",\"4\":\"5.607477\"},{\"1\":\"e\",\"2\":\"4\",\"3\":\"12\",\"4\":\"11.214953\"},{\"1\":\"e\",\"2\":\"5\",\"3\":\"8\",\"4\":\"7.476636\"},{\"1\":\"e\",\"2\":\"6\",\"3\":\"6\",\"4\":\"5.607477\"},{\"1\":\"e\",\"2\":\"7\",\"3\":\"10\",\"4\":\"9.345794\"},{\"1\":\"e\",\"2\":\"8\",\"3\":\"9\",\"4\":\"8.411215\"},{\"1\":\"e\",\"2\":\"9\",\"3\":\"15\",\"4\":\"14.018692\"},{\"1\":\"e\",\"2\":\"10\",\"3\":\"15\",\"4\":\"14.018692\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\r\r","date":1571184000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572494298,"objectID":"4a92e0aebe5c6434d1b9a7aea48185fc","permalink":"/publication/problem-2/","publishdate":"2019-10-16T00:00:00Z","relpermalink":"/publication/problem-2/","section":"publication","summary":"I´ll be solving a Tidy Evaluation Problem.","tags":["Quick Solves","Tidy Evaluation","curly-curly"],"title":"Tidy Evaluation","type":"publication"},{"authors":["Alfonso Tobar"],"categories":["Quick Solves"],"content":"\r\rThis is a pretty typical issue. Specially when you have dealing with data a long time you just stop seeing obvious things, and you just can´t find solution to inexistant problems. For instance:\nmtcars %\u0026gt;%\rfilter(cyl \u0026lt; 4)\r{\"columns\":[{\"label\":[\"mpg\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"cyl\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"disp\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"hp\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"drat\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"wt\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"qsec\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"vs\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"am\"],\"name\":[9],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"gear\"],\"name\":[10],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"carb\"],\"name\":[11],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\rYou want to get the rows having cyl less or equal to 4 and for quite a while you keep getting 0 results.\rObviously something is wrong with the code but you just can´t notice it.\nThe Solution\rWell tidylog can give you an idea. Just load tidylog and watch:\n#loading the package this way to avoid verbose messages\rlibrary(tidylog)\rmtcars %\u0026gt;%\rfilter(cyl \u0026lt; 4)\r## filter: removed all rows (100%)\r{\"columns\":[{\"label\":[\"mpg\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"cyl\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"disp\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"hp\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"drat\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"wt\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"qsec\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"vs\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"am\"],\"name\":[9],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"gear\"],\"name\":[10],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"carb\"],\"name\":[11],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\rTidylog produces short log messages for dplyr and tidyr operations that help you understand what is happening with the data. Here definitely filter is incorrect, not producing an error but removing the 100% of the data, that is not what I was looking for.\nEverytime you build a pipeline, tidylog will tell what is happening:\nmtcars %\u0026gt;%\rfilter(cyl \u0026gt; 4) %\u0026gt;%\rselect(-disp) %\u0026gt;%\rmutate( overall = rowMeans(.)) %\u0026gt;%\rsummarize_all( ~ mean(.))\r## filter: removed 11 rows (34%), 21 rows remaining\r## select: dropped one variable (disp)\r## mutate: new variable \u0026#39;overall\u0026#39; with 21 unique values and 0% NA\r## summarize_all: now one row and 11 columns, ungrouped\r{\"columns\":[{\"label\":[\"mpg\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"cyl\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"hp\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"drat\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"wt\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"qsec\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"vs\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"am\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"gear\"],\"name\":[9],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"carb\"],\"name\":[10],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"overall\"],\"name\":[11],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"16.64762\",\"2\":\"7.333333\",\"3\":\"180.2381\",\"4\":\"3.348095\",\"5\":\"3.70519\",\"6\":\"17.17381\",\"7\":\"0.1904762\",\"8\":\"0.2380952\",\"9\":\"3.47619\",\"10\":\"3.47619\",\"11\":\"23.58271\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\r\r","date":1571184000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572494298,"objectID":"5dddf605b5bf45d0ce08c08a620acd2c","permalink":"/publication/problem-3/","publishdate":"2019-10-16T00:00:00Z","relpermalink":"/publication/problem-3/","section":"publication","summary":"How to avoid easy errors by getting short summaries of the operations applied to data.","tags":["Quick Solves","tidylog"],"title":"Why this is failing?","type":"publication"},{"authors":["Alfonso Tobar"],"categories":["Blog","Rambling"],"content":"\rLatin R (Conferences Days)\rToday the Conference Days just got started, and I have to say I watched some really impresive presentations and Data Products.\nDay 1 started with a UAI local presenter talking about Sports Analytics in R, then a huge presentation by Mine Cetinkaya about Teaching R to move to specific small presentation in parallel so I was able just to watch half of the Presentations.\nDay 2 started with a UC local presenter talking about applying Data Science in R to Government Data, then Erin Ledell talked about H2o to move forward to the small presentations in Parallel. The day finished with Poster presentations and Hadley Wickham talking about different dplyr backends.\n\rLatinR Summary\rSports Analytics\r\r#LatinR2019 partiendo con datos espaciales. Hermoso :) pic.twitter.com/xUCYltzJNi\n\u0026mdash; Steph Orellana Bello (@sporella) September 26, 2019  \r\rAhora @raimun2 nos cuenta cómo generó sus mapas para analítica de deportes de montaña 🗺\n❇️ 25% de la población tiene un cerro a menos de 3 km de distancia\n❇️ 100% un cerro a menos de 10 km de distancia pic.twitter.com/l8ziSS7yXX\n\u0026mdash; LatinR (@LatinR_Conf) September 26, 2019  \rThe Conference kicked off with Spatial Data applied to some kind of Mountain Trekking. The presenter showed how to use R to calculate the distances from every Santiago door to have access to the Mountains.\nThe most interesting thing about this is that this showed the huge power of R in the Spatial Data side, that is defnitely something I have no idea.\nDuring the Presentation the following packages were presented:\n\rggmap: of Course from the ggplot family this package allows to work with Stamen maps that I think is some kind of Raster (Map Images).\n\rElevatR: to get access to Topographic Maps with Elevations for free.\n\rRstrava: An API to get access to trekking routes.\n\r\rI think this was some kind of Introduction to really interesting Spatial Presentations that showed me that there is a lot to learn about that.\n\rTeaching R\rOne of my passions is Teaching R, that is why this talk was specially touching to me. I worked hard during my time in EVS to have the Best R classes possible and we made it, a lot of people learned R but there was still a lot of things I know I did wrong and a lot of Tips that Mine presented that I have never thougth about.\n\r.@minebocek hablando sobre la enseñanza de R. Las diapositivas disponibles en este enlace https://t.co/GEtWKB5vjV pic.twitter.com/aLvWQJruVv\n\u0026mdash; LatinR (@LatinR_Conf) September 26, 2019  \rThe whole talk can be found here but there was a couple of things that I think it is important to highlight:\n\rUsing Students feedback earlier: She explained is really helpful to give our students the chance to raise questions and how the material can be improved as soon as possible, speially little things like font size, voice volume, backgorund color, contents and course expectations.\rHelp them get help: I think the best things here were {searcher}, a pakage to make automatic searchs in google or stackoverflow about errors, the well-known {reprex} and this super helpful explanation about R help format:\r\r\r\r\r{livecode}: Mine remarked the importance of live coding to explain workflows and give confidence to students, plus it helps to express the correct way to refer to R elements. the {livecode} package is being developed and this could be a life-changer tool for people who likes to teach.\rPeer review: The R community is really open and we need to encourage the constructive feedback to make each other a better useR.________\rEncourage creativity: I just loved this point speially in the kind of challenges that can be given, for instance, creating a Christmas tree with R.\r\r\r\r\ralicer package\rFor me this was the most mature and powerful data Product presented during the Conference.\n\r.@minebocek hablando sobre la enseñanza de R. Las diapositivas disponibles en este enlace https://t.co/GEtWKB5vjV pic.twitter.com/aLvWQJruVv\n\u0026mdash; LatinR (@LatinR_Conf) September 26, 2019  \rMy main takeaways from this presentation are:\n\rYou don´t need to be Hadley to create an awesome package.\rUse {usethis}.\rGive clear instructions to users.\rIf you have Functions used 3 times, it´s time to create a Package.\r\r\rHadley´s Talk\rThis is always I had waited for a long time. I used to follow the different talks he is giving around the world, because he is always presenting interesting things and that day he presented dplyr backends. Of course dbplyr is super interesting and I don´t want to overlook it, but I already knew it, and use it oftenly, so not very impressed. But {dtplyr} I think is something I had been waiting for a while.\nDuring the start of the year I was stalking Hadley´s Repos and I just found this abandoned dtplyr, and he decided to revisit during this year and they just came up with something awesome. I don´t like data.table just because of the syntax, and I love dplyr just because of the syntax, so ombining effiiency that data.table has with dplyr simplicity is just the best thing ever, and a great way to show the collaborative spirit that exist in the R Community.\nJust a fun fact, I asked about some new possible backends for arrays, and Hadley presented the {rray} package (check it out). For some reason he wanted to show the package logo, and he couldn´t so he opened a PR just to show me that. The fun thing is that David Vaughan the maintainer, quickly responded to that.\n\rI feel honored that @hadleywickham raised a PR just to show me this awesome {rray} logo at @LatinR_Conf. And the best thing was that it was responded pretty quick. XD#LatinR2019 https://t.co/i5hbIe5Cns\n\u0026mdash; Alfonso Tobar (@tobar_with_R) September 28, 2019  \rMain takeaways:\n\rHadley is a great teacher and presenter.\rI can´t believe how creative Hadley is and I know a lot of interesting things I just can´t imagine will start to come up in R.\rI just learned sparklyr is a dplyr implementation in spark (sorry it was not presented).\rI want to work in RStudio, some time.\r\r\r\r","date":1569801600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572494298,"objectID":"d7787206cbf2ba1b7919cec3132cfe9c","permalink":"/post/latin-r-ii/","publishdate":"2019-09-30T00:00:00Z","relpermalink":"/post/latin-r-ii/","section":"post","summary":"I took 2 tutorials at the LatinR Conference. I´ll be commenting about them.","tags":["R","Conference","Latin R"],"title":"Latin R (Days 2 and 3)","type":"post"},{"authors":["Alfonso Tobar"],"categories":["Blog","Rambling"],"content":"\rLatin R (Tutorials Day)\rToday Latin R just got started. It was really impressive to see how many people actually uses R. I have to say the popularity of Python in Data Science has always been something that worries me tons, but today I was able to see a lot of other Areas where people actually use R. Probably I was one of the few people working in Data Science there.\nRegarding Tutorials, they were awesome. 4 Tutorials were conducted today.\nDuring Morning, Mine Çetinkaya-Rundel taught about Teaching R. I wasn´t there so I cannot comment a lot. In parallel Erin Ledell taught about using H2o for Machine Learning.\nIn the afternoon, Joshua Kunst taught about Highcharter with plotly for data Viz and of course Hadley Wickham was in charge of teaching about Package Development.\n\rH2o Tutorial\r\r\rErin Ledell conducted this tutorial that was more like a demonstration of the capabilities of H2o. I have to say I had heard about H2o in Matt Danchos’ tutorials but I never got impressed because I´m a super fan of Tidymodels and H2o is not there yet.\nFor me I think a great tool was presented. H2o is a Java implementation of several Machine Learning Models. This includes Pre-processing, Grid Search, Cross Validation, Stacked Models and running in Clusters using CPU and GPU, The best is that it´s absolutely free.\nPros\r\rSuper easy and Intuitive syntax. Very similar to parsnip.\rSuper fast implementation in Java.\rIt has a variety of Models including GLM, Random Forest, SVM, even some Deep Learning things.\rIt has a localhost implementation with a GUI interface for non-coders.\rRuns super smoothly in Rstudio.cloud.\rOffers Stacked Models and AutoML algorithms.\rSuper Easy Implementation into Production.\r\r\rDownsides\r\rIt needs Java 8-12 to be installed. And Installing it is a pain.\rxgboost is not implemented for Windows users (this is a huge setback).\rClassification or Regression Problems are detected depending on the data type of the Target Variable. (Not a huge issue but I like to have control over that).\rIt tends to oversimplify things running things behind the scenes to facilitate user experience, but you don´t always are aware of things happening.\r\r\rOverall\rDon´t get me wrong. H2o is awesome and a great starting point for people recently learning about Machine Learning and for experienced Machine Learning people that want speed and scalability.\nIt also offers AutoML and stacked ensembles that with little work can help to achieve excellent performance.\nAnother think I liked, more like a side note, was that running this into Rstudio cloud was super smooth. I just had to install the h2o package as any other R package and that was it. The internet was not the best and even so everything ran super fast.\nFinally Erin mentioned that Max Kuhn and his team is working on integrating H2o with the TidyModels ecosystem. If that happens H2o will start being definitely one of my favorites even more.\nI think I will share a short tutorial about the commands I learned during the tutorial.\n\r\rPackage Development Tutorial\r\r\rHadley Wickham was in charge of this Tutorial and it was huge.\nThe content was not a big deal, he showed the necessary steps to create a package that is actually easier than expected. But the way he directed the class:\n\r4 to 5 TAs to help people in need by using a Post-it signal to ask for help without interrupting the class.\rA lot of hands-on exercises.\rMakes us meet our neighbors to work peer to peer.\rAnd the usethis package.\r\rThe usethis Package was a huge deal. I had heard about it but I never dimensioned how powerful it is.\nFirst it helps simplify really tedious process in the Package development such as the Creation of Package Directories, edit .Rprofile file, even share Material or Courses. It also creates test files to run with testthat (another super great package), helps create vignettes, upload to github, set Travis and create pkgdown sites.\nI think usethis is one of the great great things I take away during this Tutorial. Another great surprise is the utility of roxygen2. I know that is THE package for Documentation purposes but today I really understood how important it is. It really simplifies Documentation creation but also helps compile all the tedious Documentation files that are mandatory to pass CRAN checks.\n\rLessons learned\r\rTo value my job. Hadley had all of its work, including PDF files licensed and I think that gives value to the things you do.\rIf a package is on CRAN is because the creator put a lot of love in it. Because submitting is so convoluted and tedious that if you work that hard to pass CRAN checks is because you really think you are contributing with something that is important to you as creator. Hadley mentioned that submitting to CRAN gives credibility to the author, quality to the actual package because of all of the test that needs to pass to be accepted and a lot of experience as an R Programmer.\rI don´t feel fully prepared to create a huge package yet, but I´m not afraid anymore.\r\rMy goal after this: My Thesis definitely needs to end up into a package. I´ll do my best.\nOne of Hadley best tips:\n\r“Finish your daily work with a test failing so you can now exactly how to resume your work the next day.”\n\rTomorrow is day 2. Stay tuned!!\n\r","date":1569369600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572494298,"objectID":"c540e8a086fea3f6fd5b7761f0361473","permalink":"/post/latin-r-i/","publishdate":"2019-09-25T00:00:00Z","relpermalink":"/post/latin-r-i/","section":"post","summary":"I took 2 tutorials at the LatinR Conference. I´ll be commenting about them.","tags":["R","Conference","Latin R"],"title":"Latin R (Day 1)","type":"post"},{"authors":["Alfonso Tobar"],"categories":["Blog","Rambling"],"content":"\rLatin R\rLatin R is a conference organized by RLadies Latam and is the oportunity we have here in Chile to get acquainted about the last R breakthrough and how is being using in Research, companies, etc.\rThey also provide a full day of Tutorials and of course important keynotes are invited to come over. This year I think is huge, because 3 Main R users are coming:\nMine Çetinkaya-Rundel\rShe is one of the most important persons involved into R Teaching. Statistics Professor from Duke University. I personally had the chance to take some Coursera and Datacamp Courses with her and I learned a lot, really good teacher and really knowledgable. Having her now here in Chile to meet her in person is such a great Honor. I just can´t wait to hear about what she has to say.\rShe will be conducting a Tutorial on how to Teach R and of Course a main Speech.\n\rErin Ledell\rI have to say I don’t know a lot about her but she has a great curriculum: Chief Machine Learning Scientist at H2O, co-founder of Rladies Global and Woman+ in ML/DS, plus BioStatistics Phd at Berkeley, that is more than enough. She will be conducting a Tutorial on Machine Learning and Deep Learning that of course I´ll be taking so I´ll give more details on a later post.\n\rHadley Wickham\rThis Guy needs no Introduction. He is the Chief Scientist at RStudio and he will be giving the Main Speech of the Conference plus a Package Development Tutorial. Of ourse I will be in first row of this Tutorial and I expect to learn a lot and have a lot of questions prepared beforehand.\nThis guys is by far my most important inspiration when it comes to working in R. Definitely this guy has made my life way easier because of all of the packages that he has created and I use almost every day.\nI’ll be the next 3 days in the Conference and I expect to share some pictures and experiences about the things I will learn.\nStay tuned!!!\n\r\r","date":1569283200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572494298,"objectID":"8e89b01640dd805b3b195ff53f61bdb0","permalink":"/post/latin-r/","publishdate":"2019-09-24T00:00:00Z","relpermalink":"/post/latin-r/","section":"post","summary":"Hadley is coming to Chile and I´ll be taking some Package Development Classes with him","tags":["R","Conference"],"title":"Latin R","type":"post"},{"authors":["Alfonso Tobar"],"categories":null,"content":"Coming up with an Interesting Thesis Project is not easy at all. Actually I had 3 different projets and 5 different professors. None of them were really interested in my propositions. Thank God I found Dr. Marcos Valdebenito. He is really interested in Reliability Analysis in Structures and Study the Response of Random Field Variables into Strutures, you can learn more about his work on his website. Once I talked to him about I was doing in my former job he was really interested in applying Machine Learning techniques to solve this kind of problems.\nNormally to study the effect of Ramdom fields in Structures a Montecarlo Simulation is run several times to determine how the Structure response is affected. This is done by analyzing the mean and the Covariance of the simulations. This process is omputationally expensive since normally 10,000 to 1,000,000 simulations are needed. Every one of those simulations solves the following problem, also called the Rayleigh - Ritz Method:\n$$ [K] {u} = {f}$$\nWhere $ [K] $ is the Finite Element Matrix representing the Equivalent Stiffness of the different Degrees of Fredom of the Structure. $ {f} $ is the Equivalent Load Vector representing the forces affecting the Structure. In order to solve this problem $ [K]^{-1} $ needs to be pre-multiplied with $ {f}$ to obtain the Struture Response $ {u} $ representing the Structure displacement at every Degree of freedom. Normally $ [K] $ is a fairly large Matrix and the Inversion process costly so alternative methods to Montecarlo Simulation are deeply appreciated.\nSo that is how we came up with a Project. What about considering $ [K] $ as a black and white image (1 channel), representing the stiffness of a Truss. Therefore, Convolutional Networks could be a good alternative to analyze the Matrix and train a Network capable of determinimg in a first instance the displacement of the Structure (${u}$) and afterwards the failure of the Structure, transforming the problem into a Clasification Binary Problem (Failing - not Failing).\nI\u0026rsquo;ll be posting more technical content about how I\u0026rsquo;ve been tackling the problem. Stay tuned!!!\n","date":1569207600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569387730,"objectID":"ddc06374e9bc1835608820cda1843546","permalink":"/project/my-thesis/","publishdate":"2019-09-23T00:00:00-03:00","relpermalink":"/project/my-thesis/","section":"project","summary":"This is What my Thesis Project will be about.","tags":["Civil Engineering","Finite Elements","Stiffness Method"],"title":"My Thesis","type":"project"},{"authors":["Alfonso Tobar"],"categories":["Basics"],"content":"\r\rThe Method\rThe Problem\rRcpp Basics\rCreating an Rcpp file\rStiffness Method\rStiff Matrix by Element\rActive DoF Assembly\rConnectivity Array\rStiffness Matrix Assembly\rLoad Vector Assembly\rSolving the Problem\r\rConclusions\r\r\r\rThe Method\rThe Rayleigh Ritz Method is nothing but applying Finite Elements to Structural problems. Basically you split your structure into smaller structures that can easily be solved By solving, I mean, Calculate the specific stifness of the Structure in order to determine how the loads affects the structure. Once the individual mini-strutures are solved they are ensembled into a Merged Matrix equivalent to the total Stiffness of the Structure.\nThe purpose of this Document is not get into deep details about the Method. If you want to learn about this you can go to this paper to learn the Maths behind this. The idea is to show how to implement this in R. Since this is a computational expensive method I’ll be using library(Rcpp).\nThe Problem\r\r\rFigure 1: Problem Structure\r\r\rThis is a simple problem and useful to understand the different steps of the Method.\rThis is implementation is for a Truss with 3 Nodes and 3 Elements where:\n#Number of Nodes by Element\rNN_e \u0026lt;- 2\r#Number of Degrees of Freedom (DoF) by Node\rNgl_N \u0026lt;- 2\rL \u0026lt;- 1 #Value of L\rE \u0026lt;- 2 * 10 ^ 11 # Young Module / Elasticity Metric\rA \u0026lt;- 0.0001 # Cross Sectional Area\rP \u0026lt;- 1000 # Load\rThe First and most simple Step is to organize the Input Data. All of the Data will be input in tibble form.\nRow i of the Nodes Matrix will store the X and Y Coordinates for Every Node.\n(Nodes \u0026lt;-\rtibble::tribble(~ Xi, ~ Yi,\r0, 0,\rsqrt(2) / 2 * L, sqrt(2) / 2 * L,\rsqrt(2) * L, 0))\r## # A tibble: 3 x 2\r## Xi Yi\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 0 0 ## 2 0.707 0.707\r## 3 1.41 0\rThe Row j of the Elements Matrix will contain the Initial Node ni, the ending Node nf and the corresponding E and A properties for Element j. In this case all the Elements share the same properties.\n(Elements \u0026lt;-\rtibble::tribble(~ ni, ~ nf, ~ E, ~ A,\r1, 2, E, A,\r2, 3, E, A,\r3, 1, E, A))\r## # A tibble: 3 x 4\r## ni nf E A\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1 2 200000000000 0.0001\r## 2 2 3 200000000000 0.0001\r## 3 3 1 200000000000 0.0001\rThe Row i of the Loads Matrix contains the x and y vectorial component of the Loads for Node i.\n(Loads \u0026lt;-\rtibble::tribble(~ Px, ~ Py,\r0, 0,\r0, P,\r0, 0))\r## # A tibble: 3 x 2\r## Px Py\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 0 0\r## 2 0 1000\r## 3 0 0\rThe Row i correspond to the freedom of the X and Y Component of the Node i. 1 meaning no Movement and 0 meaning free movement.\n(Supports \u0026lt;-\rtibble::tribble(~ Rx, ~ Ry,\r1, 1,\r0, 0,\r0, 1))\r## # A tibble: 3 x 2\r## Rx Ry\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1 1\r## 2 0 0\r## 3 0 1\r\rRcpp Basics\rRcpp is the R API package to access to the huge benefits that C++ offers. I´m not an expert in C++ actually I just learned a bit of C++ because Rcpp offers easy sintax to access to C++ Elements but always showing equivalents in the R Environment.\nC++ is far for being an adequate language for Data Science, but once you want to optimize code or algorithms is definitely the way to go. In these case I´ll be showing the algorithm to the different steps of the Stiffness Method and how can be implemented in Rcpp.\nMy main sources to learn Rcpp were this excellent Rcpp for Everyone and of course Hadley´s Help. With these two resources you should have more than enough to create your first Rcpp functions.\n\rCreating an Rcpp file\r\r\rFigure 2: Create a C++ File\r\r\rIf you work with RStudio you can go to File \u0026gt; New File \u0026gt; C++ File and will open a C++ Template like this:\n\r\rFigure 3: C++ Template\r\r\rThe main thing you need to be aware of is loading the required libraries from C++. In this case we will use the following:\nAll C++ code chunks will be combined to the chunk below:\n// [[Rcpp::depends(RcppEigen)]]\r#include \u0026lt;Rcpp.h\u0026gt;\r#include \u0026lt;RcppEigen.h\u0026gt;\r#include \u0026lt;Eigen/LU\u0026gt; #include \u0026lt;Eigen/Eigenvalues\u0026gt; using namespace Rcpp;\rusing namespace Eigen;\rAs you may know C++ is a compiled language. Compilation means, in really simple words, to optimize and speed up the code making it available in R through functions. If you want functions to be available in the R environment they need to be preceeded by this special comment. Otherwise they can be called from within the C++ environment as intermediate functions but they won´t work in R.\n\rStiffness Method\rStiff Matrix by Element\rThis Step calculates Stiff for the mini-structures, meaning every single bar.\nEvery Element Matrix has the following form that needs to be created according to its properties.\n\\[\r[K]_j=\\begin{bmatrix}\rc^2 \u0026amp; \u0026amp; \u0026amp; sim\\\\\rcs \u0026amp; s^2 \u0026amp; \u0026amp; \\\\\r-c^2 \u0026amp; -cs \u0026amp; c^2 \u0026amp; \\\\\r-cs \u0026amp; -s^2 \u0026amp; cs \u0026amp; s^2 \\\\\r\\end{bmatrix}\r\\]\rThe pseudo code is as follows:\n\\[ Ne \\leftarrow \\text{Number of Rows in the Element Matrix} \\\\\rc \\leftarrow \\text{ Sparse Matrix for Director Cosines, Dimension Ne x 1 } \\\\\rs \\leftarrow \\text{ Sparse Matrix for Director Sinus, Dimension Ne x 1 } \\\\\rL \\leftarrow \\text{ Sparse Matrix for Element Length, Dimension Ne x 1 } \\\\\r\\text{for j = 1 to Ne do}\r\\left\\{ \\begin{array}{lcc}\rNi=Elements(j,1) \\\\ Nf=Elements(j,2) \\\\\r\\Delta x = Nodes(Nf,1) - Nodes(Ni,1) \\\\\r\\Delta y = Nodes(Nf,2) - Nodes(Ni,2) \\\\\rL(j)=\\sqrt{\\Delta x^2 + \\Delta y^2} \\\\\rc(j) = {\\Delta x\\over L(j)} \\\\\rs(j) = {\\Delta y\\over L(j)}\r\\end{array}\r\\right.\r\\]\nNow translating this into Rcpp looks like this:\n\rYou need define every object to use preceeded by its type.\rThe output will be an R List since I want object storing the different Element Matrix.\rAll of the Function arguments are Mandatory by default and need to go in the same order that will be used. If an Optional Argument is needed the default value needs to be defined as in NN_e.\r\r// [[Rcpp::export]]\r// First you define the Output Type. In this case an R List.\rList K_Element(NumericMatrix Nodes, NumericMatrix Elements, int NN_e = 2){\r// Ne is defined by using the nrow method to calculate number of rows.\rint Num_Elements = Elements.nrow();\r// c, s and L are defined Vectors since the second Dimension is 1.\rNumericVector c (Num_Elements);\rNumericVector s (Num_Elements);\rNumericVector L (Num_Elements);\rint j,Ni,Nf;\r// dx and dy are defined as doubles since they can contain decimals\rdouble dx,dy;\rList K_list (Num_Elements);\r// C++ is defined from 0 as the first element. So the pseudo code needs to be adjusted accordingly.\r// Notice the for syntax, from 0 to NE-1 defined as j\u0026lt;Num_Elementos and the ++j iterator\rfor(j=0;j\u0026lt;Num_Elements;++j){\rNi=Elements(j,0) -1;\rNf=Elements(j,1) - 1;\rdx=Nodes(Nf,0)-Nodes(Ni,0);\rdy=Nodes(Nf,1)-Nodes(Ni,1);\r//pow is the C++ operator for ^\rL[j]=sqrt(pow(dx,2)+pow(dy,2));\rc(j)=dx/L(j);\rs(j)=dy/L(j);\r// This is a special way to define a Matrix by Element coming from library(RcppEigen)\rMatrix4f ke;\rke \u0026lt;\u0026lt; pow(c[j],2),c[j]*s[j],-pow(c[j],2),-c[j]*s[j],\rc[j]*s[j],pow(s[j],2),-c[j]*s[j], -pow(s[j],2),\r-pow(c[j],2),-c[j]*s[j],pow(c[j],2),c[j]*s[j],\r-c[j]*s[j],-pow(s[j],2),c[j]*s[j],pow(s[j],2);\r//Here you populate every List Element with the corresponding Element Matrix\rK_list[j]= Elements(j,NN_e)*Elements(j,NN_e + 1)/L[j]*ke; }\rreturn K_list;\r}\r/*** R\r(K_E \u0026lt;- K_Element(Nodes,Elements))\r*/\r\rActive DoF Assembly\rThe Stiffness Method needs to determine what Dof are actually active, meaning that are free to move, hence are unknowns of the equation of the problem.\rIn order to do that it is necessary to determine which ones are free to move depending on the support Matrix and a Position Number is assigned to them.\nPseudocode as follows:\n\\[ Nn \\leftarrow \\text{Number of Rows in the Node Matrix} \\\\\rGl_act \\leftarrow \\text{ Sparse Matrix Dimension (NN \\cdot Ngl_N) x 1 } \\\\\rcont = 0 \\\\\r\\begin{aligned}\r\u0026amp; \\text{for i = 1 to Nn do } \\\\\r\u0026amp; \\text{for k = 1 to Ngl_N do} \\\\\r\\end{aligned} \\\\\r\\left\\{ \\begin{array}{lcc}\r\\text{if Apoyos(i,k) = 0 then} \\\\\rcont= cont +1 \\\\\rpos=Ngl_N \\cdot (i-1) + k \\\\\rGl_act(pos)=cont \\\\\r\\end{array}\r\\right.\r\\]\rRcpp Code:\n// [[Rcpp::export]] // Sparse Vector that uses Support Matrix as Input NumericVector Gr_Active(NumericMatrix Support, int Ngl_N = 2){\rint Num_Nodes = Support.nrow();\rint cont=0, i, k;\r//Defining Dimension of Gl Vector\rNumericVector Gl (Num_Nodes*Ngl_N);\rfor(i = 0; i \u0026lt; Num_Nodes; ++i){\rfor(k = 0; k \u0026lt; Ngl_N; ++k){\rif(Apoyos(i,k)==0){\r//Counter needs to be adapted since C++ starts off at Zero\rGl[Ngl_N*i+k] = ++cont;\r}\r} }\rreturn Gl;\r}\r\r\rConnectivity Array\rThe Method determines an array to identify how the different elements are connected each other. This way it is possible to create an equivalent Matrix representing the Equivalent Stiffness of the ensembled elements.\n\\[ Ngle = Ngl_N \\cdot NN_e \\\\\rconect \\leftarrow \\text{ Sparse Matrix Dimension Ne x Ngle } \\\\\r\\begin{aligned}\r\u0026amp; \\text{for j = 1 to Ne do } \\\\\r\u0026amp; \\text{for k = 1 to NN_e do} \\\\\r\u0026amp; N_k=Elementos(j,k) \\\\\r\u0026amp; pos1= (N_k - 1) \\cdot Ngl_N \\\\\r\\end{aligned} \\\\\r\\text{ for l= 1 to Ngl_N do } \\\\\r\\left\\{ \\begin{array}{lcc}\rpos2=pos1+l \\\\\rpos3= (k-1) \\cdot Ngl_N + l \\\\\rconect(j,pos3) = Gl_act(pos2) \\\\\r\\end{array}\r\\right.\r\\]\rRcpp Code:\n// [[Rcpp::export]]\r// This is a Numeric Matrix using Elements Matrix and Gl Vector as Input\rNumericMatrix Arr_Connect(NumericMatrix Elements, NumericVector Gl, int NN_e = 2, int Ngl_N = 2){\rint Num_Elements = Elements.nrow();\r// Several counters an be defined simultaneously if sharing the same properties.\rint j, k, l, pos1, pos2, pos3;\rNumericMatrix conect(Num_Elements, NN_e * Ngl_N);\rfor(j=0; j \u0026lt; Num_Elements; ++j){\rfor(k=0; k \u0026lt; NN_e; ++k){\rpos1 = (Elements(j,k) - 1) * Ngl_N;\rfor(l=0; l \u0026lt; Ngl_N; ++l){\rpos2 = pos1 + l;\r// pos3 had to be adjusted because C++ index starting at 0 pos3 = k * Ngl_N + l;\rconect(j,pos3) = Gl[pos2];\r}\r}\r}\rreturn conect;\r}\r\rStiffness Matrix Assembly\rOnce the Connectivity Array and the Active DoFs are determined the Global Stiffness Matrix can be assembled. This matrix contains the Contribution of every element to an specific Node. Less Elements joined to a specific Node will end up adding less stiffness than a lot of elements being part of a Node.\nPseudocode as follows:\n\\[ N_R \\leftarrow \\text{ sum of all of the entries of the support Matrix } \\\\\rNGl_total = Ngl_N \\cdot Nn - N_R \\\\\rK \\leftarrow \\text{ Sparse Matrix Ngl_total x Ngl_total } \\\\\r\\begin{aligned}\r\u0026amp; \\text{for j = 1 to Ne do } \\\\\r\u0026amp; \\text{for k = 1 to Ngle do} \\\\\r\\end{aligned} \\\\\r\\text{ for l= 1 to Ngl_e do } \\\\\r\\left\\{ \\begin{array}{lcc}\rpos1=conect(j,k) \\\\\rpos2=conect(j,l) \\\\\rtext{ if conect(j,k) \\neq 0 and conect(j,l) \\neq 0 then } \\\\\rK(pos1,pos2)=K_E{j}(k,l) + K(pos1,pos2) \\\\\r\\end{array}\r\\right.\r\\]\rRcpp Code:\n// [[Rcpp::export]]\r//Numeric Matrix using Support, Gl and Conect Matrix and K_E List as Inputs\rNumericMatrix K_Total(List K_E, NumericMatrix Support, NumericVector Gl, NumericMatrix conect, int NN_e = 2, int Ngl_N =2 ){\rint Num_Elements = K_E.length();\rint Num_Nodes = Support.nrow();\rint Nr=sum(Support), j, k, l, pos1, pos2;\rNumericMatrix K( Ngl_N * Num_Nodos- Nr );\rint Ngl_E = NN_e * Ngl_N;\rfor(j=0; j\u0026lt;Num_Elements; ++j){\rfor(k=0; k\u0026lt;Ngl_E; ++k){\rfor(l=0; l\u0026lt;Ngl_E; ++l){\rpos1 = conect(j,k);\rpos2 = conect(j,l);\r//Notice that List Elements need to be pulled using brakets\rNumericMatrix Ke = K_E[j];\r// and operator uses double ampersand and inequality syntax follow same rules than R\rif(pos1 != 0 \u0026amp;\u0026amp; pos2 !=0){\r// += is the C++ operator to sum the new value to the current one.\rK(pos1 - 1, pos2 - 1) += Ke(k,l);\r}\r}\r}\r}\rreturn K;\r}\r\r\rLoad Vector Assembly\rThis is the equivalent load Vector considering only Loads for active DoFs that are participating in the solution of the problem.\n\\[ F \\leftarrow \\text{ Sparse Matrix dimension Ngl_total x 1 } \\\\\r\\begin{aligned}\r\u0026amp; \\text{for i = 1 to Nn do } \\\\\r\\end{aligned} \\\\\r\\text{ for k= 1 to Ngl_n do } \\\\\r\\left\\{ \\begin{array}{lcc}\rpos1=Ngl_n \\cdot (i-1) + k \\\\\rpos2=Gl_act(pos1) \\\\\r\\text{ if pos2 Loads(i,k) } \\\\\rF(pos2)=Cargas(i,k)\\\\\r\\end{array}\r\\right.\r\\]\rRcpp Code:\n// [[Rcpp::export]]\rNumericVector f_Total(NumericMatrix Loads, NumericVector Gl, int Nr, int Ngl_N = 2 ){\rint Num_Nodos = Loads.nrow();\rint N_t = Ngl_N * Num_Nodos - Nr;\rNumericVector F (N_t);\rint i,k,pos1,pos2;\rfor(i=0; i \u0026lt; Num_Nodos; ++i){\rfor(k=0; k \u0026lt; Ngl_N; ++k){\rpos1 = Ngl_N * i + k;\rpos2 = Gl[pos1];\rif(pos2 != 0){\rF[pos2 - 1] = Cargas(i,k);\r}\r}\r}\rreturn F;\r}\r\rSolving the Problem\rAll this Steps allows to pose the following problem:\n\\[ [K] \\cdot \\{u\\} = \\{F\\} \\]\nIn order to get the desired displacements it is just necessary to inverse $ [K] $.\n\\[ \\{u\\} = [K]^{-1} \\cdot \\{F\\}\\]\nFor this case I´ll be using RcppEigen, a Rcpp Linear Algebra Library that allows some extra Matrix operations that are useful for, in this case, Matrix inversion:\n// I have defined a new object type called MapMatd whih is a Matrix with no specific size of doubles\rtypedef Map\u0026lt;MatrixXd\u0026gt; MapMatd;\r// Defined a Vector with same characteristics as before\rtypedef Map\u0026lt;VectorXd\u0026gt; MapVecd;\r// [[Rcpp::export]]\r// I use a VectorXd non defined size X with double data type d\rVectorXd u_vect(NumericMatrix K_Total, NumericVector f_Total){\r//I need to cast R Objects coming from Inputs into Eigen Objects. In this case i would just say trust me.\rconst MapMatd K(as\u0026lt;MapMatd\u0026gt;(K_Total));\rconst MapVecd f(as\u0026lt;MapVecd\u0026gt;(f_Total));\r//Applying Inverse Method, this is only available because K and f are already Eigen objets\rVectorXd result = K.inverse()*f;\rreturn result;\r}\r\r\rConclusions\r\rR and Rcpp share a very similar syntax.\rAll R objects are compatible with Rcpp, even Lists\rThe Main advantage of using Rcpp is that is way too faster than Regular R. This makes it especially suitable for Algorithms and Matrix manipulation.\rNotice that Matrices use () for indexing whereas Vectors and Lists use [].\rRcpp starts at 0, make the proper adjustments when dealing with indices.\r\rI´ll be posting another Entry using the recently reated functions to show how fast they are. Stay tuned!!\n\r\r","date":1569196800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569387730,"objectID":"aa94fc09c8d96ba39ca46ac3f1b7b171","permalink":"/project/stiffness-method/","publishdate":"2019-09-23T00:00:00Z","relpermalink":"/project/stiffness-method/","section":"project","summary":"How to Implement the Stiffness Method using Rcpp","tags":["Civil Engineering","Stiffness Method"],"title":"The Stiffness Method","type":"project"},{"authors":["Alfonso Tobar"],"categories":["Tutorial"],"content":"\r\r\r1 Tidymodels\r1.1 Spliting the Data\r1.2 Pre Processing\r\r2 Create the Logistic Regression\r3 Conclusions\r\r\rNow we have an idea on how the data looks like it is time to Model.\n1 Tidymodels\rI´m a huge fan of tidymodels framework and the way Max Kuhn has put together all of this system. I´ll be using several packages from this framework in order to show different steps of the Machine Learning Process.\n1.1 Spliting the Data\rWe will be splitting Data into Training and Test Sets with 70/30 proportion based on the Ins Response Variable.\nlibrary(rsample)\r#Reproducibility\rset.seed(27101986)\r#70/30 Split stratifying the Target Variable Ins\rsplit \u0026lt;- initial_split(data, prop = 0.7, strata = \u0026quot;Ins\u0026quot;)\rdata_training \u0026lt;- split %\u0026gt;% training()\rdata_testing \u0026lt;- split %\u0026gt;% testing()\r\r1.2 Pre Processing\rAfter splitting Data I will be conducting Pre Processing with the great Recipes Package.\nRecipes basically mimics the Pre Processing Steps to a Baking Recipe following different sequential steps in order to prepare and Bake the Data (Make the Data Ready to Model).\nRecipes have step_* functions in charge of applying different Pre-Processing Steps. Plus includes Variable helpers to call Variables by Type or Role.\nlibrary(recipes)\r## ## Attaching package: \u0026#39;recipes\u0026#39;\r## The following object is masked from \u0026#39;package:stringr\u0026#39;:\r## ## fixed\r## The following object is masked from \u0026#39;package:stats\u0026#39;:\r## ## step\r# Sets the Recipe indicating that Ins will be modeled using all the rest of the variables\radvance_rec \u0026lt;- recipe(Ins ~ . , data = data_training) %\u0026gt;%\rstep_dummy(all_nominal(), -all_outcomes()) %\u0026gt;% #create dummy variables for all categorical variables excepting the Ins Variable\rstep_nzv(all_numeric()) %\u0026gt;% #eliminates numerical variables with variance near to zero\rstep_corr(all_predictors()) %\u0026gt;% #eliminates highly correlated variables\rstep_BoxCox(all_predictors()) %\u0026gt;% #fix highly skewed variables\rstep_center(all_numeric()) %\u0026gt;% #substracts mean\rstep_scale(all_numeric()) %\u0026gt;% #divides by sd. This both steps standardize the variables\r#Prepares the data according to the data in the Training Set\rprep(training = data_training)\r#Applies Training Data according to Preprocessing\rtrain_advance \u0026lt;- bake(advance_rec, new_data = data_training)\r#The main difference with Bake is that Bake skips the processes affecting the outcome variable, suh as resamples, logs transform, etc. test_advance \u0026lt;- bake(advance_rec, new_data = data_testing)\r\r\r2 Create the Logistic Regression\rWe will use Logistic regresson using Parsnip and we will Assess the Model using yardstick\nlibrary(parsnip)\r#Using Parsnip to run classification, using glm engine and fitting train data already pre-processed\rfull_advance \u0026lt;- logistic_reg(mode = \u0026quot;classification\u0026quot;) %\u0026gt;%\rset_engine(\u0026quot;glm\u0026quot;) %\u0026gt;%\rfit(Ins ~ ., data = train_advance)\r#Predicting Class with Model \u0026quot;Full Advance\u0026quot; in the Test Set\rfull_pred_advance \u0026lt;- full_advance %\u0026gt;%\rpredict(new_data= test_advance, type = \u0026quot;class\u0026quot;)\r#Predicting class Probabilities with Model \u0026quot;Full Advance\u0026quot;\rfull_pred_probs_advance \u0026lt;- full_advance %\u0026gt;%\rpredict(new_data= test_advance, type = \u0026quot;prob\u0026quot;)\rlibrary(yardstick)\r## For binary classification, the first factor level is assumed to be the event.\r## Set the global option `yardstick.event_first` to `FALSE` to change this.\r## ## Attaching package: \u0026#39;yardstick\u0026#39;\r## The following object is masked from \u0026#39;package:readr\u0026#39;:\r## ## spec\rcomparison_test \u0026lt;- bind_cols(\r\u0026quot;Real\u0026quot; = test_advance$Ins,\r\u0026quot;Prediction\u0026quot; = full_pred_advance,\r\u0026quot;Class1\u0026quot; = full_pred_probs_advance$.pred_yes\r) %\u0026gt;% setNames(c(\u0026quot;Real\u0026quot;,\u0026quot;Prediction\u0026quot;,\u0026quot;Class1\u0026quot;))\r#Calculating Confusion Matrix\rcomparison_test %\u0026gt;% conf_mat(Real,Prediction)\r## Truth\r## Prediction yes no\r## yes 1003 464\r## no 1248 3547\r#Calculating Assesment Metrics for Model\rcomparison_test %\u0026gt;% conf_mat(Real,Prediction) %\u0026gt;%\rsummary()\r{\"columns\":[{\"label\":[\".metric\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\".estimator\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\".estimate\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"accuracy\",\"2\":\"binary\",\"3\":\"0.7266049\"},{\"1\":\"kap\",\"2\":\"binary\",\"3\":\"0.3571922\"},{\"1\":\"sens\",\"2\":\"binary\",\"3\":\"0.4455797\"},{\"1\":\"spec\",\"2\":\"binary\",\"3\":\"0.8843181\"},{\"1\":\"ppv\",\"2\":\"binary\",\"3\":\"0.6837082\"},{\"1\":\"npv\",\"2\":\"binary\",\"3\":\"0.7397289\"},{\"1\":\"mcc\",\"2\":\"binary\",\"3\":\"0.3737526\"},{\"1\":\"j_index\",\"2\":\"binary\",\"3\":\"0.3298979\"},{\"1\":\"bal_accuracy\",\"2\":\"binary\",\"3\":\"0.6649489\"},{\"1\":\"detection_prevalence\",\"2\":\"binary\",\"3\":\"0.2342702\"},{\"1\":\"precision\",\"2\":\"binary\",\"3\":\"0.6837082\"},{\"1\":\"recall\",\"2\":\"binary\",\"3\":\"0.4455797\"},{\"1\":\"f_meas\",\"2\":\"binary\",\"3\":\"0.5395374\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\r#ROC Curve\rcomparison_test %\u0026gt;%\rroc_curve(Real,Class1) %\u0026gt;%\rautoplot()\r## Setting direction: controls \u0026lt; cases\r## Warning in coords.roc(curv, x = unique(c(-Inf, options$predictor, Inf)), :\r## An upcoming version of pROC will set the \u0026#39;transpose\u0026#39; argument to FALSE\r## by default. Set transpose = TRUE explicitly to keep the current behavior,\r## or transpose = FALSE to adopt the new one and silence this warning. Type\r## help(coords_transpose) for additional information.\r#ROC AUC\rcomparison_test %\u0026gt;%\rroc_auc(Real,Class1)\r## Setting direction: controls \u0026lt; cases\r{\"columns\":[{\"label\":[\".metric\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\".estimator\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\".estimate\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"roc_auc\",\"2\":\"binary\",\"3\":\"0.7733382\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\r#Calculated Model\rfull_advance$fit %\u0026gt;%\rtidy() \r{\"columns\":[{\"label\":[\"term\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"estimate\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"std.error\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"statistic\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"p.value\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"(Intercept)\",\"2\":\"6.117855e-01\",\"3\":\"0.01993088\",\"4\":\"3.069536e+01\",\"5\":\"6.563661e-207\"},{\"1\":\"AcctAge\",\"2\":\"1.274360e-01\",\"3\":\"0.02041803\",\"4\":\"6.241345e+00\",\"5\":\"4.338251e-10\"},{\"1\":\"DDABal\",\"2\":\"-3.345807e-01\",\"3\":\"0.04233735\",\"4\":\"-7.902732e+00\",\"5\":\"2.728554e-15\"},{\"1\":\"Checks\",\"2\":\"8.001763e-02\",\"3\":\"0.02764014\",\"4\":\"2.894979e+00\",\"5\":\"3.791843e-03\"},{\"1\":\"Phone\",\"2\":\"1.188238e-01\",\"3\":\"0.02661229\",\"4\":\"4.464997e+00\",\"5\":\"8.006974e-06\"},{\"1\":\"Teller\",\"2\":\"-1.992734e-01\",\"3\":\"0.02252055\",\"4\":\"-8.848515e+00\",\"5\":\"8.869169e-19\"},{\"1\":\"SavBal\",\"2\":\"-6.805505e-01\",\"3\":\"0.04626114\",\"4\":\"-1.471106e+01\",\"5\":\"5.474173e-49\"},{\"1\":\"ATMAmt\",\"2\":\"-3.045024e-01\",\"3\":\"0.03644366\",\"4\":\"-8.355428e+00\",\"5\":\"6.519615e-17\"},{\"1\":\"POS\",\"2\":\"9.988335e-02\",\"3\":\"0.03999723\",\"4\":\"2.497256e+00\",\"5\":\"1.251584e-02\"},{\"1\":\"POSAmt\",\"2\":\"-1.015285e-01\",\"3\":\"0.03693182\",\"4\":\"-2.749079e+00\",\"5\":\"5.976290e-03\"},{\"1\":\"CCBal\",\"2\":\"6.859325e-02\",\"3\":\"0.01891700\",\"4\":\"3.626011e+00\",\"5\":\"2.878328e-04\"},{\"1\":\"CCPurc\",\"2\":\"-2.941948e-02\",\"3\":\"0.02026686\",\"4\":\"-1.451605e+00\",\"5\":\"1.466114e-01\"},{\"1\":\"Income\",\"2\":\"1.272866e-01\",\"3\":\"0.03479794\",\"4\":\"3.657877e+00\",\"5\":\"2.543134e-04\"},{\"1\":\"LORes\",\"2\":\"-6.014610e-02\",\"3\":\"0.02823814\",\"4\":\"-2.129960e+00\",\"5\":\"3.317495e-02\"},{\"1\":\"HMVal\",\"2\":\"-1.935050e-01\",\"3\":\"0.03770266\",\"4\":\"-5.132397e+00\",\"5\":\"2.860749e-07\"},{\"1\":\"Age\",\"2\":\"-2.219778e-05\",\"3\":\"0.02288118\",\"4\":\"-9.701325e-04\",\"5\":\"9.992259e-01\"},{\"1\":\"CRScore\",\"2\":\"1.337544e-02\",\"3\":\"0.02203018\",\"4\":\"6.071418e-01\",\"5\":\"5.437568e-01\"},{\"1\":\"Dep\",\"2\":\"1.028510e-01\",\"3\":\"0.03356444\",\"4\":\"3.064284e+00\",\"5\":\"2.181913e-03\"},{\"1\":\"DepAmt\",\"2\":\"-1.901625e-02\",\"3\":\"0.02478175\",\"4\":\"-7.673488e-01\",\"5\":\"4.428742e-01\"},{\"1\":\"DDA_yes\",\"2\":\"2.707683e-01\",\"3\":\"0.02521098\",\"4\":\"1.074009e+01\",\"5\":\"6.597812e-27\"},{\"1\":\"DirDep_yes\",\"2\":\"7.152448e-03\",\"3\":\"0.02218401\",\"4\":\"3.224146e-01\",\"5\":\"7.471386e-01\"},{\"1\":\"NSF_yes\",\"2\":\"-2.370392e-02\",\"3\":\"0.02123988\",\"4\":\"-1.116010e+00\",\"5\":\"2.644177e-01\"},{\"1\":\"Sav_yes\",\"2\":\"-2.547525e-01\",\"3\":\"0.02196270\",\"4\":\"-1.159932e+01\",\"5\":\"4.153464e-31\"},{\"1\":\"ATM_yes\",\"2\":\"1.292603e-01\",\"3\":\"0.02414396\",\"4\":\"5.353733e+00\",\"5\":\"8.615816e-08\"},{\"1\":\"CD_yes\",\"2\":\"-2.918515e-01\",\"3\":\"0.01859470\",\"4\":\"-1.569541e+01\",\"5\":\"1.625822e-55\"},{\"1\":\"LOC_yes\",\"2\":\"6.110583e-03\",\"3\":\"0.01938014\",\"4\":\"3.153013e-01\",\"5\":\"7.525329e-01\"},{\"1\":\"MM_yes\",\"2\":\"-2.589903e-01\",\"3\":\"0.01974444\",\"4\":\"-1.311713e+01\",\"5\":\"2.627075e-39\"},{\"1\":\"CC_yes\",\"2\":\"-1.798393e-01\",\"3\":\"0.02157689\",\"4\":\"-8.334809e+00\",\"5\":\"7.762339e-17\"},{\"1\":\"SDB_yes\",\"2\":\"-3.708755e-02\",\"3\":\"0.01901094\",\"4\":\"-1.950853e+00\",\"5\":\"5.107453e-02\"},{\"1\":\"HMOwn_yes\",\"2\":\"-1.060297e-02\",\"3\":\"0.02920242\",\"4\":\"-3.630853e-01\",\"5\":\"7.165412e-01\"},{\"1\":\"Branch_B2\",\"2\":\"-4.272629e-03\",\"3\":\"0.02524865\",\"4\":\"-1.692221e-01\",\"5\":\"8.656220e-01\"},{\"1\":\"Branch_B3\",\"2\":\"-2.564723e-02\",\"3\":\"0.02535713\",\"4\":\"-1.011440e+00\",\"5\":\"3.118057e-01\"},{\"1\":\"Branch_B1\",\"2\":\"2.605656e-02\",\"3\":\"0.02565672\",\"4\":\"1.015584e+00\",\"5\":\"3.098275e-01\"},{\"1\":\"Branch_B7\",\"2\":\"3.878624e-02\",\"3\":\"0.02309364\",\"4\":\"1.679521e+00\",\"5\":\"9.305058e-02\"},{\"1\":\"Branch_B5\",\"2\":\"-1.824560e-02\",\"3\":\"0.02531664\",\"4\":\"-7.206961e-01\",\"5\":\"4.710965e-01\"},{\"1\":\"Branch_B6\",\"2\":\"-8.867298e-03\",\"3\":\"0.02270427\",\"4\":\"-3.905563e-01\",\"5\":\"6.961252e-01\"},{\"1\":\"Branch_B4\",\"2\":\"-1.583160e-02\",\"3\":\"0.02902725\",\"4\":\"-5.454048e-01\",\"5\":\"5.854752e-01\"},{\"1\":\"Branch_B16\",\"2\":\"1.673182e-01\",\"3\":\"0.02501004\",\"4\":\"6.690040e+00\",\"5\":\"2.231092e-11\"},{\"1\":\"Branch_B8\",\"2\":\"-4.156275e-02\",\"3\":\"0.02204090\",\"4\":\"-1.885710e+00\",\"5\":\"5.933403e-02\"},{\"1\":\"Res_suburb\",\"2\":\"2.037763e-02\",\"3\":\"0.02505362\",\"4\":\"8.133607e-01\",\"5\":\"4.160113e-01\"},{\"1\":\"Res_urban\",\"2\":\"4.361963e-02\",\"3\":\"0.02516920\",\"4\":\"1.733056e+00\",\"5\":\"8.308567e-02\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\r\r3 Conclusions\rWe have run a Machine Learning Process using:\n\rrsamples for splitting data.\rrecipes for Pre-Processing.\rparsnip to fit the model\ryardstick to measure the performance.\r\rFinally 41 variables were kept getting a 72% of accuracy and a 77.3% of ROC AUC.\n\r","date":1566864000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571210825,"objectID":"1648f0dff9399f69ae0d6a3c66ce4a69","permalink":"/project/machine-learning-diploma-iii/","publishdate":"2019-08-27T00:00:00Z","relpermalink":"/project/machine-learning-diploma-iii/","section":"project","summary":"I´ll be showing the TidyModels frame work to create a Machine Learning Model","tags":["Machine Learning","Parsnip","Rsample","Recipes","Yardstick"],"title":"My Final Project at the ML Diploma (Part III)","type":"project"},{"authors":["Alfonso Tobar"],"categories":["Tutorial"],"content":"\r\r\r1 Checking Numerical Distribution\r2 Checking Categorical Variables\r3 Chi-Square Test\r4 Conclusion\r\r\rLast time we conducted a high level cleansing of the data. Now it´s time to understand what is going on in it. In order to do that we´ll use a lot ggplot to visualize the data.\n1 Checking Numerical Distribution\rIn order to do this I should pick Numerical Variables one by one and create a ggplot.\rThis ould actually be quite tedious, why not to use the power of the tidyverse?\nWe will combine select_if and walk 2 to create histograms for every of the 28 Numerical Variables.\n\rNotice that in order to make walk work silently I had to add a print function that will use .x (every column) to create a histogram labeling it with .y that is the actual name of the current .x.\n\r# Take the data\rdata %\u0026gt;%\r# I select only data that is numerical\rselect_if(is.numeric) %\u0026gt;%\r# I use walk 2 where .x is every numerical column seleted by select_if and\r#.y are the names of .x that will be used to add the proper label.\rwalk2(names(.), ~ print( data %\u0026gt;%\rggplot(aes(.x)) + geom_histogram() + labs(x = .y))) \r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r\r2 Checking Categorical Variables\rSomething equivalent can be done with categorical variables to check how they are distributed with the following code:\ndata %\u0026gt;%\rselect_if(is.factor) %\u0026gt;%\rsummary()\r## DDA DirDep NSF Sav ATM CD ## no : 3742 no :14838 no :19044 no :11135 no : 8085 no :18244 ## yes:17135 yes: 6039 yes: 1833 yes: 9742 yes:12792 yes: 2633 ## ## ## ## ## ## IRA LOC ILS MM MTG CC ## no :19837 no :19742 no :19926 no :18588 no :19932 no :10936 ## yes: 1040 yes: 1135 yes: 951 yes: 2289 yes: 945 yes: 9941 ## ## ## ## ## ## SDB HMOwn Moved InArea Ins ## no :18573 no : 9617 no :20251 no : 823 yes: 7504 ## yes: 2304 yes:11260 yes: 626 yes:20054 no :13373 ## ## ## ## ## ## Branch Res Inv ## B4 :4586 rural :5532 no :20272 ## B3 :2332 suburb:7359 yes: 605 ## B1 :2292 urban :7986 ## B5 :2269 ## B2 :2267 ## B16 :1261 ## (Other):5870\rIn case you want something more visual you could go with this:\n# Take the data\rdata %\u0026gt;%\r# I select only data that is numerical\rselect_if(is.factor) %\u0026gt;%\r# I use walk 2 where .x is every numerical column seleted by select_if and\r#.y are the names of .x that will be used to add the proper label.\rwalk2(names(.), ~ print( data %\u0026gt;%\rggplot(aes(.x)) + geom_bar() + labs(x = .y))) \r\r3 Chi-Square Test\rWhat about performing a Chi-Square test to check the relationship between the Response variable and the Categorical Variables.\nLet´s create a NSE function to apply Chi-Square using purrr.\nWe´ll use the Categorical Object created in the previous part to be looped over the chi-square function.\n#Listing all of the Categorical Variables according to Metadata\rcategorical \u0026lt;- c(\u0026quot;ATM\u0026quot;, \u0026quot;Branch\u0026quot;, \u0026quot;CC\u0026quot;, \u0026quot;CD\u0026quot;, \u0026quot;DDA\u0026quot;, \u0026quot;DirDep\u0026quot;, \u0026quot;HMOwn\u0026quot;, \u0026quot;ILS\u0026quot;, \u0026quot;IRA\u0026quot;, \u0026quot;InArea\u0026quot;, \u0026quot;Ins\u0026quot;, \u0026quot;Inv\u0026quot;, \u0026quot;LOC\u0026quot;, \u0026quot;MM\u0026quot;, \u0026quot;MTG\u0026quot;, \u0026quot;Moved\u0026quot;, \u0026quot;NSF\u0026quot;, \u0026quot;Res\u0026quot;, \u0026quot;SDB\u0026quot;, \u0026quot;Sav\u0026quot;)\r#Loading rlang\rsuppressPackageStartupMessages(library(rlang))\r#since I want to use var as a Non Standard Evaluation Variable I need to pass that variable using the Curly-Curly Operator. That way I don´t need to quote variables and can go directly into dplyr functions such as select.\rchi_comparison \u0026lt;- function(var){\rpred \u0026lt;- data %\u0026gt;%\rselect({{ var }})\r#Performs Chi-Square test and returns p.value\rreturn(tibble(p_val = chisq.test(pred, data$Ins)$p.value))\r}\r(independent \u0026lt;- categorical %\u0026gt;%\rmap_dfr(chi_comparison) %\u0026gt;%\rcbind(independent = categorical) %\u0026gt;%\rfilter(p_val \u0026gt; 0.05) )\r{\"columns\":[{\"label\":[\"p_val\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"independent\"],\"name\":[2],\"type\":[\"fctr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"0.1629036\",\"2\":\"HMOwn\"},{\"1\":\"0.5485035\",\"2\":\"ILS\"},{\"1\":\"0.1298597\",\"2\":\"MTG\"},{\"1\":\"0.8984779\",\"2\":\"Moved\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\rThis results in 4 Variables returning a p-value grater than 0.05. This means this variables are independent to the Response Variables, so no relationship between them exist, hence they could be removed from the model to build.\n\r4 Conclusion\rA quick EDA has been performed using ggplot2 combined with purrr and dplyr.\r* It can be seen that Age and CRSore have distribution fairly close to Normal.\r* Income is right skewed.\r* Most of the Numerical Variables are higly concentrated at lower values.\rOn the categorical side:\r* Most of the categorical variables show severe problems with class imbalances.\r* HMOwn, ILS, MTG and Moved seem to have no relationship with the Response Variable.\r* The Response Variable Ins show some imbalances but nothing to severe to be treated in a special way.\nMore to come on this problem. Stay Tuned!!!\n\r","date":1565222400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571210825,"objectID":"87f85a4759e80fcf9b7025246594f550","permalink":"/project/machine-learning-diploma-ii/","publishdate":"2019-08-08T00:00:00Z","relpermalink":"/project/machine-learning-diploma-ii/","section":"project","summary":"Second Part of the ML Diploma Project. This time I´ll be showing some EDA","tags":["Machine Learning","Data Import (haven)","Data Cleaning"],"title":"My Final Project at the ML Diploma (Part II)","type":"project"},{"authors":["Alfonso Tobar"],"categories":["Tutorial"],"content":"\r\r\r1 Importing Data\r2 Redefining Categorical Variables\r3 Discovering Missing Values\r4 Conclusion\r\r\rDuring my Machine Learning Diploma I had the chance to work on a very interesting project that was actually created in SAS. Of course I absolutely refused to use that old fashioned tool and I move everything to R.\nI will try to demonstrate as much of the packages I used to perform this analysis.\n1 Importing Data\rSince the data is coming from a SAS format, it was absolutely necessary to use this incredibly tidyverse package called haven.\nThe code to import the data is super simple and goes like this:\n#Loading silent tidyverse to make normal utility functions available\rsuppressPackageStartupMessages(library(tidyverse))\rlibrary(haven)\rdata \u0026lt;- read_sas(\u0026quot;develop.sas7bdat\u0026quot;)\rThis will import the data into an R object:\n#Showing 10 first Obs\rdata %\u0026gt;%\rhead\r{\"columns\":[{\"label\":[\"AcctAge\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"DDA\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"DDABal\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"CashBk\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Checks\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"DirDep\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"NSF\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"NSFAmt\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Phone\"],\"name\":[9],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Teller\"],\"name\":[10],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Sav\"],\"name\":[11],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"SavBal\"],\"name\":[12],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"ATM\"],\"name\":[13],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"ATMAmt\"],\"name\":[14],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"POS\"],\"name\":[15],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"POSAmt\"],\"name\":[16],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"CD\"],\"name\":[17],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"CDBal\"],\"name\":[18],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"IRA\"],\"name\":[19],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"IRABal\"],\"name\":[20],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"LOC\"],\"name\":[21],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"LOCBal\"],\"name\":[22],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"ILS\"],\"name\":[23],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"ILSBal\"],\"name\":[24],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"MM\"],\"name\":[25],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"MMBal\"],\"name\":[26],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"MMCred\"],\"name\":[27],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"MTG\"],\"name\":[28],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"MTGBal\"],\"name\":[29],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"CC\"],\"name\":[30],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"CCBal\"],\"name\":[31],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"CCPurc\"],\"name\":[32],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"SDB\"],\"name\":[33],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Income\"],\"name\":[34],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"HMOwn\"],\"name\":[35],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"LORes\"],\"name\":[36],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"HMVal\"],\"name\":[37],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Age\"],\"name\":[38],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"CRScore\"],\"name\":[39],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Moved\"],\"name\":[40],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"InArea\"],\"name\":[41],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Ins\"],\"name\":[42],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Branch\"],\"name\":[43],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"Res\"],\"name\":[44],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"Dep\"],\"name\":[45],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"DepAmt\"],\"name\":[46],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Inv\"],\"name\":[47],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"InvBal\"],\"name\":[48],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"0.3\",\"2\":\"1\",\"3\":\"419.27\",\"4\":\"0\",\"5\":\"0\",\"6\":\"0\",\"7\":\"0\",\"8\":\"0\",\"9\":\"0\",\"10\":\"0\",\"11\":\"1\",\"12\":\"10233.72\",\"13\":\"1\",\"14\":\"106.74\",\"15\":\"0\",\"16\":\"0\",\"17\":\"0\",\"18\":\"0\",\"19\":\"0\",\"20\":\"0\",\"21\":\"0\",\"22\":\"0\",\"23\":\"0\",\"24\":\"0\",\"25\":\"0\",\"26\":\"0\",\"27\":\"0\",\"28\":\"0\",\"29\":\"0\",\"30\":\"1\",\"31\":\"483.65\",\"32\":\"0\",\"33\":\"0\",\"34\":\"16\",\"35\":\"1\",\"36\":\"11.0\",\"37\":\"89\",\"38\":\"63\",\"39\":\"696\",\"40\":\"0\",\"41\":\"1\",\"42\":\"1\",\"43\":\"B17\",\"44\":\"R\",\"45\":\"2\",\"46\":\"1170.06\",\"47\":\"0\",\"48\":\"0\"},{\"1\":\"0.7\",\"2\":\"1\",\"3\":\"1986.81\",\"4\":\"0\",\"5\":\"1\",\"6\":\"1\",\"7\":\"0\",\"8\":\"0\",\"9\":\"0\",\"10\":\"0\",\"11\":\"0\",\"12\":\"0.00\",\"13\":\"1\",\"14\":\"268.88\",\"15\":\"0\",\"16\":\"0\",\"17\":\"0\",\"18\":\"0\",\"19\":\"0\",\"20\":\"0\",\"21\":\"0\",\"22\":\"0\",\"23\":\"0\",\"24\":\"0\",\"25\":\"0\",\"26\":\"0\",\"27\":\"0\",\"28\":\"0\",\"29\":\"0\",\"30\":\"1\",\"31\":\"0.00\",\"32\":\"1\",\"33\":\"0\",\"34\":\"4\",\"35\":\"1\",\"36\":\"7.0\",\"37\":\"87\",\"38\":\"51\",\"39\":\"674\",\"40\":\"0\",\"41\":\"1\",\"42\":\"0\",\"43\":\"B2\",\"44\":\"R\",\"45\":\"1\",\"46\":\"446.93\",\"47\":\"0\",\"48\":\"0\"},{\"1\":\"4.1\",\"2\":\"0\",\"3\":\"0.00\",\"4\":\"0\",\"5\":\"0\",\"6\":\"0\",\"7\":\"0\",\"8\":\"0\",\"9\":\"0\",\"10\":\"0\",\"11\":\"0\",\"12\":\"0.00\",\"13\":\"0\",\"14\":\"0.00\",\"15\":\"0\",\"16\":\"0\",\"17\":\"0\",\"18\":\"0\",\"19\":\"0\",\"20\":\"0\",\"21\":\"0\",\"22\":\"0\",\"23\":\"0\",\"24\":\"0\",\"25\":\"0\",\"26\":\"0\",\"27\":\"0\",\"28\":\"0\",\"29\":\"0\",\"30\":\"1\",\"31\":\"0.00\",\"32\":\"0\",\"33\":\"0\",\"34\":\"30\",\"35\":\"1\",\"36\":\"8.5\",\"37\":\"97\",\"38\":\"60\",\"39\":\"640\",\"40\":\"0\",\"41\":\"1\",\"42\":\"1\",\"43\":\"B3\",\"44\":\"S\",\"45\":\"0\",\"46\":\"0.00\",\"47\":\"0\",\"48\":\"0\"},{\"1\":\"0.5\",\"2\":\"1\",\"3\":\"1594.84\",\"4\":\"0\",\"5\":\"1\",\"6\":\"0\",\"7\":\"0\",\"8\":\"0\",\"9\":\"0\",\"10\":\"1\",\"11\":\"1\",\"12\":\"425.06\",\"13\":\"1\",\"14\":\"278.07\",\"15\":\"0\",\"16\":\"0\",\"17\":\"0\",\"18\":\"0\",\"19\":\"0\",\"20\":\"0\",\"21\":\"0\",\"22\":\"0\",\"23\":\"0\",\"24\":\"0\",\"25\":\"0\",\"26\":\"0\",\"27\":\"0\",\"28\":\"0\",\"29\":\"0\",\"30\":\"1\",\"31\":\"65.76\",\"32\":\"0\",\"33\":\"0\",\"34\":\"125\",\"35\":\"1\",\"36\":\"7.5\",\"37\":\"145\",\"38\":\"44\",\"39\":\"672\",\"40\":\"0\",\"41\":\"1\",\"42\":\"0\",\"43\":\"B1\",\"44\":\"S\",\"45\":\"1\",\"46\":\"1144.24\",\"47\":\"0\",\"48\":\"0\"},{\"1\":\"6.7\",\"2\":\"1\",\"3\":\"2813.45\",\"4\":\"0\",\"5\":\"2\",\"6\":\"0\",\"7\":\"0\",\"8\":\"0\",\"9\":\"0\",\"10\":\"5\",\"11\":\"1\",\"12\":\"2716.55\",\"13\":\"0\",\"14\":\"0.00\",\"15\":\"0\",\"16\":\"0\",\"17\":\"0\",\"18\":\"0\",\"19\":\"0\",\"20\":\"0\",\"21\":\"0\",\"22\":\"0\",\"23\":\"0\",\"24\":\"0\",\"25\":\"0\",\"26\":\"0\",\"27\":\"0\",\"28\":\"0\",\"29\":\"0\",\"30\":\"0\",\"31\":\"0.00\",\"32\":\"0\",\"33\":\"0\",\"34\":\"25\",\"35\":\"1\",\"36\":\"6.0\",\"37\":\"101\",\"38\":\"46\",\"39\":\"648\",\"40\":\"0\",\"41\":\"1\",\"42\":\"1\",\"43\":\"B1\",\"44\":\"S\",\"45\":\"2\",\"46\":\"1208.94\",\"47\":\"0\",\"48\":\"0\"},{\"1\":\"12.3\",\"2\":\"1\",\"3\":\"1069.78\",\"4\":\"0\",\"5\":\"13\",\"6\":\"1\",\"7\":\"0\",\"8\":\"0\",\"9\":\"2\",\"10\":\"9\",\"11\":\"0\",\"12\":\"0.00\",\"13\":\"0\",\"14\":\"0.00\",\"15\":\"0\",\"16\":\"0\",\"17\":\"0\",\"18\":\"0\",\"19\":\"0\",\"20\":\"0\",\"21\":\"0\",\"22\":\"0\",\"23\":\"0\",\"24\":\"0\",\"25\":\"0\",\"26\":\"0\",\"27\":\"0\",\"28\":\"0\",\"29\":\"0\",\"30\":\"1\",\"31\":\"38.62\",\"32\":\"0\",\"33\":\"0\",\"34\":\"19\",\"35\":\"0\",\"36\":\"3.0\",\"37\":\"107\",\"38\":\"55\",\"39\":\"662\",\"40\":\"0\",\"41\":\"1\",\"42\":\"1\",\"43\":\"B7\",\"44\":\"U\",\"45\":\"5\",\"46\":\"6813.58\",\"47\":\"0\",\"48\":\"0\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\r(data_types \u0026lt;- data %\u0026gt;%\rmap_dfc(class) %\u0026gt;%\rgather(key = \u0026quot;Variable\u0026quot;, value = \u0026quot;Type\u0026quot;))\r{\"columns\":[{\"label\":[\"Variable\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"Type\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"AcctAge\",\"2\":\"numeric\"},{\"1\":\"DDA\",\"2\":\"numeric\"},{\"1\":\"DDABal\",\"2\":\"numeric\"},{\"1\":\"CashBk\",\"2\":\"numeric\"},{\"1\":\"Checks\",\"2\":\"numeric\"},{\"1\":\"DirDep\",\"2\":\"numeric\"},{\"1\":\"NSF\",\"2\":\"numeric\"},{\"1\":\"NSFAmt\",\"2\":\"numeric\"},{\"1\":\"Phone\",\"2\":\"numeric\"},{\"1\":\"Teller\",\"2\":\"numeric\"},{\"1\":\"Sav\",\"2\":\"numeric\"},{\"1\":\"SavBal\",\"2\":\"numeric\"},{\"1\":\"ATM\",\"2\":\"numeric\"},{\"1\":\"ATMAmt\",\"2\":\"numeric\"},{\"1\":\"POS\",\"2\":\"numeric\"},{\"1\":\"POSAmt\",\"2\":\"numeric\"},{\"1\":\"CD\",\"2\":\"numeric\"},{\"1\":\"CDBal\",\"2\":\"numeric\"},{\"1\":\"IRA\",\"2\":\"numeric\"},{\"1\":\"IRABal\",\"2\":\"numeric\"},{\"1\":\"LOC\",\"2\":\"numeric\"},{\"1\":\"LOCBal\",\"2\":\"numeric\"},{\"1\":\"ILS\",\"2\":\"numeric\"},{\"1\":\"ILSBal\",\"2\":\"numeric\"},{\"1\":\"MM\",\"2\":\"numeric\"},{\"1\":\"MMBal\",\"2\":\"numeric\"},{\"1\":\"MMCred\",\"2\":\"numeric\"},{\"1\":\"MTG\",\"2\":\"numeric\"},{\"1\":\"MTGBal\",\"2\":\"numeric\"},{\"1\":\"CC\",\"2\":\"numeric\"},{\"1\":\"CCBal\",\"2\":\"numeric\"},{\"1\":\"CCPurc\",\"2\":\"numeric\"},{\"1\":\"SDB\",\"2\":\"numeric\"},{\"1\":\"Income\",\"2\":\"numeric\"},{\"1\":\"HMOwn\",\"2\":\"numeric\"},{\"1\":\"LORes\",\"2\":\"numeric\"},{\"1\":\"HMVal\",\"2\":\"numeric\"},{\"1\":\"Age\",\"2\":\"numeric\"},{\"1\":\"CRScore\",\"2\":\"numeric\"},{\"1\":\"Moved\",\"2\":\"numeric\"},{\"1\":\"InArea\",\"2\":\"numeric\"},{\"1\":\"Ins\",\"2\":\"numeric\"},{\"1\":\"Branch\",\"2\":\"character\"},{\"1\":\"Res\",\"2\":\"character\"},{\"1\":\"Dep\",\"2\":\"numeric\"},{\"1\":\"DepAmt\",\"2\":\"numeric\"},{\"1\":\"Inv\",\"2\":\"numeric\"},{\"1\":\"InvBal\",\"2\":\"numeric\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\rdata_types %\u0026gt;%\rcount(Type)\r{\"columns\":[{\"label\":[\"Type\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"n\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"character\",\"2\":\"2\"},{\"1\":\"numeric\",\"2\":\"46\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\rWe can notice that 2 out of 48 Variables are strings and all the rest are Numeric. This is not necessary correct because some of the variables could be factors. Having Metadata will be super useful right know.\n\r2 Redefining Categorical Variables\rAfter taking a look at the data and the Metadata (that I can´t find now, but I promise I will upload) all the Variables listed next are not correctly numbers but Factors:\n#Listing all of the Categorical Variables according to Metadata\rcategorical \u0026lt;- c(\u0026quot;ATM\u0026quot;, \u0026quot;Branch\u0026quot;, \u0026quot;CC\u0026quot;, \u0026quot;CD\u0026quot;, \u0026quot;DDA\u0026quot;, \u0026quot;DirDep\u0026quot;, \u0026quot;HMOwn\u0026quot;, \u0026quot;ILS\u0026quot;, \u0026quot;IRA\u0026quot;, \u0026quot;InArea\u0026quot;, \u0026quot;Ins\u0026quot;, \u0026quot;Inv\u0026quot;, \u0026quot;LOC\u0026quot;, \u0026quot;MM\u0026quot;, \u0026quot;MTG\u0026quot;, \u0026quot;Moved\u0026quot;, \u0026quot;NSF\u0026quot;, \u0026quot;Res\u0026quot;, \u0026quot;SDB\u0026quot;, \u0026quot;Sav\u0026quot;)\rWe can quickly transform this into factors by using dplyr, with no need to even loop.\n#Transforming to Factor (Categorical Data Type in R)\rdata \u0026lt;- data %\u0026gt;%\rmutate_at(vars(categorical), as_factor)\rThe excellent package forcats offers really easy functions to recode the numbers 1 and 0 into “yes” and “no”.\n#Factor variables will be relabeled for better intepretation of the data\rdata \u0026lt;- data %\u0026gt;%\rmutate_if(is.factor, ~ fct_recode(. , yes = \u0026#39;1\u0026#39;, no = \u0026#39;0\u0026#39;)) \r## Warning: Unknown levels in `f`: 1, 0\r## Warning: Unknown levels in `f`: 1, 0\rdata \u0026lt;- data %\u0026gt;%\rmutate_at(\u0026quot;Res\u0026quot;, ~fct_recode(\r. ,\rrural = \u0026#39;R\u0026#39;,\rsuburb = \u0026#39;S\u0026#39;,\rurban = \u0026#39;U\u0026#39;\r))\rThe “Ins” Variable is the response variable and by using forcats we can shift the order of the Event Variable correctly.\n#Defining Yes as the Event/Positive Category.\rdata$Ins \u0026lt;- data$Ins %\u0026gt;% fct_shift\r\r3 Discovering Missing Values\rdata %\u0026gt;%\rsummarize_all(funs(. %\u0026gt;% is.na %\u0026gt;% sum)) %\u0026gt;%\rmap_df( ~ .x * 100 / nrow(data)) %\u0026gt;%\rgather(key = \u0026quot;Variable\u0026quot;, value = \u0026quot;percent_NAs\u0026quot;) %\u0026gt;%\rarrange(desc(percent_NAs)) %\u0026gt;%\rfilter(percent_NAs \u0026gt; 0)\r## Warning: funs() is soft deprecated as of dplyr 0.8.0\r## Please use a list of either functions or lambdas: ## ## # Simple named list: ## list(mean = mean, median = median)\r## ## # Auto named with `tibble::lst()`: ## tibble::lst(mean, median)\r## ## # Using lambdas\r## list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\r## This warning is displayed once per session.\r{\"columns\":[{\"label\":[\"Variable\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"percent_NAs\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"Age\",\"2\":\"19.703075\"},{\"1\":\"Income\",\"2\":\"17.920903\"},{\"1\":\"LORes\",\"2\":\"17.920903\"},{\"1\":\"HMVal\",\"2\":\"17.920903\"},{\"1\":\"HMOwn\",\"2\":\"17.149145\"},{\"1\":\"Phone\",\"2\":\"12.809943\"},{\"1\":\"POS\",\"2\":\"12.809943\"},{\"1\":\"POSAmt\",\"2\":\"12.809943\"},{\"1\":\"CC\",\"2\":\"12.809943\"},{\"1\":\"CCBal\",\"2\":\"12.809943\"},{\"1\":\"CCPurc\",\"2\":\"12.809943\"},{\"1\":\"Inv\",\"2\":\"12.809943\"},{\"1\":\"InvBal\",\"2\":\"12.809943\"},{\"1\":\"AcctAge\",\"2\":\"6.415819\"},{\"1\":\"CRScore\",\"2\":\"2.191297\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\rWe can show this results in a fancy way with the following code:\n#Counting if columns have any NAs in them\rdata_NA \u0026lt;- data %\u0026gt;%\rmap_dfr(anyNA) %\u0026gt;%\rgather(key = \u0026quot;Variable\u0026quot;, value = \u0026quot;any_NA\u0026quot;) %\u0026gt;%\rfilter(any_NA == TRUE)\r#This hunk was run before to obtain the atual data types of every column\rdata_types \u0026lt;- data %\u0026gt;%\rmap_dfc(class) %\u0026gt;%\rgather(key = \u0026quot;Variable\u0026quot;, value = \u0026quot;Type\u0026quot;)\r#This Chunk counts the actual Number of NAs n_NA \u0026lt;- data %\u0026gt;%\rsummarize_all(funs(. %\u0026gt;%\ris.na %\u0026gt;%\rsum)) %\u0026gt;%\rgather(key = \u0026quot;Variable\u0026quot;, value = \u0026quot;n_NA\u0026quot;)\r# All the previous results are joined into a summary Table\rdata_NA %\u0026gt;%\rleft_join(data_types, by = \u0026quot;Variable\u0026quot;) %\u0026gt;%\rleft_join(n_NA, by = \u0026quot;Variable\u0026quot;) %\u0026gt;%\rarrange(Type)\r{\"columns\":[{\"label\":[\"Variable\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"any_NA\"],\"name\":[2],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"Type\"],\"name\":[3],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"n_NA\"],\"name\":[4],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"CC\",\"2\":\"TRUE\",\"3\":\"factor\",\"4\":\"4133\"},{\"1\":\"HMOwn\",\"2\":\"TRUE\",\"3\":\"factor\",\"4\":\"5533\"},{\"1\":\"Inv\",\"2\":\"TRUE\",\"3\":\"factor\",\"4\":\"4133\"},{\"1\":\"AcctAge\",\"2\":\"TRUE\",\"3\":\"numeric\",\"4\":\"2070\"},{\"1\":\"Phone\",\"2\":\"TRUE\",\"3\":\"numeric\",\"4\":\"4133\"},{\"1\":\"POS\",\"2\":\"TRUE\",\"3\":\"numeric\",\"4\":\"4133\"},{\"1\":\"POSAmt\",\"2\":\"TRUE\",\"3\":\"numeric\",\"4\":\"4133\"},{\"1\":\"CCBal\",\"2\":\"TRUE\",\"3\":\"numeric\",\"4\":\"4133\"},{\"1\":\"CCPurc\",\"2\":\"TRUE\",\"3\":\"numeric\",\"4\":\"4133\"},{\"1\":\"Income\",\"2\":\"TRUE\",\"3\":\"numeric\",\"4\":\"5782\"},{\"1\":\"LORes\",\"2\":\"TRUE\",\"3\":\"numeric\",\"4\":\"5782\"},{\"1\":\"HMVal\",\"2\":\"TRUE\",\"3\":\"numeric\",\"4\":\"5782\"},{\"1\":\"Age\",\"2\":\"TRUE\",\"3\":\"numeric\",\"4\":\"6357\"},{\"1\":\"CRScore\",\"2\":\"TRUE\",\"3\":\"numeric\",\"4\":\"707\"},{\"1\":\"InvBal\",\"2\":\"TRUE\",\"3\":\"numeric\",\"4\":\"4133\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\rIt can be seen that:\n\r15 Variables have Missing Values. T\rThe range of Missing values varies from 2.19 % to 19.7 %.\n\r3 out of 15 are Categorical Values whereas the rest are Numeric Variables.\r\rDuring the Diploma a 2% threshold for Missing Values was discussed. Imputation was not recommended if Missing Values are greater than that. So in order to simplify the problem we will just get rid of NAs. The tidyr package does this really easily.\n\rSidenote: I´m not completely sure about this criterion. I will be asking about this during LatinR_2019.\n\r#Droping observations with missing Values\rdata \u0026lt;- data %\u0026gt;% drop_na()\r#Showing distribution of records of th Target Variable\rdata %\u0026gt;% count(Ins)\r{\"columns\":[{\"label\":[\"Ins\"],\"name\":[1],\"type\":[\"fctr\"],\"align\":[\"left\"]},{\"label\":[\"n\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"yes\",\"2\":\"7504\"},{\"1\":\"no\",\"2\":\"13373\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\r\r4 Conclusion\rSo far we have been able to import a SAS dataset and apply a high level cleansing to organize the data, discover factor variables, reorganize the event Variable correctly and get rid of NAs.\ndata %\u0026gt;% glimpse\r## Observations: 20,877\r## Variables: 48\r## $ AcctAge \u0026lt;dbl\u0026gt; 0.3, 0.7, 4.1, 0.5, 6.7, 12.3, 8.8, 9.3, 0.9, 3.0, 4.8...\r## $ DDA \u0026lt;fct\u0026gt; yes, yes, no, yes, yes, yes, yes, yes, yes, yes, yes, ...\r## $ DDABal \u0026lt;dbl\u0026gt; 419.27, 1986.81, 0.00, 1594.84, 2813.45, 1069.78, 1437...\r## $ CashBk \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\r## $ Checks \u0026lt;dbl\u0026gt; 0, 1, 0, 1, 2, 13, 12, 2, 4, 1, 0, 0, 5, 4, 9, 8, 0, 2...\r## $ DirDep \u0026lt;fct\u0026gt; no, yes, no, no, no, yes, yes, yes, no, yes, no, no, n...\r## $ NSF \u0026lt;fct\u0026gt; no, no, no, no, no, no, no, no, no, no, no, no, no, ye...\r## $ NSFAmt \u0026lt;dbl\u0026gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, ...\r## $ Phone \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, ...\r## $ Teller \u0026lt;dbl\u0026gt; 0, 0, 0, 1, 5, 9, 0, 0, 2, 1, 0, 0, 3, 2, 1, 1, 0, 2, ...\r## $ Sav \u0026lt;fct\u0026gt; yes, no, no, yes, yes, no, no, yes, yes, no, no, yes, ...\r## $ SavBal \u0026lt;dbl\u0026gt; 10233.72, 0.00, 0.00, 425.06, 2716.55, 0.00, 0.00, 967...\r## $ ATM \u0026lt;fct\u0026gt; yes, yes, no, yes, no, no, yes, yes, yes, no, no, no, ...\r## $ ATMAmt \u0026lt;dbl\u0026gt; 106.74, 268.88, 0.00, 278.07, 0.00, 0.00, 391.63, 276....\r## $ POS \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 5, 0, 0, 0, 0, ...\r## $ POSAmt \u0026lt;dbl\u0026gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 23.13,...\r## $ CD \u0026lt;fct\u0026gt; no, no, no, no, no, no, no, no, no, no, no, no, no, no...\r## $ CDBal \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\r## $ IRA \u0026lt;fct\u0026gt; no, no, no, no, no, no, no, no, no, no, no, no, no, no...\r## $ IRABal \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\r## $ LOC \u0026lt;fct\u0026gt; no, no, no, no, no, no, no, no, no, no, no, no, no, no...\r## $ LOCBal \u0026lt;dbl\u0026gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, ...\r## $ ILS \u0026lt;fct\u0026gt; no, no, no, no, no, no, no, no, no, no, no, no, no, no...\r## $ ILSBal \u0026lt;dbl\u0026gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, ...\r## $ MM \u0026lt;fct\u0026gt; no, no, no, no, no, no, no, no, no, no, no, no, no, no...\r## $ MMBal \u0026lt;dbl\u0026gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, ...\r## $ MMCred \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, ...\r## $ MTG \u0026lt;fct\u0026gt; no, no, no, no, no, no, yes, no, no, no, no, no, no, n...\r## $ MTGBal \u0026lt;dbl\u0026gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 94539.95, 0.00, 0....\r## $ CC \u0026lt;fct\u0026gt; yes, yes, yes, yes, no, yes, yes, yes, no, no, yes, ye...\r## $ CCBal \u0026lt;dbl\u0026gt; 483.65, 0.00, 0.00, 65.76, 0.00, 38.62, 85202.99, 0.00...\r## $ CCPurc \u0026lt;dbl\u0026gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...\r## $ SDB \u0026lt;fct\u0026gt; no, no, no, no, no, no, no, no, yes, no, no, no, no, n...\r## $ Income \u0026lt;dbl\u0026gt; 16, 4, 30, 125, 25, 19, 55, 13, 54, 25, 100, 13, 7, 9,...\r## $ HMOwn \u0026lt;fct\u0026gt; yes, yes, yes, yes, yes, no, yes, no, no, yes, yes, ye...\r## $ LORes \u0026lt;dbl\u0026gt; 11.0, 7.0, 8.5, 7.5, 6.0, 3.0, 3.5, 4.5, 4.0, 7.5, 13....\r## $ HMVal \u0026lt;dbl\u0026gt; 89, 87, 97, 145, 101, 107, 128, 99, 129, 95, 135, 77, ...\r## $ Age \u0026lt;dbl\u0026gt; 63, 51, 60, 44, 46, 55, 57, 58, 73, 29, 75, 51, 49, 39...\r## $ CRScore \u0026lt;dbl\u0026gt; 696, 674, 640, 672, 648, 662, 659, 675, 667, 612, 715,...\r## $ Moved \u0026lt;fct\u0026gt; no, no, no, no, no, no, no, no, no, no, no, no, no, no...\r## $ InArea \u0026lt;fct\u0026gt; yes, yes, yes, yes, yes, yes, yes, yes, yes, yes, yes,...\r## $ Ins \u0026lt;fct\u0026gt; yes, no, yes, no, yes, yes, no, yes, yes, no, no, no, ...\r## $ Branch \u0026lt;fct\u0026gt; B17, B2, B3, B1, B1, B7, B1, B5, B6, B4, B9, B7, B7, B...\r## $ Res \u0026lt;fct\u0026gt; rural, rural, suburb, suburb, suburb, urban, urban, ur...\r## $ Dep \u0026lt;dbl\u0026gt; 2, 1, 0, 1, 2, 5, 2, 3, 2, 2, 0, 1, 3, 5, 2, 4, 1, 1, ...\r## $ DepAmt \u0026lt;dbl\u0026gt; 1170.06, 446.93, 0.00, 1144.24, 1208.94, 6813.58, 2237...\r## $ Inv \u0026lt;fct\u0026gt; no, no, no, no, no, no, no, no, no, no, no, no, no, no...\r## $ InvBal \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\rMore to come on this problem. Stay Tuned!!!\n\r","date":1563494400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571210825,"objectID":"b464c47dc9ba5b22e55e25317c7cdd54","permalink":"/project/machine-learning-diploma/","publishdate":"2019-07-19T00:00:00Z","relpermalink":"/project/machine-learning-diploma/","section":"project","summary":"This is will be some kind of tutorial of the different Packages I used to perform a Machine Learning Project for my diploma. This first Part will be focused on Importing Data and a high level Data Harmonization.","tags":["Machine Learning","Data Import (haven)","Data Cleaning"],"title":"My Final Project at the ML Diploma (Part I)","type":"project"},{"authors":["Alfonso Tobar"],"categories":["Basics"],"content":"\r\rRedes Convolucionales\r¿Cómo se relaciona esto con las Redes Convolucionales?\r¿Cómo se va especializando la red para ser cada vez más detallista?\r¿Cómo se relaciona este problema con Estructuras?\r\rEl Paper de Finol\rSolución\rPuntos Interesantes para la Investigación.\r1era Sugerencia (Quizás no para la Memoria, pero para seguir investigando)\r2da Sugerencia\r3era Sugerencia.\r4ta Sugerencia “Crear Imágenes Estructurales”\r\r\r\r\rRedes Convolucionales\rLas redes convolucionales es un tipo de Red que esta especializada principalmente en extraer características de imágenes. Si bien es cierto es posible utilizar Redes convolucionales para otras aplicaciones, el análisis de imágenes y la Visión Computacional son la especialidad de estas redes.\nLa Principal ventaja sobre una Red neural normal es que no está densamente conectada. Sino que selecciona cuidadosamente las neuronas a las que se conectará cambiando la operación matricial de Producto Punto por una convolución.\nLa definición formal de Convolución es:\n\\[ (f * g)(x)= \\int_{-\\infty}^{\\infty}{f(\\eta)\\cdot g(x-\\eta) d\\eta} \\]\rLa interpretación más clásica de esta integral corresponde al producto de funciones al desplazar una por sobre la otra.\n¿Cómo se relaciona esto con las Redes Convolucionales?\rEl análisis de Imágenes es bastante complicado. La manera en la que un computador es capaz de ver una imágen es parseando e interpretandola por un Tensor de Pixeles. Dependiendo de la resolución dela Imagen este Tensor será cada vez más grande, por lo tanto usar Redes Densas supone un gran costo computacional por todos los $ w_{i,j}$ que será necesario calcular.\nNormalmente el análisis de Imágenes utiliza tensor de este estilo:\n\r\rFigure 1: Estructura de Tensores de Imágenes\r\r\rLa Matriz de Pixeles de Dimensiones Alto x Ancho más una Pofundidad de Canales RGB que dan el color.\nEs por esto que las redes convolucionales ocupan esta operación para reconocer patrones específicos dentro del Tensor de Imagen. La manera en que está convolución se lleva a cabo es por medio de un Filtro. Este filtro es otro Tensor que posee generalmente dimensiones de 3x3 o 5x5 el cual se desplaza a través del tensor de imágenes para construir un Mapa de Características.\n\r\r\r\rFigure 2: Aplicación de un Filtro en redes Convolucionales\r\r\rNormalmente luego de la Aplicación del Filtro el Mapa de Características es pasado por una ReLU lo que genera que se realcen ciertos atributos de la imagen lo que permite que la red aprenda a diferenciar características específicas.\nDependiendo del filtro aplicado, se verán ciertos atributos de la imagen o no. Cabe destacar que los filtros parten como números aleatorios que se van entrenando en los distintos Pasos de la red (Forward and Backward).\nSi intentamos analizar la foto de un gato, luego de aplicar filtros, esto es lo que un computador comienza a ver:\n\r\rFigure 3: Feature Map\r\r\rFigure 4: Resultado del Entrenamiento de Filtros\r\r\r\r¿Cómo se va especializando la red para ser cada vez más detallista?\rLuego de la red Convolucional hay una reducción de Dimensionalidad, para llevarse acabo se utiliza una Capa de pooling, esta puede ser una Average Pooling o Max Pooling (generalmente ésta última es la más utilizada) la cual reduce la dimensión del problema para luego aplicar una nueva Capa de Red convolucional pero con más filtros que el paso anterior.\r\r\r\r\rFigure 5: Max Pooling\r\r\rUna vez que se ha llegado al nivel de espacialización deseado, se llevan todas las caracterícticas a un vector que es pasado por una capa densamente conectada para entregar los outputs correspondientes.\nUna Configuración típica podría verse así:\n\r\rFigure 6: Especialización de la Red\r\r\r\r¿Cómo se relaciona este problema con Estructuras?\rNo tiene relación alguna. Porque este tipo de redes fue diseñada para el desarrollo del análisis de imágenes. Además es la manera más intuitiva de analizar el funcionamiento de estas redes.\rLas dos principales caracerísticas de las Redes Convolucionales son:\n\rLa extracción de Características\rTranslation/Shift Invariance\r\rEsto quiere decir que la red se especializa en detectar Patrones que no dependen de su posición ni que deben ser identicos para ser detectados. Por ejemplo una oreja de gato se detectara si es grande, si es pequeña, si es de otro color si esta rotada o desplazada.\nSon estas características las que permitirían desarrollar otro tipo de problemas si es que pueden ser planteadas como un tensor de imágen o similar.\n\r\rEl Paper de Finol\rEl Paper propone solucionar un problema cualquiera de estructuras usando redes Convolucionales. Lo que ellos primeramente plantean es la obtención de Valores propios para un material de propiedades variables.\nPara ello se utiliza una barra dividida en 100 Elementos en las que se miden dos Propiedades, el Módulo de Elasticidad E y la Densidad $ $ .\nEn simple lo que se está planteando es una red convolucional para analizar una Imagen de 1 Pixel de Alto por 100 de Ancho, donde los Canales que representan color, en este caso representan propiedades físicas del Elemento. Dada las propiedades de la imagen esto puede extrapolarse a que las Redes convolucionales pueden analizar Data Secuencial, en este caso cada Elemento de la Barra están en secuencia debido a la continuidad del la barra. Aunque cabe destacar que la red Convolucional no se deja llevar por el orden ya que sus patrones no dependen de la posición.\nMi Supuesto:\nEsto podría traducirse en que los invesigadores pensaron en quizás un elemento equivalente de un Modulo $ E_{eq} $ en serie dado que es una barra unidimensional y el orden de los $ E_{i} $ y $ _{i} $ no influyen en la propiedad equivalente final del elemento.\nAdemás el problema que se está resolviendo es de valores propios, no hay elementos externos que indiquen que el orden de las propiedades mecánicas influyan en el cálculo final.\nBajo estas características es que el uso de la Red Convolucional es válida.\nSolución\rPara resolver el problema de Valores propios para un Cristal dividido en 100 Elementos se utiliza la siguiente configuración:\n\r\rFigure 7: Solución de Finol\r\r\r\rSe utiliza una Red Secuencial aplicando un Filtro de Largo 3.\rAl no utilizar padding el vector se reduce a 98 x 1 x 275 donde 275 corresponde teóricamente al número de filtros que está utilizando. Esto no sale definido en el Paper y es algo que averigué investigando por mi parte.\rSe utiliza una segunda capa convolucional. La razón de esto no lo sé. Normalmente no he visto casos de dos Capas convolucionales seguidas excepto en modelos muy avanzados.\rSe utiliza una capa Max Pooling para reducir de largo 2 que disminuye el tamaño de la red a la mitad.\rLuego de esto se une con una Red Convencional densamente conectada. Lo que normalmente he visto es que se usa una flatten Layer que aplana la reducción de dimensiones que se ha dado hasta ahora. El valor esperado debería ser de 275 x 48, pero no sé porque ocupa 500 nodos.\r*Se utilizan 3 capas ocultas para luego generar una capa de 20 unidades de salida por los 20 valores propios esperados para el problema.\r\r\rPuntos Interesantes para la Investigación.\r\rEstoy tomando un curso en Datacamp y uno público de Stanford acerca del uso de redes Convolucionales. Dentro del curso se dan varias recomendaciones:\n\rPara un problema de imágenes de 64 x 64 x 3 para predecir si hay un gato en la imágen o no se requieren aproximadamente 10.000 imágenes.\n\rSe recomienda usar la mínima resolución posible para distinguir a nivel humano de tal manera de bajar el poder computacional que requiere la red.\n\rA mayor resolución más imágenes. Menor Resolución menos Imágenes.\n\r\rCualquier Matriz puede ser interpretada como una imágen. En la cual una Red Convolucional puede encontrar patrones. Por lo que perfectamente la matriz de rigidez puede ser tratado como una imagen.\n\r\r\r\rFigure 8: Transformación de Matriz en Imágen\r\r\r\rLas redes convolucionales poseen propiedades de Memoria dado que su principal función es la Extracción de Características. Existen modelos pre-entrenados de redes que Pueden usarse para perfeccionar otras tareas.\n\rUn ejemplo que leí fue una red que fue pre-entrenada con imágenes de muebles pero que luego se adaptó para mejor la identificación de animales. La red a pesar de no ser entrenada con animales, era capaz de reconocer bordes y características muy finas.\r\r\r1era Sugerencia (Quizás no para la Memoria, pero para seguir investigando)\rUso de Redes convolucionales para calcular Inversas de la Matriz de Rigidez. Siendo este el proceso más caro de la resolución de estructuras quizás podría realizarse el desarrollo de una Red Pre-entrenada que sea capaz de calcular Inversas de Muchas Matrices de tal Manera de agilizar el proceso más caro de la resolución de estructuras.\n\r2da Sugerencia\rSi bien las redes Convolucionales pueden dar buenos resultados para data secuencial, dada su caracteristica de “Invariante a la Traslación”, puede encontrar patrones incorrectos cuando el orden sí importa. Se desaconseja su uso en series de tiempo, ya que el orden de la información importa al momento de predecir.\rPara contrarestar esto, se agrega una capa de redes recurrentes (RNN) que ayuda a establecer la secuencia como un prerequisito del análisis.\n\rEs posible realizar modelos paralelos en la que se ingrese data distinta como input de la Red. Por ejemplo la Matriz de Rigidez del problema como “imagen” y los Propiedades Mecánicas como vector o hasta las Cargas a las que está sometida la estructura como vector, de esa manera todos los elementos son parte de los Input del problema para predecir de manera más acertada los desplazamientos normales o umbrales del problema.\r\r\r3era Sugerencia.\rGenerar una red no secuencial considerando todos los inputs del problema.\n\rDado que el análisis de las Imágenes poseen una característica Espacial y secuencial me da la impresión que se puede trabajar en optimizaciones al proceso de Condensación de Grados de Libertad. Sería posible que la red redujera problemas con gran cantidad de grados de libertad en Rigideces equivalentes que pueden ser planteadas como una imágen.\n\rAsí mismo dado que también posee propiedades de Profundidad podría ayudar al desarrollo de analisis de estructuras en 3 dimensiones que es algo que yo no recuerdo haber visto si no era usando algo como ETABS o SAP.\n\rMe da la impresión de que Redes Convolucionales pueden ayudar al desarrollo de Propiedades equivalentes usando su característica secuencial para propiedades en serie y su profundidad como propiedades en paralelo.\n\r\r\r4ta Sugerencia “Crear Imágenes Estructurales”\rUtilizar la estructura de la matriz de Rigidez con 3 Canales simulando el RGB: Rigidez, Cargas en Eje X y Cargas en Eje Y.\rHabría que buscar una manera de modelar dentro de la matriz los apoyos, que normalmente no son considerados dentro de la Matriz a priori.\nPara un Enrejado de 9 Elementos esto sería una Imágen de 9 x 9 x 3, lo cual no supondría una gran cantidad de casos para entrenar.\n\r\r\r","date":1554940800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580786696,"objectID":"7465b3b7e0f8a8e6deade092f6b5568a","permalink":"/project/convolutional-nets-sp/memoria-1/","publishdate":"2019-04-11T00:00:00Z","relpermalink":"/project/convolutional-nets-sp/memoria-1/","section":"project","summary":"Propuestas para Uso de CNN en estructuras.","tags":["Deep Learning","Convolutional Networks"],"title":"Propuesta Redes Convolucionales","type":"project"},{"authors":["Alfonso Tobar"],"categories":["Basics"],"content":"\r\rActivation Functions\rIdentity\rStep Function\rLinear Function\rSigmoid Function\rHyperbolic Tangent\rReLU\rLeaky ReLU\rSoftmax\r\rHow to choose the perfect Activation function?\r\r\rActivation Functions\rActivation functions are one of the most important characteristic of ANN. They basically decide whether a neuron should be activated or not. When a particular threshold is reached the Neuron will fire, meaning they will transmit the input signal to the next layer of the Network. Another Important feature of Activation Functions is that some of them provide the non-linearity. This is particular important because Activation functions help expand the range of problems that the Neural Networks can address. Finally Activation functions will play a major role when optimizing the edges weights when Backpropagation Algorithm comes into play, depending of their derivatives values is how the Gradient will change helping to decrease the error associated the Network prediction.\nSome of these Activation Functions are:\nIdentity\rIt is the most basic Activation, basically, do not alter the Neuron at all. The problem with this type of activation function is that is linear, transforming the Network into a Linear Regression limiting its classification capabilities for non-linear phenomena.\n\rStep Function\rThe binary function is extremely simple. It returns 1 if certain threshold is reached or 0 otherwise. The main drawback of this function is that his derivative is 0, meaning it is not useful in the optimizing process.\nThe function is defined as follows:\n\\[ f(x)= \\left\\{ \\begin{array}{lcc}\r0 \u0026amp; if \u0026amp; x \u0026lt; 0 \\\\\r\\\\ 1 \u0026amp; if \u0026amp; x \\geq 0 \\end{array}\r\\right. \\]\n\r\rFigure 1: Step Function\r\r\r\rLinear Function\rThis is another option, being the main difference the existence of a slope. In this case the derivative will be constant, which can be problematic because when trying to decrease the error no matter how right or off you are the gradient will be the same.\nThe function goes as follows:\n\\[ f(x) = a x \\]\n\r\rFigure 2: Linear Function\r\r\r\rSigmoid Function\rThis is a very popular activation function. The main advantages of this function is that it is smooth, S-shaped, it is continuously differentiable and non-linear.\nThe function is defined as follows:\n\\[ f(x)= \\frac{1}{1+e^{-x}} \\]\n\r\rFigure 3: Sigmoid Function\r\r\rThe derivative of this function is always positive and greater than 0 and x-dependent so it is very helpful when optimizing.\nOne of the setbacks is that only ranges from 0 to 1, for one thing is very limiting with the output but for the other it is particularly useful when dealing with probabilities.\n\rHyperbolic Tangent\rHyperbolic Tangent or $ tanh(x) $ is just an scaled version of the Sigmoid function. It is defined as follows:\n\\[ tanh(x)= 2 \\cdot sigmoid(2x) - 1 = \\frac{2}{1+e^{-2x}} - 1 \\]\r\r\rFigure 4: Hyperbolic Tangent Function\r\r\r$ tanh $ works similarly to sigmoid but it is symmetric at the x - axis. Normally $ tanh $ and sigmoid can be used interchangeably depending on the requirements of the problem.\n\rReLU\rReLu stands for Rectified Linear unit and it is defined as follows:\n\\[ f(x)= max(0,x) \\]\r\r\rFigure 5: ReLU Function\r\r\rIt is the most used function nowadays in hidden layers. The main capability is that it doesn’t activate all of the functions creating sparsity in the network, allowing efficiency in computation.\nThis function is limited at the positive side so it is not suggested for Output Layers. Another drawback is that not activated neurons in the range $ x \u0026lt; 0 $ will not be optimized since derivative is zero.\n\rLeaky ReLU\rThis is an improved version of the ReLU function. It is not widely used yet and it has a subtle difference with ReLu: \\[ f(x)= \\left\\{ \\begin{array}{lcc}\rax \u0026amp; if \u0026amp; x \u0026lt; 0 \\\\\r\\\\ x \u0026amp; if \u0026amp; x \\geq 0 \\end{array}\r\\right. \\]\n\r\rFigure 6: Leaky ReLU Function\r\r\rThis solves the problem of dead neurons during Optimization process, since the derivative of $ x \u0026lt; 0 $ is not zero.\n\rSoftmax\rThis is a sigmoid kind-of function capable of handling more than 2 classes. The function is defined as follows:\n\\[ \\sigma(z)_{j}=\\frac{e^{z_{j}}}{\\sum_{k=1}^{K}e^{z_{k}}} \\]\nThe softmax functions are normally used in output layers when trying to solve classification problems with more than 2 classes..\n\r\rHow to choose the perfect Activation function?\rWell, there is not a clear answer to this, but definitely some guidelines we can follow:\n\rSigmoid functions generally work better in classification problems.\rSigmoids and tanh functions are sometimes avoided due to the vanishing gradient problem.\rReLU function is a general activation function and is used in most cases these days.\rIf we encounter a case of dead neurons in our networks the leaky ReLU function is the best choice.\rAlways keep in mind that ReLU function should only be used in the hidden layers.\rAs a rule of thumb, you can begin with using ReLU function and then move over to other activation functions in case ReLU doesn’t provide with optimum results.\r\rDeep Learning with R provides some other Guidelines to use Activation Functions in the Output Layer:\n\rBinary Classification: Sigmoid\rMulticlass Single-Label Classification: Softmax\rMulticlass Multi-Label Classification: Sigmoid\rRegression to Arbitrary Values: Identity or None\rRegression to Values between 0 to 1: Sigmoid\r\r\r","date":1554681600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554783338,"objectID":"7d22714745bd3ed36a07d7ead5033e85","permalink":"/project/activation-functions/activation-functions/","publishdate":"2019-04-08T00:00:00Z","relpermalink":"/project/activation-functions/activation-functions/","section":"project","summary":"Definitions of the Most Common ACtivations Functions.","tags":["Deep Learning"],"title":"Activation Functions","type":"project"},{"authors":["Alfonso Tobar"],"categories":["Basics"],"content":"\r\rWhat is an Artificial Neural Network?\rBasic Structure\rWhat Problems can Neural Networks solve?\rClassification\rRegression\r\rTypical Problems solved with Neural Networks\r\rConclusion\r\r\rWhat is an Artificial Neural Network?\rThe Picture you see up there is far a way from what a real Neural Network looks like:\nActually it is more similar to something like this:\n\r\rFigure 1: Multi-Layer Perceptron\r\r\rThe configuration showed above is nothing but a visual representation of serial Matrix Multiplications. The neural network (aka ANN that stands for Artificial Neural Networks) itself is not an algorithm, but rather a framework for many different machine learning algorithms to work together and process complex data inputs1.\nANN are inspired by Neurons but they work in a completely different fashion. The main advantage of this system is that organizes successive Matrix multiplications and provides a visual representation of the different steps in the Optimization algorithm.\nThere are different types of Networks, Densely Connected, Convolutional, Recurrent, Long Short Memory, Radial Biased, Autoencoders, and so on. All of them having their own area of specialization, strengths and shortcomings.\nBasic Structure\rEvery Network will contain Nodes/Units, emulating Neurons and Edges, emulating the connection between Units.\nNormally the Units are organized by Layers, every Network should contain a first layer for Inputs, a last Layer for Outputs and Intermediate Layers, also known as Hidden Layers.\n\r\rFigure 2: Basic Neural Network\r\r\r\rInput Nodes $ i_{j} $ will contain Input Values to train the Network.\n\rEdges will provide weights $ w_{j} $.\n\rHidden Layer Nodes $ h_{j} $ will contain the result of Sum Product between Input Nodes and weighted edges connected to them.\n\rFinally Output Nodes $ o_{j} $ will contain the result of Sum Product between hidden Nodes and the Edges Connected to them.\n\rOptionally, Networks can include a Bias $ b_{j} $ to control values of the network.\n\r\rThe Network then will calculate values in the following way:\n\\[ h_{1} = w_{1} i_{1} + w_{2} i_{2} + b_{1} = 0.15 \\cdot 0.05 + 0.25 \\cdot 0.10 + 0.35 = 0.3825 \\]\n\\[ h_{2} = w_{2} i_{1} + w_{4} i_{2} + b_{1} = 0.20 \\cdot 0.05 + 0.30 \\cdot 0.10 + 0.35 = 0.39 \\]\n\\[ o_{1} = w_{5} h_{1} + w_{7} h_{2} + b_{2} = 0.40 \\cdot 0.3825 + 0.50 \\cdot 0.39 + 0.60 = 0.948 \\]\n\\[ o_{2} = w_{6} h_{1} + w_{8} h_{2} + b_{2} = 0.45 \\cdot 0.3825 + 0.55 \\cdot 0.39 + 0.60 = 0.986625 \\]\nThis is called a forward pass. All the Input Values were able to move through the Network by the Edge Connections up to the Output. Normally, when the Network is trained, there are expected Output values that will be compared with the ones obtained with the Forward Pass in order to compute the Error. In this case 0.01 and 0.99 respectively.\n\rWhat Problems can Neural Networks solve?\rNormally there are two Problems that Neural Networks Solves, Classification and Regression.\nClassification\rThe Classification problem is the most common problem addressed by Neural Networks. It implies to classify based on a Probability. The output will calculate how likely is that an specific label corresponds to a class. Classification problems can be sub-divided into other sub-types:\n\rBinary Classification: As the name implies, it involves two classes: Spam or not Span, Positive or Negative, Man or Woman, etc.\rMulticlass Classification: In this case several labels can be applied: Is it a Dog, Cat, Horse? What Car Brand is that? etc.\r\r\rRegression\rThis kind of problems involved calculate a number associated to a Metric. Typical Problems are predicting House Values, Temperature, Balances, Displacements, etc.\n\r\rTypical Problems solved with Neural Networks\rNeural Networks are powerful and they are the most cutting-edge methodology to make computers do the most incredibly/creepy things.\nEven though we think computers can do anything like Analyzing Photos, Driving Cars, Recognizing Animals, Predicting Prices and so on, the scope of their work is completely limited to just one thing: Computing Tensors.\nTensors are the generalization to N dimensions of Matrices. Basically any problem that can be represented by Tensors is something that Neural Networks could potentially solve.\nDifferent kind of Tensors can solve specific problems. Here some examples taken from Deep Learning with R:\n\rVector data—2D tensors of shape (samples, features): This is the Most common Data Structure Data Scientist uses in a daily basis. Basically a Matrix, having Features as Columns and Samples as Rows.\n\rTimeseries data or sequence data—3D tensors of shape (samples, timesteps, features): This is something a little bit fancier, having several timeseries organized as a collection of matrices, this creates a 3D Tensor.\n\r\r\r\rFigure 3: Multiple Timeseries data\r\r\r\rImages—4D tensors of shape (samples, height, width, channels) or (samples, channels, height, width): Images are represented as Pixel Matrices, Every Pixel also has RGB Channels giving the color properties to it, thus a 3D Tensor. Adding several samples of Images to analize and you have a 4D Tensor.\r\r\r\rFigure 4: Image Data\r\r\r\rVideo—5D tensors of shape (samples, frames, height, width, channels) or (samples, frames, channels, height, width): Videos are nothing but a collection of Sequencial Images. So You’ll have different Samples of Sequencial Images producing which is a 4D Tensor, since you analize several samples of Videos, you’ll get a 5D Tensor.\r\r\r\rConclusion\rNeural Networks are simple visual representations of Tensor Calcultions that are capable of addressing different real life problems. So far we have covered how the Networks transmit Information from Input to Output also called Forward Pass, but there are some other concepts that are necessary to understan in order to fully unerstand how to properly train a Neural Network.\n\r\rWikipedia: https://en.wikipedia.org/wiki/Artificial_neural_network↩\n\r\r\r","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554783338,"objectID":"5ab0020c69035456ab833ae99bb5d554","permalink":"/project/neural-nets-101/intro-to-nn/","publishdate":"2019-04-07T00:00:00Z","relpermalink":"/project/neural-nets-101/intro-to-nn/","section":"project","summary":"Short Summary of What Neural Networks are","tags":["Deep Learning"],"title":"Intro to Neural Networks","type":"project"},{"authors":["Alfonso Tobar"],"categories":["Blog"],"content":" Deep Learning Well, I got to know Deep Learning by chance. I remember to have had a College subject called Operational Investigation Fundamentals and they taugth Artifical Intelligence algorithms. Neural networks were mentioned but I never thought I could even understand what they were about. During my first real job (as a quasi-Engineer) I always thougth I never paid sufficient attention to that subject. The subject basically covered some optimization models and for some reason AI was there, just sky-high level mentions in some classes.\nThen I remember during my Data Science Program, we had 3 different Research Projects (this was at my Butterflies and Unicorns time, if you don\u0026rsquo;t know what I\u0026rsquo;m talking, go here). One was Data Analytics to Measure Cars Price (super-duper boring), the second was Using Sentiment Analysis to Understand whether Lyrics of Top 20 Billboard were related to Decade\u0026rsquo;s Most Important Facts (I did this, and it was super interesting) and there was a third one I really tried to sneak out: Sentiment Annalysis with Machine Learning. Machine Learning sounded really scary to be my very first Data Science Project, so I put it off.\nIt turns out that a colleague just joined my team at EVS and he had this awesome Max Kuhn book: Applied Predictive Modeling. And I thought: \u0026ldquo;I\u0026rsquo;m more experienced now, so probably I can take a look at this\u0026rdquo;. And well I discovered caret and completely blew my mind (But I still dislike its little to null compatibility with Tidyverse, but thanks Max Kuhn for creating parsnip).\nThen Nico, I think, was the first guy mentioning Deep Learning, because he created the XoR problem in VBA and also took Andrew Ng specialization Course, and I started to feel interested in the Field.\nWell, I dicovered Keras, the R API just came out (I think so) and I got this other super Book: Deep Learning with R and I just learned so much, but not enough. I think I learned a bit of Keras but I suddenly realized I had no idea about the inner black-box, so I just wanted to learn and understand what the hell is happening inside that box.\nWell here I am\u0026hellip; Trying to understand\u0026hellip; and I didn\u0026rsquo;t do very well. Internet has little to no information about theory. Codes are everywhere, but why that code is useful is just not important for users.\nSo my intent is creating content (R based obviously, because R is not popular for Deep Learning either) for me to learn, and hopefully spread the word about this.\nWell, the thing is I need to finish my Thesis, and my Professor told me: \u0026ldquo;Why don\u0026rsquo;t you combine what you know about Deep Learning (sincerely, not too much) with Structure Engineering? And you know what, this is something really unexplored in the field, so here I go.\nHopefully publishing my findings help to have a better understanding of what I\u0026rsquo;m doing, also i can keep track of it and help others on my way (I love teaching, so this is a good way to get started).\nHope to have news about my Thesis soon\u0026hellip;\n","date":1553647800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572494298,"objectID":"2046ae8bd00899bda7e50b0ef72f0071","permalink":"/post/deep-learning/","publishdate":"2019-03-27T00:50:00Z","relpermalink":"/post/deep-learning/","section":"post","summary":"How I ran into DL?","tags":["Thesis","Deep Learning"],"title":"Deep Learning","type":"post"},{"authors":["Alfonso Tobar"],"categories":["Blog"],"content":" Why am I not graduated yet? Graduation is something we all are longing. Well, I recognize this is something I\u0026rsquo;ve been evading. The main reason is because I\u0026rsquo;m not working on what I studied. But, what the hell, I think I like Data Science.\nOk, Long Story Short\u0026hellip; I was about to study Mathematical Engineering but I asked myself, where can I possibly work as a Mathematical Engineer ? (In my current job, obviously 😅)\nSo I decide to go for Civil Engineering because it has a decent amount of advanced Maths that was something I was looking for and a High Employment Index.\nWell I started working as a Civil Engineer in INVAR as soon as I finished my internship. I just loved it, I learned so much, but the thingwas that they didn\u0026rsquo;t pay too much. So I decided to take a position at Aguas del Valle. Sincerely, and very respectfully, it has been the worst choice of my life, not only because it was 5 hours away of my Hometown, but also because they took my soul out of my body and make me spend awful moments doing absolutely nothing. There is nothing more frustrating than doing nothing all day long in a desk in a Company you don\u0026rsquo;t feel a part of.\nSo the Evalueserve Post showed up. A friend referred me to the Data Science Program (DSP) and then I fell in love with Data Science. During the program I learned SQL, VBA, SAS 👍, R, Tableau and definitely built up my English and Communication skills. Felipe, help me prepare my first interview in English and thanks to him I had the English I needed to learn all I know so far.\nUpskilling and moving through cities took all of my time, besides I got married (Best Decision so far) but a lot to do, hence, no time to finish my Thesis.\nWhat happened with my Thesis? Well in INVAR I worked as an Hidraulics Engineer so I started a Thesis back in 2013 about Water Hammer. Really interesting topic, that needed Finite Differences Methods to solve really complicated Partial Differential Equations. The only proffesor with this knowledge started working wih me, but after a year he got retired. So I had to find another Professor, so in order to avoid this issue to happen again, I decided to go with one of the youngest Professor, well he left me for his Phd.\nDamn, Suddenly working in INVAR I noticed the CEO was my professor of Planning so I ask him to mentor me. This happened just before leaving to Aguas del Valle in La Serena. 6 months later a Cancer was diagnosed and he passed away, so freaking fast. The Chief of Department took over all of the abandoned Thesis that my Professor\u0026rsquo;s dead left behind, but he couldn\u0026rsquo;t continue to mentor me, because he considered being in La Serena was too far away. In 2015 I came back to Viña and I started to learn, upskill, teach, develop as a Data Scientist that I decided to put it on hold. On 2017 I started another Thesis. Another Professor wanted to mentor me but he was of the Structural Area. That meant I needed to move to that Area. We started investigating uncertainty on Structural Analysis. The thing is my work again didn\u0026rsquo;t give me any chance to continue. Until now. Why now? Probably I will explain it later, because it is also related with the creation of this site.\nWhat My Thesis is going to be about? Good news, I will be able to mix my previous work with one thing I discovered as I developed as Data Science: Deep Learning in R 👏.\nI will be posting some progress about that. Hang tight!!\n","date":1553647800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572494298,"objectID":"fc07de7a276a157101a10808a9efd24b","permalink":"/post/my-thesis/","publishdate":"2019-03-27T00:50:00Z","relpermalink":"/post/my-thesis/","section":"post","summary":"Where, What and Why is ~~Gamora~~ my Thesis About?","tags":["Thesis"],"title":"My Thesis Project","type":"post"},{"authors":null,"categories":null,"content":"","date":1551409200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553554159,"objectID":"d7c99b5f0ba2ef066af2121e72286554","permalink":"/bio/","publishdate":"2019-03-01T00:00:00-03:00","relpermalink":"/bio/","section":"","summary":"This is me!","tags":null,"title":"Bio","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1551409200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571210825,"objectID":"10b23fb9a98faf23942f651c808b6081","permalink":"/contact/","publishdate":"2019-03-01T00:00:00-03:00","relpermalink":"/contact/","section":"","summary":"This is me!","tags":null,"title":"Contact","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1551409200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571210825,"objectID":"6087c0ef875554f4409ac52928d79279","permalink":"/projects/","publishdate":"2019-03-01T00:00:00-03:00","relpermalink":"/projects/","section":"","summary":"Some Projets","tags":null,"title":"Projects","type":"widget_page"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\ngetwd()  ## [1] \u0026quot;D:/Alfonso/website/content/tutorial\u0026quot;  Tip 1 \u0026hellip;\nTip 2 \u0026hellip;\nbla bla ","date":1536462000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553562400,"objectID":"2e59fda17a04e7a8318c6191daa0102b","permalink":"/tutorial/example/","publishdate":"2018-09-09T00:00:00-03:00","relpermalink":"/tutorial/example/","section":"tutorial","summary":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\ngetwd()  ## [1] \u0026quot;D:/Alfonso/website/content/tutorial\u0026quot;  Tip 1 \u0026hellip;\nTip 2 \u0026hellip;\nbla bla ","tags":null,"title":"Example Page","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\ngetwd()  ## [1] \u0026quot;E:/Alfonso/website/content/tutorial\u0026quot;  Tip 1 \u0026hellip;\nTip 2 \u0026hellip;\nbla bla ","date":1536462000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571210825,"objectID":"c40d38224c2b476cbfc1f032fbb6a78e","permalink":"/tutorial/copyofexample/","publishdate":"2018-09-09T00:00:00-03:00","relpermalink":"/tutorial/copyofexample/","section":"tutorial","summary":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\ngetwd()  ## [1] \u0026quot;E:/Alfonso/website/content/tutorial\u0026quot;  Tip 1 \u0026hellip;\nTip 2 \u0026hellip;\nbla bla ","tags":null,"title":"Example Page","type":"docs"},{"authors":null,"categories":null,"content":"My blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License.\n  \n","date":1530140400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571210825,"objectID":"18d05a63a1c8d7ed973cc51838494e41","permalink":"/privacy/","publishdate":"2018-06-28T00:00:00+01:00","relpermalink":"/privacy/","section":"","summary":"My blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License.","tags":null,"title":"Licensed under Creative Commons Attribution-ShareAlike 4.0 International","type":"page"},{"authors":null,"categories":null,"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553404091,"objectID":"c2915ec5da95791851caafdcba9664af","permalink":"/slides/example-slides/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example-slides/","section":"slides","summary":"Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$","tags":null,"title":"Slides","type":"slides"}]