[{"authors":["admin"],"categories":null,"content":"I¬¥m a Civil Engineer, but I got to know Data Science and I just loved it. I¬¥ve worked the last 4 years as an R developer, involved in almost every aspect of the Data Science Workflow.\nI love teaching and I aspire to eventually teach R classes to help Data Scientist around the world know and get better with this awesome language.\nIn my spare time I love Drumming, playing Table Tennis, solving the Rubik¬¥s Cube and spending time with my beautiful wife Valentina.\n","date":1569473082,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1569473082,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":"I¬¥m a Civil Engineer, but I got to know Data Science and I just loved it. I¬¥ve worked the last 4 years as an R developer, involved in almost every aspect of the Data Science Workflow.\nI love teaching and I aspire to eventually teach R classes to help Data Scientist around the world know and get better with this awesome language.\nIn my spare time I love Drumming, playing Table Tennis, solving the Rubik¬¥s Cube and spending time with my beautiful wife Valentina.","tags":null,"title":"Alfonso Tobar","type":"author"},{"authors":null,"categories":null,"content":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.main]] menu links to it in the config.toml.\n","date":1536462000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1553404091,"objectID":"c3224f3a64174f08aaf31e1f1d16ffd3","permalink":"/tutorial/","publishdate":"2018-09-09T00:00:00-03:00","relpermalink":"/tutorial/","section":"tutorial","summary":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.","tags":null,"title":"Overview","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.\n  Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using url_slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906563600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553404091,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00-03:00","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"COMING SOON","type":"talk"},{"authors":["Alfonso Tobar"],"categories":["Vampire Diaries","Rambling"],"content":"\rLatin R (Conferences Days)\rToday the Conference Days just got started, and I have to say I watched some really impresive presentations and Data Products.\nDay 1 started with a UAI local presenter talking about Sports Analytics in R, then a huge presentation by Mine Cetinkaya about Teaching R to move to specific small presentation in parallel so I was able just to watch half of the Presentations.\nDay 2 started with a UC local presenter talking about applying Data Science in R to Government Data, then Erin Ledell talked about H2o to move forward to the small presentations in Parallel. The day finished with Poster presentations and Hadley Wickham talking about different dplyr backends.\n\rDay 1 Summary\rSports Analytics\r\r#LatinR2019 partiendo con datos espaciales. Hermoso :) pic.twitter.com/xUCYltzJNi\n\u0026mdash; Steph Orellana Bello (@sporella) September 26, 2019  \r\rAhora @raimun2 nos cuenta c√≥mo gener√≥ sus mapas para anal√≠tica de deportes de monta√±a üó∫\n‚ùáÔ∏è 25% de la poblaci√≥n tiene un cerro a menos de 3 km de distancia\n‚ùáÔ∏è 100% un cerro a menos de 10 km de distancia pic.twitter.com/l8ziSS7yXX\n\u0026mdash; LatinR (@LatinR_Conf) September 26, 2019  \rThe Conference kicked off with Spatial Data applied to some kind of Mountain Trekking. The presenter showed how to use R to calculate the distances from every Santiago door to have access to the Mountains.\nThe most interesting thing about this is that this showed the huge power of R in the Spatial Data side, that is defnitely something I have no idea.\nDuring the Presentation the following packages were presented:\n\rggmap: of Course from the ggplot family this package allows to work with Stamen maps that I think is some kind of Raster (Map Images).\n\rElevatR: to get access to Topographic Maps with Elevations for free.\n\rRstrava: An API to get access to trekking routes.\n\r\rI think this was some kind of Introduction to really interesting Spatial Presentations that showed me that there is a lot to learn about that.\n\rTeaching R\rOne of my passions is Teaching R, that is why this talk was specially touching to me. I worked hard during my time in EVS to have the Best R classes possible and we made it, a lot of people learned R but there was still a lot of things I know I did wrong and a lot of Tips that Mine presented that I have never thougth about.\n\r.@minebocek hablando sobre la ense√±anza de R. Las diapositivas disponibles en este enlace https://t.co/GEtWKB5vjV pic.twitter.com/aLvWQJruVv\n\u0026mdash; LatinR (@LatinR_Conf) September 26, 2019  \rThe whole talk can be found here but there was a couple of things that I think it is important to highlight:\n\rUsing Students feedback earlier: She explained is really helpful to give our students the chance to raise questions and how the material can be improved as soon as possible, speially little things like font size, voice volume, backgorund color, contents and course expectations.\rHelp them get help: I think the best things here were {searcher}, a pakage to make automatic searchs in google or stackoverflow about errors, the well-known {reprex} and this super helpful explanation about R help format:\r\r\r\r\r{livecode}: Mine remarked the importance of live coding to explain workflows and give confidence to students, plus it helps to express the correct way to refer to R elements. the {livecode} package is being developed and this could be a life-changer tool for people who likes to teach.\rPeer review: The R community is really open and we need to encourage the constructive feedback to make each other a better useR.________\rEncourage creativity: I just loved this point speially in the kind of challenges that can be given, for instance, creating a Christmas tree with R.\r\r\r\r\ralicer package\rFor me this was the most mature and powerful data Product presented during the Conference.\n\r.@minebocek hablando sobre la ense√±anza de R. Las diapositivas disponibles en este enlace https://t.co/GEtWKB5vjV pic.twitter.com/aLvWQJruVv\n\u0026mdash; LatinR (@LatinR_Conf) September 26, 2019  \rMy main takeaways from this presentation are:\n\rYou don¬¥t need to be Hadley to create an awesome package.\rUse use\r\r\r\r","date":1569801600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569801600,"objectID":"d7787206cbf2ba1b7919cec3132cfe9c","permalink":"/post/latin-r-ii/","publishdate":"2019-09-30T00:00:00Z","relpermalink":"/post/latin-r-ii/","section":"post","summary":"I took 2 tutorials at the LatinR Conference. I¬¥ll be commenting about them.","tags":["R","Conference","Latin R"],"title":"Latin R (Days 2 and 3)","type":"post"},{"authors":["Alfonso Tobar"],"categories":["Vampire Diaries","Rambling"],"content":"\rLatin R (Tutorials Day)\rToday Latin R just got started. It was really impressive to see how many people actually uses R. I have to say the popularity of Python in Data Science has always been something that worries me tons, but today I was able to see a lot of other Areas where people actually use R. Probably I was one of the few people working in Data Science there.\nRegarding Tutorials, they were awesome. 4 Tutorials were conducted today.\nDuring Morning, Mine √áetinkaya-Rundel taught about Teaching R. I wasn¬¥t there so I cannot comment a lot. In parallel Erin Ledell taught about using H2o for Machine Learning.\nIn the afternoon, Joshua Kunst taught about Highcharter with plotly for data Viz and of course Hadley Wickham was in charge of teaching about Package Development.\n\rH2o Tutorial\r\r\rErin Ledell conducted this tutorial that was more like a demonstration of the capabilities of H2o. I have to say I had heard about H2o in Matt Danchos‚Äô tutorials but I never got impressed because I¬¥m a super fan of Tidymodels and H2o is not there yet.\nFor me I think a great tool was presented. H2o is a Java implementation of several Machine Learning Models. This includes Pre-processing, Grid Search, Cross Validation, Stacked Models and running in Clusters using CPU and GPU, The best is that it¬¥s absolutely free.\nPros\r\rSuper easy and Intuitive syntax. Very similar to parsnip.\rSuper fast implementation in Java.\rIt has a variety of Models including GLM, Random Forest, SVM, even some Deep Learning things.\rIt has a localhost implementation with a GUI interface for non-coders.\rRuns super smoothly in Rstudio.cloud.\rOffers Stacked Models and AutoML algorithms.\rSuper Easy Implementation into Production.\r\r\rDownsides\r\rIt needs Java 8-12 to be installed. And Installing it is a pain.\rxgboost is not implemented for Windows users (this is a huge setback).\rClassification or Regression Problems are detected depending on the data type of the Target Variable. (Not a huge issue but I like to have control over that).\rIt tends to oversimplify things running things behind the scenes to facilitate user experience, but you don¬¥t always are aware of things happening.\r\r\rOverall\rDon¬¥t get me wrong. H2o is awesome and a great starting point for people recently learning about Machine Learning and for experienced Machine Learning people that want speed and scalability.\nIt also offers AutoML and stacked ensembles that with little work can help to achieve excellent performance.\nAnother think I liked, more like a side note, was that running this into Rstudio cloud was super smooth. I just had to install the h2o package as any other R package and that was it. The internet was not the best and even so everything ran super fast.\nFinally Erin mentioned that Max Kuhn and his team is working on integrating H2o with the TidyModels ecosystem. If that happens H2o will start being definitely one of my favorites even more.\nI think I will share a short tutorial about the commands I learned during the tutorial.\n\r\rPackage Development Tutorial\r\r\rHadley Wickham was in charge of this Tutorial and it was huge.\nThe content was not a big deal, he showed the necessary steps to create a package that is actually easier than expected. But the way he directed the class:\n\r4 to 5 TAs to help people in need by using a Post-it signal to ask for help without interrupting the class.\rA lot of hands-on exercises.\rMakes us meet our neighbors to work peer to peer.\rAnd the usethis package.\r\rThe usethis Package was a huge deal. I had heard about it but I never dimensioned how powerful it is.\nFirst it helps simplify really tedious process in the Package development such as the Creation of Package Directories, edit .Rprofile file, even share Material or Courses. It also creates test files to run with testthat (another super great package), helps create vignettes, upload to github, set Travis and create pkgdown sites.\nI think usethis is one of the great great things I take away during this Tutorial. Another great surprise is the utility of roxygen2. I know that is THE package for Documentation purposes but today I really understood how important it is. It really simplifies Documentation creation but also helps compile all the tedious Documentation files that are mandatory to pass CRAN checks.\n\rLessons learned\r\rTo value my job. Hadley had all of its work, including PDF files licensed and I think that gives value to the things you do.\rIf a package is on CRAN is because the creator put a lot of love in it. Because submitting is so convoluted and tedious that if you work that hard to pass CRAN checks is because you really think you are contributing with something that is important to you as creator. Hadley mentioned that submitting to CRAN gives credibility to the author, quality to the actual package because of all of the test that needs to pass to be accepted and a lot of experience as an R Programmer.\rI don¬¥t feel fully prepared to create a huge package yet, but I¬¥m not afraid anymore.\r\rMy goal after this: My Thesis definitely needs to end up into a package. I¬¥ll do my best.\nOne of Hadley best tips:\n\r‚ÄúFinish your daily work with a test failing so you can now exactly how to resume your work the next day.‚Äù\n\rTomorrow is day 2. Stay tuned!!\n\r","date":1569369600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569369600,"objectID":"c540e8a086fea3f6fd5b7761f0361473","permalink":"/post/latin-r-i/","publishdate":"2019-09-25T00:00:00Z","relpermalink":"/post/latin-r-i/","section":"post","summary":"I took 2 tutorials at the LatinR Conference. I¬¥ll be commenting about them.","tags":["R","Conference","Latin R"],"title":"Latin R (Day 1)","type":"post"},{"authors":["Alfonso Tobar"],"categories":["Vapire Diaries","Rambling"],"content":"\rLatin R\rLatin R is a conference organized by RLadies Latam and is the oportunity we have here in Chile to get acquainted about the last R breakthrough and how is being using in Research, companies, etc.\rThey also provide a full day of Tutorials and of course important keynotes are invited to come over. This year I think is huge, because 3 Main R users are coming:\nMine √áetinkaya-Rundel\rShe is one of the most important persons involved into R Teaching. Statistics Professor from Duke University. I personally had the chance to take some Coursera and Datacamp Courses with her and I learned a lot, really good teacher and really knowledgable. Having her now here in Chile to meet her in person is such a great Honor. I just can¬¥t wait to hear about what she has to say.\rShe will be conducting a Tutorial on how to Teach R and of Course a main Speech.\n\rErin Ledell\rI have to say I don‚Äôt know a lot about her but she has a great curriculum: Chief Machine Learning Scientist at H2O, co-founder of Rladies Global and Woman+ in ML/DS, plus BioStatistics Phd at Berkeley, that is more than enough. She will be conducting a Tutorial on Machine Learning and Deep Learning that of course I¬¥ll be taking so I¬¥ll give more details on a later post.\n\rHadley Wickham\rThis Guy needs no Introduction. He is the Chief Scientist at RStudio and he will be giving the Main Speech of the Conference plus a Package Development Tutorial. Of ourse I will be in first row of this Tutorial and I expect to learn a lot and have a lot of questions prepared beforehand.\nThis guys is by far my most important inspiration when it comes to working in R. Definitely this guy has made my life way easier because of all of the packages that he has created and I use almost every day.\nI‚Äôll be the next 3 days in the Conference and I expect to share some pictures and experiences about the things I will learn.\nStay tuned!!!\n\r\r","date":1569283200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569472701,"objectID":"8e89b01640dd805b3b195ff53f61bdb0","permalink":"/post/latin-r/","publishdate":"2019-09-24T00:00:00Z","relpermalink":"/post/latin-r/","section":"post","summary":"Hadley is coming to Chile and I¬¥ll be taking some Package Development Classes with him","tags":["R","Conference"],"title":"Latin R","type":"post"},{"authors":["Alfonso Tobar"],"categories":null,"content":"Coming up with an Interesting Thesis Project is not easy at all. Actually I had 3 different projets and 5 different professors. None of them were really interested in my propositions. Thank God I found Dr. Marcos Valdebenito. He is really interested in Reliability Analysis in Structures and Study the Response of Random Field Variables into Strutures, you can learn more about his work on his website. Once I talked to him about I was doing in my former job he was really interested in applying Machine Learning techniques to solve this kind of problems.\nNormally to study the effect of Ramdom fields in Structures a Montecarlo Simulation is run several times to determine how the Structure response is affected. This is done by analyzing the mean and the Covariance of the simulations. This process is omputationally expensive since normally 10,000 to 1,000,000 simulations are needed. Every one of those simulations solves the following problem, also called the Rayleigh - Ritz Method:\n$$ [K] {u} = {f}$$\nWhere $ [K] $ is the Finite Element Matrix representing the Equivalent Stiffness of the different Degrees of Fredom of the Structure. $ {f} $ is the Equivalent Load Vector representing the forces affecting the Structure. In order to solve this problem $ [K]^{-1} $ needs to be pre-multiplied with $ {f}$ to obtain the Struture Response $ {u} $ representing the Structure displacement at every Degree of freedom. Normally $ [K] $ is a fairly large Matrix and the Inversion process costly so alternative methods to Montecarlo Simulation are deeply appreciated.\nSo that is how we came up with a Project. What about considering $ [K] $ as a black and white image (1 channel), representing the stiffness of a Truss. Therefore, Convolutional Networks could be a good alternative to analyze the Matrix and train a Network capable of determinimg in a first instance the displacement of the Structure (${u}$) and afterwards the failure of the Structure, transforming the problem into a Clasification Binary Problem (Failing - not Failing).\nI\u0026rsquo;ll be posting more technical content about how I\u0026rsquo;ve been tackling the problem. Stay tuned!!!\n","date":1569207600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569387730,"objectID":"ddc06374e9bc1835608820cda1843546","permalink":"/project/my-thesis/","publishdate":"2019-09-23T00:00:00-03:00","relpermalink":"/project/my-thesis/","section":"project","summary":"This is What my Thesis Project will be about.","tags":["Civil Engineering","Finite Elements","Stiffness Method"],"title":"My Thesis","type":"project"},{"authors":["Alfonso Tobar"],"categories":["Basics"],"content":"\r\rThe Method\rThe Problem\rRcpp Basics\rCreating an Rcpp file\rStiffness Method\rStiff Matrix by Element\rActive DoF Assembly\rConnectivity Array\rStiffness Matrix Assembly\rLoad Vector Assembly\rSolving the Problem\r\rConclusions\r\r\r\rThe Method\rThe Rayleigh Ritz Method is nothing but applying Finite Elements to Structural problems. Basically you split your structure into smaller structures that can easily be solved By solving, I mean, Calculate the specific stifness of the Structure in order to determine how the loads affects the structure. Once the individual mini-strutures are solved they are ensembled into a Merged Matrix equivalent to the total Stiffness of the Structure.\nThe purpose of this Document is not get into deep details about the Method. If you want to learn about this you can go to this paper to learn the Maths behind this. The idea is to show how to implement this in R. Since this is a computational expensive method I‚Äôll be using library(Rcpp).\nThe Problem\r\r\rFigure 1: Problem Structure\r\r\rThis is a simple problem and useful to understand the different steps of the Method.\rThis is implementation is for a Truss with 3 Nodes and 3 Elements where:\n#Number of Nodes by Element\rNN_e \u0026lt;- 2\r#Number of Degrees of Freedom (DoF) by Node\rNgl_N \u0026lt;- 2\rL \u0026lt;- 1 #Value of L\rE \u0026lt;- 2 * 10 ^ 11 # Young Module / Elasticity Metric\rA \u0026lt;- 0.0001 # Cross Sectional Area\rP \u0026lt;- 1000 # Load\rThe First and most simple Step is to organize the Input Data. All of the Data will be input in tibble form.\nRow i of the Nodes Matrix will store the X and Y Coordinates for Every Node.\n(Nodes \u0026lt;-\rtibble::tribble(~ Xi, ~ Yi,\r0, 0,\rsqrt(2) / 2 * L, sqrt(2) / 2 * L,\rsqrt(2) * L, 0))\r## # A tibble: 3 x 2\r## Xi Yi\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 0 0 ## 2 0.707 0.707\r## 3 1.41 0\rThe Row j of the Elements Matrix will contain the Initial Node ni, the ending Node nf and the corresponding E and A properties for Element j. In this case all the Elements share the same properties.\n(Elements \u0026lt;-\rtibble::tribble(~ ni, ~ nf, ~ E, ~ A,\r1, 2, E, A,\r2, 3, E, A,\r3, 1, E, A))\r## # A tibble: 3 x 4\r## ni nf E A\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1 2 200000000000 0.0001\r## 2 2 3 200000000000 0.0001\r## 3 3 1 200000000000 0.0001\rThe Row i of the Loads Matrix contains the x and y vectorial component of the Loads for Node i.\n(Loads \u0026lt;-\rtibble::tribble(~ Px, ~ Py,\r0, 0,\r0, P,\r0, 0))\r## # A tibble: 3 x 2\r## Px Py\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 0 0\r## 2 0 1000\r## 3 0 0\rThe Row i correspond to the freedom of the X and Y Component of the Node i. 1 meaning no Movement and 0 meaning free movement.\n(Supports \u0026lt;-\rtibble::tribble(~ Rx, ~ Ry,\r1, 1,\r0, 0,\r0, 1))\r## # A tibble: 3 x 2\r## Rx Ry\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1 1\r## 2 0 0\r## 3 0 1\r\rRcpp Basics\rRcpp is the R API package to access to the huge benefits that C++ offers. I¬¥m not an expert in C++ actually I just learned a bit of C++ because Rcpp offers easy sintax to access to C++ Elements but always showing equivalents in the R Environment.\nC++ is far for being an adequate language for Data Science, but once you want to optimize code or algorithms is definitely the way to go. In these case I¬¥ll be showing the algorithm to the different steps of the Stiffness Method and how can be implemented in Rcpp.\nMy main sources to learn Rcpp were this excellent Rcpp for Everyone and of course Hadley¬¥s Help. With these two resources you should have more than enough to create your first Rcpp functions.\n\rCreating an Rcpp file\r\r\rFigure 2: Create a C++ File\r\r\rIf you work with RStudio you can go to File \u0026gt; New File \u0026gt; C++ File and will open a C++ Template like this:\n\r\rFigure 3: C++ Template\r\r\rThe main thing you need to be aware of is loading the required libraries from C++. In this case we will use the following:\nAll C++ code chunks will be combined to the chunk below:\n// [[Rcpp::depends(RcppEigen)]]\r#include \u0026lt;Rcpp.h\u0026gt;\r#include \u0026lt;RcppEigen.h\u0026gt;\r#include \u0026lt;Eigen/LU\u0026gt; #include \u0026lt;Eigen/Eigenvalues\u0026gt; using namespace Rcpp;\rusing namespace Eigen;\rAs you may know C++ is a compiled language. Compilation means, in really simple words, to optimize and speed up the code making it available in R through functions. If you want functions to be available in the R environment they need to be preceeded by this special comment. Otherwise they can be called from within the C++ environment as intermediate functions but they won¬¥t work in R.\n\rStiffness Method\rStiff Matrix by Element\rThis Step calculates Stiff for the mini-structures, meaning every single bar.\nEvery Element Matrix has the following form that needs to be created according to its properties.\n\\[\r[K]_j=\\begin{bmatrix}\rc^2 \u0026amp; \u0026amp; \u0026amp; sim\\\\\rcs \u0026amp; s^2 \u0026amp; \u0026amp; \\\\\r-c^2 \u0026amp; -cs \u0026amp; c^2 \u0026amp; \\\\\r-cs \u0026amp; -s^2 \u0026amp; cs \u0026amp; s^2 \\\\\r\\end{bmatrix}\r\\]\rThe pseudo code is as follows:\n\\[ Ne \\leftarrow \\text{Number of Rows in the Element Matrix} \\\\\rc \\leftarrow \\text{ Sparse Matrix for Director Cosines, Dimension Ne x 1 } \\\\\rs \\leftarrow \\text{ Sparse Matrix for Director Sinus, Dimension Ne x 1 } \\\\\rL \\leftarrow \\text{ Sparse Matrix for Element Length, Dimension Ne x 1 } \\\\\r\\text{for j = 1 to Ne do}\r\\left\\{ \\begin{array}{lcc}\rNi=Elements(j,1) \\\\ Nf=Elements(j,2) \\\\\r\\Delta x = Nodes(Nf,1) - Nodes(Ni,1) \\\\\r\\Delta y = Nodes(Nf,2) - Nodes(Ni,2) \\\\\rL(j)=\\sqrt{\\Delta x^2 + \\Delta y^2} \\\\\rc(j) = {\\Delta x\\over L(j)} \\\\\rs(j) = {\\Delta y\\over L(j)}\r\\end{array}\r\\right.\r\\]\nNow translating this into Rcpp looks like this:\n\rYou need define every object to use preceeded by its type.\rThe output will be an R List since I want object storing the different Element Matrix.\rAll of the Function arguments are Mandatory by default and need to go in the same order that will be used. If an Optional Argument is needed the default value needs to be defined as in NN_e.\r\r// [[Rcpp::export]]\r// First you define the Output Type. In this case an R List.\rList K_Element(NumericMatrix Nodes, NumericMatrix Elements, int NN_e = 2){\r// Ne is defined by using the nrow method to calculate number of rows.\rint Num_Elements = Elements.nrow();\r// c, s and L are defined Vectors since the second Dimension is 1.\rNumericVector c (Num_Elements);\rNumericVector s (Num_Elements);\rNumericVector L (Num_Elements);\rint j,Ni,Nf;\r// dx and dy are defined as doubles since they can contain decimals\rdouble dx,dy;\rList K_list (Num_Elements);\r// C++ is defined from 0 as the first element. So the pseudo code needs to be adjusted accordingly.\r// Notice the for syntax, from 0 to NE-1 defined as j\u0026lt;Num_Elementos and the ++j iterator\rfor(j=0;j\u0026lt;Num_Elements;++j){\rNi=Elements(j,0) -1;\rNf=Elements(j,1) - 1;\rdx=Nodes(Nf,0)-Nodes(Ni,0);\rdy=Nodes(Nf,1)-Nodes(Ni,1);\r//pow is the C++ operator for ^\rL[j]=sqrt(pow(dx,2)+pow(dy,2));\rc(j)=dx/L(j);\rs(j)=dy/L(j);\r// This is a special way to define a Matrix by Element coming from library(RcppEigen)\rMatrix4f ke;\rke \u0026lt;\u0026lt; pow(c[j],2),c[j]*s[j],-pow(c[j],2),-c[j]*s[j],\rc[j]*s[j],pow(s[j],2),-c[j]*s[j], -pow(s[j],2),\r-pow(c[j],2),-c[j]*s[j],pow(c[j],2),c[j]*s[j],\r-c[j]*s[j],-pow(s[j],2),c[j]*s[j],pow(s[j],2);\r//Here you populate every List Element with the corresponding Element Matrix\rK_list[j]= Elements(j,NN_e)*Elements(j,NN_e + 1)/L[j]*ke; }\rreturn K_list;\r}\r/*** R\r(K_E \u0026lt;- K_Element(Nodes,Elements))\r*/\r\rActive DoF Assembly\rThe Stiffness Method needs to determine what Dof are actually active, meaning that are free to move, hence are unknowns of the equation of the problem.\rIn order to do that it is necessary to determine which ones are free to move depending on the support Matrix and a Position Number is assigned to them.\nPseudocode as follows:\n\\[ Nn \\leftarrow \\text{Number of Rows in the Node Matrix} \\\\\rGl_act \\leftarrow \\text{ Sparse Matrix Dimension (NN \\cdot Ngl_N) x 1 } \\\\\rcont = 0 \\\\\r\\begin{aligned}\r\u0026amp; \\text{for i = 1 to Nn do } \\\\\r\u0026amp; \\text{for k = 1 to Ngl_N do} \\\\\r\\end{aligned} \\\\\r\\left\\{ \\begin{array}{lcc}\r\\text{if Apoyos(i,k) = 0 then} \\\\\rcont= cont +1 \\\\\rpos=Ngl_N \\cdot (i-1) + k \\\\\rGl_act(pos)=cont \\\\\r\\end{array}\r\\right.\r\\]\rRcpp Code:\n// [[Rcpp::export]] // Sparse Vector that uses Support Matrix as Input NumericVector Gr_Active(NumericMatrix Support, int Ngl_N = 2){\rint Num_Nodes = Support.nrow();\rint cont=0, i, k;\r//Defining Dimension of Gl Vector\rNumericVector Gl (Num_Nodes*Ngl_N);\rfor(i = 0; i \u0026lt; Num_Nodes; ++i){\rfor(k = 0; k \u0026lt; Ngl_N; ++k){\rif(Apoyos(i,k)==0){\r//Counter needs to be adapted since C++ starts off at Zero\rGl[Ngl_N*i+k] = ++cont;\r}\r} }\rreturn Gl;\r}\r\r\rConnectivity Array\rThe Method determines an array to identify how the different elements are connected each other. This way it is possible to create an equivalent Matrix representing the Equivalent Stiffness of the ensembled elements.\n\\[ Ngle = Ngl_N \\cdot NN_e \\\\\rconect \\leftarrow \\text{ Sparse Matrix Dimension Ne x Ngle } \\\\\r\\begin{aligned}\r\u0026amp; \\text{for j = 1 to Ne do } \\\\\r\u0026amp; \\text{for k = 1 to NN_e do} \\\\\r\u0026amp; N_k=Elementos(j,k) \\\\\r\u0026amp; pos1= (N_k - 1) \\cdot Ngl_N \\\\\r\\end{aligned} \\\\\r\\text{ for l= 1 to Ngl_N do } \\\\\r\\left\\{ \\begin{array}{lcc}\rpos2=pos1+l \\\\\rpos3= (k-1) \\cdot Ngl_N + l \\\\\rconect(j,pos3) = Gl_act(pos2) \\\\\r\\end{array}\r\\right.\r\\]\rRcpp Code:\n// [[Rcpp::export]]\r// This is a Numeric Matrix using Elements Matrix and Gl Vector as Input\rNumericMatrix Arr_Connect(NumericMatrix Elements, NumericVector Gl, int NN_e = 2, int Ngl_N = 2){\rint Num_Elements = Elements.nrow();\r// Several counters an be defined simultaneously if sharing the same properties.\rint j, k, l, pos1, pos2, pos3;\rNumericMatrix conect(Num_Elements, NN_e * Ngl_N);\rfor(j=0; j \u0026lt; Num_Elements; ++j){\rfor(k=0; k \u0026lt; NN_e; ++k){\rpos1 = (Elements(j,k) - 1) * Ngl_N;\rfor(l=0; l \u0026lt; Ngl_N; ++l){\rpos2 = pos1 + l;\r// pos3 had to be adjusted because C++ index starting at 0 pos3 = k * Ngl_N + l;\rconect(j,pos3) = Gl[pos2];\r}\r}\r}\rreturn conect;\r}\r\rStiffness Matrix Assembly\rOnce the Connectivity Array and the Active DoFs are determined the Global Stiffness Matrix can be assembled. This matrix contains the Contribution of every element to an specific Node. Less Elements joined to a specific Node will end up adding less stiffness than a lot of elements being part of a Node.\nPseudocode as follows:\n\\[ N_R \\leftarrow \\text{ sum of all of the entries of the support Matrix } \\\\\rNGl_total = Ngl_N \\cdot Nn - N_R \\\\\rK \\leftarrow \\text{ Sparse Matrix Ngl_total x Ngl_total } \\\\\r\\begin{aligned}\r\u0026amp; \\text{for j = 1 to Ne do } \\\\\r\u0026amp; \\text{for k = 1 to Ngle do} \\\\\r\\end{aligned} \\\\\r\\text{ for l= 1 to Ngl_e do } \\\\\r\\left\\{ \\begin{array}{lcc}\rpos1=conect(j,k) \\\\\rpos2=conect(j,l) \\\\\rtext{ if conect(j,k) \\neq 0 and conect(j,l) \\neq 0 then } \\\\\rK(pos1,pos2)=K_E{j}(k,l) + K(pos1,pos2) \\\\\r\\end{array}\r\\right.\r\\]\rRcpp Code:\n// [[Rcpp::export]]\r//Numeric Matrix using Support, Gl and Conect Matrix and K_E List as Inputs\rNumericMatrix K_Total(List K_E, NumericMatrix Support, NumericVector Gl, NumericMatrix conect, int NN_e = 2, int Ngl_N =2 ){\rint Num_Elements = K_E.length();\rint Num_Nodes = Support.nrow();\rint Nr=sum(Support), j, k, l, pos1, pos2;\rNumericMatrix K( Ngl_N * Num_Nodos- Nr );\rint Ngl_E = NN_e * Ngl_N;\rfor(j=0; j\u0026lt;Num_Elements; ++j){\rfor(k=0; k\u0026lt;Ngl_E; ++k){\rfor(l=0; l\u0026lt;Ngl_E; ++l){\rpos1 = conect(j,k);\rpos2 = conect(j,l);\r//Notice that List Elements need to be pulled using brakets\rNumericMatrix Ke = K_E[j];\r// and operator uses double ampersand and inequality syntax follow same rules than R\rif(pos1 != 0 \u0026amp;\u0026amp; pos2 !=0){\r// += is the C++ operator to sum the new value to the current one.\rK(pos1 - 1, pos2 - 1) += Ke(k,l);\r}\r}\r}\r}\rreturn K;\r}\r\r\rLoad Vector Assembly\rThis is the equivalent load Vector considering only Loads for active DoFs that are participating in the solution of the problem.\n\\[ F \\leftarrow \\text{ Sparse Matrix dimension Ngl_total x 1 } \\\\\r\\begin{aligned}\r\u0026amp; \\text{for i = 1 to Nn do } \\\\\r\\end{aligned} \\\\\r\\text{ for k= 1 to Ngl_n do } \\\\\r\\left\\{ \\begin{array}{lcc}\rpos1=Ngl_n \\cdot (i-1) + k \\\\\rpos2=Gl_act(pos1) \\\\\r\\text{ if pos2 Loads(i,k) } \\\\\rF(pos2)=Cargas(i,k)\\\\\r\\end{array}\r\\right.\r\\]\rRcpp Code:\n// [[Rcpp::export]]\rNumericVector f_Total(NumericMatrix Loads, NumericVector Gl, int Nr, int Ngl_N = 2 ){\rint Num_Nodos = Loads.nrow();\rint N_t = Ngl_N * Num_Nodos - Nr;\rNumericVector F (N_t);\rint i,k,pos1,pos2;\rfor(i=0; i \u0026lt; Num_Nodos; ++i){\rfor(k=0; k \u0026lt; Ngl_N; ++k){\rpos1 = Ngl_N * i + k;\rpos2 = Gl[pos1];\rif(pos2 != 0){\rF[pos2 - 1] = Cargas(i,k);\r}\r}\r}\rreturn F;\r}\r\rSolving the Problem\rAll this Steps allows to pose the following problem:\n\\[ [K] \\cdot \\{u\\} = \\{F\\} \\]\nIn order to get the desired displacements it is just necessary to inverse $ [K] $.\n\\[ \\{u\\} = [K]^{-1} \\cdot \\{F\\}\\]\nFor this case I¬¥ll be using RcppEigen, a Rcpp Linear Algebra Library that allows some extra Matrix operations that are useful for, in this case, Matrix inversion:\n// I have defined a new object type called MapMatd whih is a Matrix with no specific size of doubles\rtypedef Map\u0026lt;MatrixXd\u0026gt; MapMatd;\r// Defined a Vector with same characteristics as before\rtypedef Map\u0026lt;VectorXd\u0026gt; MapVecd;\r// [[Rcpp::export]]\r// I use a VectorXd non defined size X with double data type d\rVectorXd u_vect(NumericMatrix K_Total, NumericVector f_Total){\r//I need to cast R Objects coming from Inputs into Eigen Objects. In this case i would just say trust me.\rconst MapMatd K(as\u0026lt;MapMatd\u0026gt;(K_Total));\rconst MapVecd f(as\u0026lt;MapVecd\u0026gt;(f_Total));\r//Applying Inverse Method, this is only available because K and f are already Eigen objets\rVectorXd result = K.inverse()*f;\rreturn result;\r}\r\r\rConclusions\r\rR and Rcpp share a very similar syntax.\rAll R objects are compatible with Rcpp, even Lists\rThe Main advantage of using Rcpp is that is way too faster than Regular R. This makes it especially suitable for Algorithms and Matrix manipulation.\rNotice that Matrices use () for indexing whereas Vectors and Lists use [].\rRcpp starts at 0, make the proper adjustments when dealing with indices.\r\rI¬¥ll be posting another Entry using the recently reated functions to show how fast they are. Stay tuned!!\n\r\r","date":1569196800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569387730,"objectID":"aa94fc09c8d96ba39ca46ac3f1b7b171","permalink":"/project/stiffness-method/","publishdate":"2019-09-23T00:00:00Z","relpermalink":"/project/stiffness-method/","section":"project","summary":"How to Implement the Stiffness Method using Rcpp","tags":["Civil Engineering","Stiffness Method"],"title":"The Stiffness Method","type":"project"},{"authors":["Alfonso Tobar"],"categories":["Tutorial"],"content":"\r\r\r1 Tidymodels\r1.1 Spliting the Data\r1.2 Pre Processing\r\r2 Create the Logistic Regression\r3 Conclusions\r\r\rNow we have an idea on how the data looks like it is time to Model.\n1 Tidymodels\rI¬¥m a huge fan of tidymodels framework and the way Max Kuhn has put together all of this system. I¬¥ll be using several packages from this framework in order to show different steps of the Machine Learning Process.\n1.1 Spliting the Data\rWe will be splitting Data into Training and Test Sets with 70/30 proportion based on the Ins Response Variable.\nlibrary(rsample)\r#Reproducibility\rset.seed(27101986)\r#70/30 Split stratifying the Target Variable Ins\rsplit \u0026lt;- initial_split(data, prop = 0.7, strata = \u0026quot;Ins\u0026quot;)\rdata_training \u0026lt;- split %\u0026gt;% training()\rdata_testing \u0026lt;- split %\u0026gt;% testing()\r\r1.2 Pre Processing\rAfter splitting Data I will be conducting Pre Processing with the great Recipes Package.\nRecipes basically mimics the Pre Processing Steps to a Baking Recipe following different sequential steps in order to prepare and Bake the Data (Make the Data Ready to Model).\nRecipes have step_* functions in charge of applying different Pre-Processing Steps. Plus includes Variable helpers to call Variables by Type or Role.\nlibrary(recipes)\r## ## Attaching package: \u0026#39;recipes\u0026#39;\r## The following object is masked from \u0026#39;package:stringr\u0026#39;:\r## ## fixed\r## The following object is masked from \u0026#39;package:stats\u0026#39;:\r## ## step\r# Sets the Recipe indicating that Ins will be modeled using all the rest of the variables\radvance_rec \u0026lt;- recipe(Ins ~ . , data = data_training) %\u0026gt;%\rstep_dummy(all_nominal(), -all_outcomes()) %\u0026gt;% #create dummy variables for all categorical variables excepting the Ins Variable\rstep_nzv(all_numeric()) %\u0026gt;% #eliminates numerical variables with variance near to zero\rstep_corr(all_predictors()) %\u0026gt;% #eliminates highly correlated variables\rstep_BoxCox(all_predictors()) %\u0026gt;% #fix highly skewed variables\rstep_center(all_numeric()) %\u0026gt;% #substracts mean\rstep_scale(all_numeric()) %\u0026gt;% #divides by sd. This both steps standardize the variables\r#Prepares the data according to the data in the Training Set\rprep(training = data_training)\r#Applies Training Data according to Preprocessing\rtrain_advance \u0026lt;- bake(advance_rec, new_data = data_training)\r#The main difference with Bake is that Bake skips the processes affecting the outcome variable, suh as resamples, logs transform, etc. test_advance \u0026lt;- bake(advance_rec, new_data = data_testing)\r\r\r2 Create the Logistic Regression\rWe will use Logistic regresson using Parsnip and we will Assess the Model using yardstick\nlibrary(parsnip)\r#Using Parsnip to run classification, using glm engine and fitting train data already pre-processed\rfull_advance \u0026lt;- logistic_reg(mode = \u0026quot;classification\u0026quot;) %\u0026gt;%\rset_engine(\u0026quot;glm\u0026quot;) %\u0026gt;%\rfit(Ins ~ ., data = train_advance)\r#Predicting Class with Model \u0026quot;Full Advance\u0026quot; in the Test Set\rfull_pred_advance \u0026lt;- full_advance %\u0026gt;%\rpredict(new_data= test_advance, type = \u0026quot;class\u0026quot;)\r#Predicting class Probabilities with Model \u0026quot;Full Advance\u0026quot;\rfull_pred_probs_advance \u0026lt;- full_advance %\u0026gt;%\rpredict(new_data= test_advance, type = \u0026quot;prob\u0026quot;)\rlibrary(yardstick)\r## For binary classification, the first factor level is assumed to be the event.\r## Set the global option `yardstick.event_first` to `FALSE` to change this.\r## ## Attaching package: \u0026#39;yardstick\u0026#39;\r## The following object is masked from \u0026#39;package:readr\u0026#39;:\r## ## spec\rcomparison_test \u0026lt;- bind_cols(\r\u0026quot;Real\u0026quot; = test_advance$Ins,\r\u0026quot;Prediction\u0026quot; = full_pred_advance,\r\u0026quot;Class1\u0026quot; = full_pred_probs_advance$.pred_yes\r) %\u0026gt;% setNames(c(\u0026quot;Real\u0026quot;,\u0026quot;Prediction\u0026quot;,\u0026quot;Class1\u0026quot;))\r#Calculating Confusion Matrix\rcomparison_test %\u0026gt;% conf_mat(Real,Prediction)\r## Truth\r## Prediction yes no\r## yes 1003 464\r## no 1248 3547\r#Calculating Assesment Metrics for Model\rcomparison_test %\u0026gt;% conf_mat(Real,Prediction) %\u0026gt;%\rsummary()\r{\"columns\":[{\"label\":[\".metric\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\".estimator\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\".estimate\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"accuracy\",\"2\":\"binary\",\"3\":\"0.7266049\"},{\"1\":\"kap\",\"2\":\"binary\",\"3\":\"0.3571922\"},{\"1\":\"sens\",\"2\":\"binary\",\"3\":\"0.4455797\"},{\"1\":\"spec\",\"2\":\"binary\",\"3\":\"0.8843181\"},{\"1\":\"ppv\",\"2\":\"binary\",\"3\":\"0.6837082\"},{\"1\":\"npv\",\"2\":\"binary\",\"3\":\"0.7397289\"},{\"1\":\"mcc\",\"2\":\"binary\",\"3\":\"0.3737526\"},{\"1\":\"j_index\",\"2\":\"binary\",\"3\":\"0.3298979\"},{\"1\":\"bal_accuracy\",\"2\":\"binary\",\"3\":\"0.6649489\"},{\"1\":\"detection_prevalence\",\"2\":\"binary\",\"3\":\"0.2342702\"},{\"1\":\"precision\",\"2\":\"binary\",\"3\":\"0.6837082\"},{\"1\":\"recall\",\"2\":\"binary\",\"3\":\"0.4455797\"},{\"1\":\"f_meas\",\"2\":\"binary\",\"3\":\"0.5395374\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\r#ROC Curve\rcomparison_test %\u0026gt;%\rroc_curve(Real,Class1) %\u0026gt;%\rautoplot()\r## Setting direction: controls \u0026lt; cases\r## Warning in coords.roc(curv, x = unique(c(-Inf, options$predictor, Inf)), :\r## An upcoming version of pROC will set the \u0026#39;transpose\u0026#39; argument to FALSE\r## by default. Set transpose = TRUE explicitly to keep the current behavior,\r## or transpose = FALSE to adopt the new one and silence this warning. Type\r## help(coords_transpose) for additional information.\r#ROC AUC\rcomparison_test %\u0026gt;%\rroc_auc(Real,Class1)\r## Setting direction: controls \u0026lt; cases\r{\"columns\":[{\"label\":[\".metric\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\".estimator\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\".estimate\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"roc_auc\",\"2\":\"binary\",\"3\":\"0.7733382\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\r#Calculated Model\rfull_advance$fit %\u0026gt;%\rtidy() \r{\"columns\":[{\"label\":[\"term\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"estimate\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"std.error\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"statistic\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"p.value\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"(Intercept)\",\"2\":\"6.117855e-01\",\"3\":\"0.01993088\",\"4\":\"3.069536e+01\",\"5\":\"6.563661e-207\"},{\"1\":\"AcctAge\",\"2\":\"1.274360e-01\",\"3\":\"0.02041803\",\"4\":\"6.241345e+00\",\"5\":\"4.338251e-10\"},{\"1\":\"DDABal\",\"2\":\"-3.345807e-01\",\"3\":\"0.04233735\",\"4\":\"-7.902732e+00\",\"5\":\"2.728554e-15\"},{\"1\":\"Checks\",\"2\":\"8.001763e-02\",\"3\":\"0.02764014\",\"4\":\"2.894979e+00\",\"5\":\"3.791843e-03\"},{\"1\":\"Phone\",\"2\":\"1.188238e-01\",\"3\":\"0.02661229\",\"4\":\"4.464997e+00\",\"5\":\"8.006974e-06\"},{\"1\":\"Teller\",\"2\":\"-1.992734e-01\",\"3\":\"0.02252055\",\"4\":\"-8.848515e+00\",\"5\":\"8.869169e-19\"},{\"1\":\"SavBal\",\"2\":\"-6.805505e-01\",\"3\":\"0.04626114\",\"4\":\"-1.471106e+01\",\"5\":\"5.474173e-49\"},{\"1\":\"ATMAmt\",\"2\":\"-3.045024e-01\",\"3\":\"0.03644366\",\"4\":\"-8.355428e+00\",\"5\":\"6.519615e-17\"},{\"1\":\"POS\",\"2\":\"9.988335e-02\",\"3\":\"0.03999723\",\"4\":\"2.497256e+00\",\"5\":\"1.251584e-02\"},{\"1\":\"POSAmt\",\"2\":\"-1.015285e-01\",\"3\":\"0.03693182\",\"4\":\"-2.749079e+00\",\"5\":\"5.976290e-03\"},{\"1\":\"CCBal\",\"2\":\"6.859325e-02\",\"3\":\"0.01891700\",\"4\":\"3.626011e+00\",\"5\":\"2.878328e-04\"},{\"1\":\"CCPurc\",\"2\":\"-2.941948e-02\",\"3\":\"0.02026686\",\"4\":\"-1.451605e+00\",\"5\":\"1.466114e-01\"},{\"1\":\"Income\",\"2\":\"1.272866e-01\",\"3\":\"0.03479794\",\"4\":\"3.657877e+00\",\"5\":\"2.543134e-04\"},{\"1\":\"LORes\",\"2\":\"-6.014610e-02\",\"3\":\"0.02823814\",\"4\":\"-2.129960e+00\",\"5\":\"3.317495e-02\"},{\"1\":\"HMVal\",\"2\":\"-1.935050e-01\",\"3\":\"0.03770266\",\"4\":\"-5.132397e+00\",\"5\":\"2.860749e-07\"},{\"1\":\"Age\",\"2\":\"-2.219778e-05\",\"3\":\"0.02288118\",\"4\":\"-9.701325e-04\",\"5\":\"9.992259e-01\"},{\"1\":\"CRScore\",\"2\":\"1.337544e-02\",\"3\":\"0.02203018\",\"4\":\"6.071418e-01\",\"5\":\"5.437568e-01\"},{\"1\":\"Dep\",\"2\":\"1.028510e-01\",\"3\":\"0.03356444\",\"4\":\"3.064284e+00\",\"5\":\"2.181913e-03\"},{\"1\":\"DepAmt\",\"2\":\"-1.901625e-02\",\"3\":\"0.02478175\",\"4\":\"-7.673488e-01\",\"5\":\"4.428742e-01\"},{\"1\":\"DDA_yes\",\"2\":\"2.707683e-01\",\"3\":\"0.02521098\",\"4\":\"1.074009e+01\",\"5\":\"6.597812e-27\"},{\"1\":\"DirDep_yes\",\"2\":\"7.152448e-03\",\"3\":\"0.02218401\",\"4\":\"3.224146e-01\",\"5\":\"7.471386e-01\"},{\"1\":\"NSF_yes\",\"2\":\"-2.370392e-02\",\"3\":\"0.02123988\",\"4\":\"-1.116010e+00\",\"5\":\"2.644177e-01\"},{\"1\":\"Sav_yes\",\"2\":\"-2.547525e-01\",\"3\":\"0.02196270\",\"4\":\"-1.159932e+01\",\"5\":\"4.153464e-31\"},{\"1\":\"ATM_yes\",\"2\":\"1.292603e-01\",\"3\":\"0.02414396\",\"4\":\"5.353733e+00\",\"5\":\"8.615816e-08\"},{\"1\":\"CD_yes\",\"2\":\"-2.918515e-01\",\"3\":\"0.01859470\",\"4\":\"-1.569541e+01\",\"5\":\"1.625822e-55\"},{\"1\":\"LOC_yes\",\"2\":\"6.110583e-03\",\"3\":\"0.01938014\",\"4\":\"3.153013e-01\",\"5\":\"7.525329e-01\"},{\"1\":\"MM_yes\",\"2\":\"-2.589903e-01\",\"3\":\"0.01974444\",\"4\":\"-1.311713e+01\",\"5\":\"2.627075e-39\"},{\"1\":\"CC_yes\",\"2\":\"-1.798393e-01\",\"3\":\"0.02157689\",\"4\":\"-8.334809e+00\",\"5\":\"7.762339e-17\"},{\"1\":\"SDB_yes\",\"2\":\"-3.708755e-02\",\"3\":\"0.01901094\",\"4\":\"-1.950853e+00\",\"5\":\"5.107453e-02\"},{\"1\":\"HMOwn_yes\",\"2\":\"-1.060297e-02\",\"3\":\"0.02920242\",\"4\":\"-3.630853e-01\",\"5\":\"7.165412e-01\"},{\"1\":\"Branch_B2\",\"2\":\"-4.272629e-03\",\"3\":\"0.02524865\",\"4\":\"-1.692221e-01\",\"5\":\"8.656220e-01\"},{\"1\":\"Branch_B3\",\"2\":\"-2.564723e-02\",\"3\":\"0.02535713\",\"4\":\"-1.011440e+00\",\"5\":\"3.118057e-01\"},{\"1\":\"Branch_B1\",\"2\":\"2.605656e-02\",\"3\":\"0.02565672\",\"4\":\"1.015584e+00\",\"5\":\"3.098275e-01\"},{\"1\":\"Branch_B7\",\"2\":\"3.878624e-02\",\"3\":\"0.02309364\",\"4\":\"1.679521e+00\",\"5\":\"9.305058e-02\"},{\"1\":\"Branch_B5\",\"2\":\"-1.824560e-02\",\"3\":\"0.02531664\",\"4\":\"-7.206961e-01\",\"5\":\"4.710965e-01\"},{\"1\":\"Branch_B6\",\"2\":\"-8.867298e-03\",\"3\":\"0.02270427\",\"4\":\"-3.905563e-01\",\"5\":\"6.961252e-01\"},{\"1\":\"Branch_B4\",\"2\":\"-1.583160e-02\",\"3\":\"0.02902725\",\"4\":\"-5.454048e-01\",\"5\":\"5.854752e-01\"},{\"1\":\"Branch_B16\",\"2\":\"1.673182e-01\",\"3\":\"0.02501004\",\"4\":\"6.690040e+00\",\"5\":\"2.231092e-11\"},{\"1\":\"Branch_B8\",\"2\":\"-4.156275e-02\",\"3\":\"0.02204090\",\"4\":\"-1.885710e+00\",\"5\":\"5.933403e-02\"},{\"1\":\"Res_suburb\",\"2\":\"2.037763e-02\",\"3\":\"0.02505362\",\"4\":\"8.133607e-01\",\"5\":\"4.160113e-01\"},{\"1\":\"Res_urban\",\"2\":\"4.361963e-02\",\"3\":\"0.02516920\",\"4\":\"1.733056e+00\",\"5\":\"8.308567e-02\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\r\r3 Conclusions\rWe have run a Machine Learning Process using:\n\rrsamples for splitting data.\rrecipes for Pre-Processing.\rparsnip to fit the model\ryardstick to measure the performance.\r\rFinally 41 variables were kept getting a 72% of accuracy and a 77.3% of ROC AUC.\n\r","date":1566864000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566864000,"objectID":"1648f0dff9399f69ae0d6a3c66ce4a69","permalink":"/project/machine-learning-diploma-iii/","publishdate":"2019-08-27T00:00:00Z","relpermalink":"/project/machine-learning-diploma-iii/","section":"project","summary":"I¬¥ll be showing the TidyModels frame work to create a Machine Learning Model","tags":["Machine Learning","Parsnip","Rsample","Recipes","Yardstick"],"title":"My Final Project at the ML Diploma (Part III)","type":"project"},{"authors":["Alfonso Tobar"],"categories":["Tutorial"],"content":"\r\r\r1 Checking Numerical Distribution\r2 Checking Categorical Variables\r3 Chi-Square Test\r4 Conclusion\r\r\rLast time we conducted a high level cleansing of the data. Now it¬¥s time to understand what is going on in it. In order to do that we¬¥ll use a lot ggplot to visualize the data.\n1 Checking Numerical Distribution\rIn order to do this I should pick Numerical Variables one by one and create a ggplot.\rThis ould actually be quite tedious, why not to use the power of the tidyverse?\nWe will combine select_if and walk 2 to create histograms for every of the 28 Numerical Variables.\n\rNotice that in order to make walk work silently I had to add a print function that will use .x (every column) to create a histogram labeling it with .y that is the actual name of the current .x.\n\r# Take the data\rdata %\u0026gt;%\r# I select only data that is numerical\rselect_if(is.numeric) %\u0026gt;%\r# I use walk 2 where .x is every numerical column seleted by select_if and\r#.y are the names of .x that will be used to add the proper label.\rwalk2(names(.), ~ print( data %\u0026gt;%\rggplot(aes(.x)) + geom_histogram() + labs(x = .y))) \r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r\r2 Checking Categorical Variables\rSomething equivalent can be done with categorical variables to check how they are distributed with the following code:\ndata %\u0026gt;%\rselect_if(is.factor) %\u0026gt;%\rsummary()\r## DDA DirDep NSF Sav ATM CD ## no : 3742 no :14838 no :19044 no :11135 no : 8085 no :18244 ## yes:17135 yes: 6039 yes: 1833 yes: 9742 yes:12792 yes: 2633 ## ## ## ## ## ## IRA LOC ILS MM MTG CC ## no :19837 no :19742 no :19926 no :18588 no :19932 no :10936 ## yes: 1040 yes: 1135 yes: 951 yes: 2289 yes: 945 yes: 9941 ## ## ## ## ## ## SDB HMOwn Moved InArea Ins ## no :18573 no : 9617 no :20251 no : 823 yes: 7504 ## yes: 2304 yes:11260 yes: 626 yes:20054 no :13373 ## ## ## ## ## ## Branch Res Inv ## B4 :4586 rural :5532 no :20272 ## B3 :2332 suburb:7359 yes: 605 ## B1 :2292 urban :7986 ## B5 :2269 ## B2 :2267 ## B16 :1261 ## (Other):5870\rIn case you want something more visual you could go with this:\n# Take the data\rdata %\u0026gt;%\r# I select only data that is numerical\rselect_if(is.factor) %\u0026gt;%\r# I use walk 2 where .x is every numerical column seleted by select_if and\r#.y are the names of .x that will be used to add the proper label.\rwalk2(names(.), ~ print( data %\u0026gt;%\rggplot(aes(.x)) + geom_bar() + labs(x = .y))) \r\r3 Chi-Square Test\rWhat about performing a Chi-Square test to check the relationship between the Response variable and the Categorical Variables.\nLet¬¥s create a NSE function to apply Chi-Square using purrr.\nWe¬¥ll use the Categorical Object created in the previous part to be looped over the chi-square function.\n#Listing all of the Categorical Variables according to Metadata\rcategorical \u0026lt;- c(\u0026quot;ATM\u0026quot;, \u0026quot;Branch\u0026quot;, \u0026quot;CC\u0026quot;, \u0026quot;CD\u0026quot;, \u0026quot;DDA\u0026quot;, \u0026quot;DirDep\u0026quot;, \u0026quot;HMOwn\u0026quot;, \u0026quot;ILS\u0026quot;, \u0026quot;IRA\u0026quot;, \u0026quot;InArea\u0026quot;, \u0026quot;Ins\u0026quot;, \u0026quot;Inv\u0026quot;, \u0026quot;LOC\u0026quot;, \u0026quot;MM\u0026quot;, \u0026quot;MTG\u0026quot;, \u0026quot;Moved\u0026quot;, \u0026quot;NSF\u0026quot;, \u0026quot;Res\u0026quot;, \u0026quot;SDB\u0026quot;, \u0026quot;Sav\u0026quot;)\r#Loading rlang\rsuppressPackageStartupMessages(library(rlang))\r#since I want to use var as a Non Standard Evaluation Variable I need to pass that variable using the Curly-Curly Operator. That way I don¬¥t need to quote variables and can go directly into dplyr functions such as select.\rchi_comparison \u0026lt;- function(var){\rpred \u0026lt;- data %\u0026gt;%\rselect({{ var }})\r#Performs Chi-Square test and returns p.value\rreturn(tibble(p_val = chisq.test(pred, data$Ins)$p.value))\r}\r(independent \u0026lt;- categorical %\u0026gt;%\rmap_dfr(chi_comparison) %\u0026gt;%\rcbind(independent = categorical) %\u0026gt;%\rfilter(p_val \u0026gt; 0.05) )\r{\"columns\":[{\"label\":[\"p_val\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"independent\"],\"name\":[2],\"type\":[\"fctr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"0.1629036\",\"2\":\"HMOwn\"},{\"1\":\"0.5485035\",\"2\":\"ILS\"},{\"1\":\"0.1298597\",\"2\":\"MTG\"},{\"1\":\"0.8984779\",\"2\":\"Moved\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\rThis results in 4 Variables returning a p-value grater than 0.05. This means this variables are independent to the Response Variables, so no relationship between them exist, hence they could be removed from the model to build.\n\r4 Conclusion\rA quick EDA has been performed using ggplot2 combined with purrr and dplyr.\r* It can be seen that Age and CRSore have distribution fairly close to Normal.\r* Income is right skewed.\r* Most of the Numerical Variables are higly concentrated at lower values.\rOn the categorical side:\r* Most of the categorical variables show severe problems with class imbalances.\r* HMOwn, ILS, MTG and Moved seem to have no relationship with the Response Variable.\r* The Response Variable Ins show some imbalances but nothing to severe to be treated in a special way.\nMore to come on this problem. Stay Tuned!!!\n\r","date":1565222400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565222400,"objectID":"87f85a4759e80fcf9b7025246594f550","permalink":"/project/machine-learning-diploma-ii/","publishdate":"2019-08-08T00:00:00Z","relpermalink":"/project/machine-learning-diploma-ii/","section":"project","summary":"Second Part of the ML Diploma Project. This time I¬¥ll be showing some EDA","tags":["Machine Learning","Data Import (haven)","Data Cleaning"],"title":"My Final Project at the ML Diploma (Part II)","type":"project"},{"authors":["Alfonso Tobar"],"categories":["Tutorial"],"content":"\r\r\r1 Importing Data\r2 Redefining Categorical Variables\r3 Discovering Missing Values\r4 Conclusion\r\r\rDuring my Machine Learning Diploma I had the chance to work on a very interesting project that was actually created in SAS. Of course I absolutely refused to use that old fashioned tool and I move everything to R.\nI will try to demonstrate as much of the packages I used to perform this analysis.\n1 Importing Data\rSince the data is coming from a SAS format, it was absolutely necessary to use this incredibly tidyverse package called haven.\nThe code to import the data is super simple and goes like this:\n#Loading silent tidyverse to make normal utility functions available\rsuppressPackageStartupMessages(library(tidyverse))\rlibrary(haven)\rdata \u0026lt;- read_sas(\u0026quot;develop.sas7bdat\u0026quot;)\rThis will import the data into an R object:\n#Showing 10 first Obs\rdata %\u0026gt;%\rhead\r{\"columns\":[{\"label\":[\"AcctAge\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"DDA\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"DDABal\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"CashBk\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Checks\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"DirDep\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"NSF\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"NSFAmt\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Phone\"],\"name\":[9],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Teller\"],\"name\":[10],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Sav\"],\"name\":[11],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"SavBal\"],\"name\":[12],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"ATM\"],\"name\":[13],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"ATMAmt\"],\"name\":[14],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"POS\"],\"name\":[15],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"POSAmt\"],\"name\":[16],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"CD\"],\"name\":[17],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"CDBal\"],\"name\":[18],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"IRA\"],\"name\":[19],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"IRABal\"],\"name\":[20],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"LOC\"],\"name\":[21],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"LOCBal\"],\"name\":[22],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"ILS\"],\"name\":[23],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"ILSBal\"],\"name\":[24],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"MM\"],\"name\":[25],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"MMBal\"],\"name\":[26],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"MMCred\"],\"name\":[27],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"MTG\"],\"name\":[28],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"MTGBal\"],\"name\":[29],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"CC\"],\"name\":[30],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"CCBal\"],\"name\":[31],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"CCPurc\"],\"name\":[32],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"SDB\"],\"name\":[33],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Income\"],\"name\":[34],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"HMOwn\"],\"name\":[35],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"LORes\"],\"name\":[36],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"HMVal\"],\"name\":[37],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Age\"],\"name\":[38],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"CRScore\"],\"name\":[39],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Moved\"],\"name\":[40],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"InArea\"],\"name\":[41],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Ins\"],\"name\":[42],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Branch\"],\"name\":[43],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"Res\"],\"name\":[44],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"Dep\"],\"name\":[45],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"DepAmt\"],\"name\":[46],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Inv\"],\"name\":[47],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"InvBal\"],\"name\":[48],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"0.3\",\"2\":\"1\",\"3\":\"419.27\",\"4\":\"0\",\"5\":\"0\",\"6\":\"0\",\"7\":\"0\",\"8\":\"0\",\"9\":\"0\",\"10\":\"0\",\"11\":\"1\",\"12\":\"10233.72\",\"13\":\"1\",\"14\":\"106.74\",\"15\":\"0\",\"16\":\"0\",\"17\":\"0\",\"18\":\"0\",\"19\":\"0\",\"20\":\"0\",\"21\":\"0\",\"22\":\"0\",\"23\":\"0\",\"24\":\"0\",\"25\":\"0\",\"26\":\"0\",\"27\":\"0\",\"28\":\"0\",\"29\":\"0\",\"30\":\"1\",\"31\":\"483.65\",\"32\":\"0\",\"33\":\"0\",\"34\":\"16\",\"35\":\"1\",\"36\":\"11.0\",\"37\":\"89\",\"38\":\"63\",\"39\":\"696\",\"40\":\"0\",\"41\":\"1\",\"42\":\"1\",\"43\":\"B17\",\"44\":\"R\",\"45\":\"2\",\"46\":\"1170.06\",\"47\":\"0\",\"48\":\"0\"},{\"1\":\"0.7\",\"2\":\"1\",\"3\":\"1986.81\",\"4\":\"0\",\"5\":\"1\",\"6\":\"1\",\"7\":\"0\",\"8\":\"0\",\"9\":\"0\",\"10\":\"0\",\"11\":\"0\",\"12\":\"0.00\",\"13\":\"1\",\"14\":\"268.88\",\"15\":\"0\",\"16\":\"0\",\"17\":\"0\",\"18\":\"0\",\"19\":\"0\",\"20\":\"0\",\"21\":\"0\",\"22\":\"0\",\"23\":\"0\",\"24\":\"0\",\"25\":\"0\",\"26\":\"0\",\"27\":\"0\",\"28\":\"0\",\"29\":\"0\",\"30\":\"1\",\"31\":\"0.00\",\"32\":\"1\",\"33\":\"0\",\"34\":\"4\",\"35\":\"1\",\"36\":\"7.0\",\"37\":\"87\",\"38\":\"51\",\"39\":\"674\",\"40\":\"0\",\"41\":\"1\",\"42\":\"0\",\"43\":\"B2\",\"44\":\"R\",\"45\":\"1\",\"46\":\"446.93\",\"47\":\"0\",\"48\":\"0\"},{\"1\":\"4.1\",\"2\":\"0\",\"3\":\"0.00\",\"4\":\"0\",\"5\":\"0\",\"6\":\"0\",\"7\":\"0\",\"8\":\"0\",\"9\":\"0\",\"10\":\"0\",\"11\":\"0\",\"12\":\"0.00\",\"13\":\"0\",\"14\":\"0.00\",\"15\":\"0\",\"16\":\"0\",\"17\":\"0\",\"18\":\"0\",\"19\":\"0\",\"20\":\"0\",\"21\":\"0\",\"22\":\"0\",\"23\":\"0\",\"24\":\"0\",\"25\":\"0\",\"26\":\"0\",\"27\":\"0\",\"28\":\"0\",\"29\":\"0\",\"30\":\"1\",\"31\":\"0.00\",\"32\":\"0\",\"33\":\"0\",\"34\":\"30\",\"35\":\"1\",\"36\":\"8.5\",\"37\":\"97\",\"38\":\"60\",\"39\":\"640\",\"40\":\"0\",\"41\":\"1\",\"42\":\"1\",\"43\":\"B3\",\"44\":\"S\",\"45\":\"0\",\"46\":\"0.00\",\"47\":\"0\",\"48\":\"0\"},{\"1\":\"0.5\",\"2\":\"1\",\"3\":\"1594.84\",\"4\":\"0\",\"5\":\"1\",\"6\":\"0\",\"7\":\"0\",\"8\":\"0\",\"9\":\"0\",\"10\":\"1\",\"11\":\"1\",\"12\":\"425.06\",\"13\":\"1\",\"14\":\"278.07\",\"15\":\"0\",\"16\":\"0\",\"17\":\"0\",\"18\":\"0\",\"19\":\"0\",\"20\":\"0\",\"21\":\"0\",\"22\":\"0\",\"23\":\"0\",\"24\":\"0\",\"25\":\"0\",\"26\":\"0\",\"27\":\"0\",\"28\":\"0\",\"29\":\"0\",\"30\":\"1\",\"31\":\"65.76\",\"32\":\"0\",\"33\":\"0\",\"34\":\"125\",\"35\":\"1\",\"36\":\"7.5\",\"37\":\"145\",\"38\":\"44\",\"39\":\"672\",\"40\":\"0\",\"41\":\"1\",\"42\":\"0\",\"43\":\"B1\",\"44\":\"S\",\"45\":\"1\",\"46\":\"1144.24\",\"47\":\"0\",\"48\":\"0\"},{\"1\":\"6.7\",\"2\":\"1\",\"3\":\"2813.45\",\"4\":\"0\",\"5\":\"2\",\"6\":\"0\",\"7\":\"0\",\"8\":\"0\",\"9\":\"0\",\"10\":\"5\",\"11\":\"1\",\"12\":\"2716.55\",\"13\":\"0\",\"14\":\"0.00\",\"15\":\"0\",\"16\":\"0\",\"17\":\"0\",\"18\":\"0\",\"19\":\"0\",\"20\":\"0\",\"21\":\"0\",\"22\":\"0\",\"23\":\"0\",\"24\":\"0\",\"25\":\"0\",\"26\":\"0\",\"27\":\"0\",\"28\":\"0\",\"29\":\"0\",\"30\":\"0\",\"31\":\"0.00\",\"32\":\"0\",\"33\":\"0\",\"34\":\"25\",\"35\":\"1\",\"36\":\"6.0\",\"37\":\"101\",\"38\":\"46\",\"39\":\"648\",\"40\":\"0\",\"41\":\"1\",\"42\":\"1\",\"43\":\"B1\",\"44\":\"S\",\"45\":\"2\",\"46\":\"1208.94\",\"47\":\"0\",\"48\":\"0\"},{\"1\":\"12.3\",\"2\":\"1\",\"3\":\"1069.78\",\"4\":\"0\",\"5\":\"13\",\"6\":\"1\",\"7\":\"0\",\"8\":\"0\",\"9\":\"2\",\"10\":\"9\",\"11\":\"0\",\"12\":\"0.00\",\"13\":\"0\",\"14\":\"0.00\",\"15\":\"0\",\"16\":\"0\",\"17\":\"0\",\"18\":\"0\",\"19\":\"0\",\"20\":\"0\",\"21\":\"0\",\"22\":\"0\",\"23\":\"0\",\"24\":\"0\",\"25\":\"0\",\"26\":\"0\",\"27\":\"0\",\"28\":\"0\",\"29\":\"0\",\"30\":\"1\",\"31\":\"38.62\",\"32\":\"0\",\"33\":\"0\",\"34\":\"19\",\"35\":\"0\",\"36\":\"3.0\",\"37\":\"107\",\"38\":\"55\",\"39\":\"662\",\"40\":\"0\",\"41\":\"1\",\"42\":\"1\",\"43\":\"B7\",\"44\":\"U\",\"45\":\"5\",\"46\":\"6813.58\",\"47\":\"0\",\"48\":\"0\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\r(data_types \u0026lt;- data %\u0026gt;%\rmap_dfc(class) %\u0026gt;%\rgather(key = \u0026quot;Variable\u0026quot;, value = \u0026quot;Type\u0026quot;))\r{\"columns\":[{\"label\":[\"Variable\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"Type\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"AcctAge\",\"2\":\"numeric\"},{\"1\":\"DDA\",\"2\":\"numeric\"},{\"1\":\"DDABal\",\"2\":\"numeric\"},{\"1\":\"CashBk\",\"2\":\"numeric\"},{\"1\":\"Checks\",\"2\":\"numeric\"},{\"1\":\"DirDep\",\"2\":\"numeric\"},{\"1\":\"NSF\",\"2\":\"numeric\"},{\"1\":\"NSFAmt\",\"2\":\"numeric\"},{\"1\":\"Phone\",\"2\":\"numeric\"},{\"1\":\"Teller\",\"2\":\"numeric\"},{\"1\":\"Sav\",\"2\":\"numeric\"},{\"1\":\"SavBal\",\"2\":\"numeric\"},{\"1\":\"ATM\",\"2\":\"numeric\"},{\"1\":\"ATMAmt\",\"2\":\"numeric\"},{\"1\":\"POS\",\"2\":\"numeric\"},{\"1\":\"POSAmt\",\"2\":\"numeric\"},{\"1\":\"CD\",\"2\":\"numeric\"},{\"1\":\"CDBal\",\"2\":\"numeric\"},{\"1\":\"IRA\",\"2\":\"numeric\"},{\"1\":\"IRABal\",\"2\":\"numeric\"},{\"1\":\"LOC\",\"2\":\"numeric\"},{\"1\":\"LOCBal\",\"2\":\"numeric\"},{\"1\":\"ILS\",\"2\":\"numeric\"},{\"1\":\"ILSBal\",\"2\":\"numeric\"},{\"1\":\"MM\",\"2\":\"numeric\"},{\"1\":\"MMBal\",\"2\":\"numeric\"},{\"1\":\"MMCred\",\"2\":\"numeric\"},{\"1\":\"MTG\",\"2\":\"numeric\"},{\"1\":\"MTGBal\",\"2\":\"numeric\"},{\"1\":\"CC\",\"2\":\"numeric\"},{\"1\":\"CCBal\",\"2\":\"numeric\"},{\"1\":\"CCPurc\",\"2\":\"numeric\"},{\"1\":\"SDB\",\"2\":\"numeric\"},{\"1\":\"Income\",\"2\":\"numeric\"},{\"1\":\"HMOwn\",\"2\":\"numeric\"},{\"1\":\"LORes\",\"2\":\"numeric\"},{\"1\":\"HMVal\",\"2\":\"numeric\"},{\"1\":\"Age\",\"2\":\"numeric\"},{\"1\":\"CRScore\",\"2\":\"numeric\"},{\"1\":\"Moved\",\"2\":\"numeric\"},{\"1\":\"InArea\",\"2\":\"numeric\"},{\"1\":\"Ins\",\"2\":\"numeric\"},{\"1\":\"Branch\",\"2\":\"character\"},{\"1\":\"Res\",\"2\":\"character\"},{\"1\":\"Dep\",\"2\":\"numeric\"},{\"1\":\"DepAmt\",\"2\":\"numeric\"},{\"1\":\"Inv\",\"2\":\"numeric\"},{\"1\":\"InvBal\",\"2\":\"numeric\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\rdata_types %\u0026gt;%\rcount(Type)\r{\"columns\":[{\"label\":[\"Type\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"n\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"character\",\"2\":\"2\"},{\"1\":\"numeric\",\"2\":\"46\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\rWe can notice that 2 out of 48 Variables are strings and all the rest are Numeric. This is not necessary correct because some of the variables could be factors. Having Metadata will be super useful right know.\n\r2 Redefining Categorical Variables\rAfter taking a look at the data and the Metadata (that I can¬¥t find now, but I promise I will upload) all the Variables listed next are not correctly numbers but Factors:\n#Listing all of the Categorical Variables according to Metadata\rcategorical \u0026lt;- c(\u0026quot;ATM\u0026quot;, \u0026quot;Branch\u0026quot;, \u0026quot;CC\u0026quot;, \u0026quot;CD\u0026quot;, \u0026quot;DDA\u0026quot;, \u0026quot;DirDep\u0026quot;, \u0026quot;HMOwn\u0026quot;, \u0026quot;ILS\u0026quot;, \u0026quot;IRA\u0026quot;, \u0026quot;InArea\u0026quot;, \u0026quot;Ins\u0026quot;, \u0026quot;Inv\u0026quot;, \u0026quot;LOC\u0026quot;, \u0026quot;MM\u0026quot;, \u0026quot;MTG\u0026quot;, \u0026quot;Moved\u0026quot;, \u0026quot;NSF\u0026quot;, \u0026quot;Res\u0026quot;, \u0026quot;SDB\u0026quot;, \u0026quot;Sav\u0026quot;)\rWe can quickly transform this into factors by using dplyr, with no need to even loop.\n#Transforming to Factor (Categorical Data Type in R)\rdata \u0026lt;- data %\u0026gt;%\rmutate_at(vars(categorical), as_factor)\rThe excellent package forcats offers really easy functions to recode the numbers 1 and 0 into ‚Äúyes‚Äù and ‚Äúno‚Äù.\n#Factor variables will be relabeled for better intepretation of the data\rdata \u0026lt;- data %\u0026gt;%\rmutate_if(is.factor, ~ fct_recode(. , yes = \u0026#39;1\u0026#39;, no = \u0026#39;0\u0026#39;)) \r## Warning: Unknown levels in `f`: 1, 0\r## Warning: Unknown levels in `f`: 1, 0\rdata \u0026lt;- data %\u0026gt;%\rmutate_at(\u0026quot;Res\u0026quot;, ~fct_recode(\r. ,\rrural = \u0026#39;R\u0026#39;,\rsuburb = \u0026#39;S\u0026#39;,\rurban = \u0026#39;U\u0026#39;\r))\rThe ‚ÄúIns‚Äù Variable is the response variable and by using forcats we can shift the order of the Event Variable correctly.\n#Defining Yes as the Event/Positive Category.\rdata$Ins \u0026lt;- data$Ins %\u0026gt;% fct_shift\r\r3 Discovering Missing Values\rdata %\u0026gt;%\rsummarize_all(funs(. %\u0026gt;% is.na %\u0026gt;% sum)) %\u0026gt;%\rmap_df( ~ .x * 100 / nrow(data)) %\u0026gt;%\rgather(key = \u0026quot;Variable\u0026quot;, value = \u0026quot;percent_NAs\u0026quot;) %\u0026gt;%\rarrange(desc(percent_NAs)) %\u0026gt;%\rfilter(percent_NAs \u0026gt; 0)\r## Warning: funs() is soft deprecated as of dplyr 0.8.0\r## Please use a list of either functions or lambdas: ## ## # Simple named list: ## list(mean = mean, median = median)\r## ## # Auto named with `tibble::lst()`: ## tibble::lst(mean, median)\r## ## # Using lambdas\r## list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\r## This warning is displayed once per session.\r{\"columns\":[{\"label\":[\"Variable\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"percent_NAs\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"Age\",\"2\":\"19.703075\"},{\"1\":\"Income\",\"2\":\"17.920903\"},{\"1\":\"LORes\",\"2\":\"17.920903\"},{\"1\":\"HMVal\",\"2\":\"17.920903\"},{\"1\":\"HMOwn\",\"2\":\"17.149145\"},{\"1\":\"Phone\",\"2\":\"12.809943\"},{\"1\":\"POS\",\"2\":\"12.809943\"},{\"1\":\"POSAmt\",\"2\":\"12.809943\"},{\"1\":\"CC\",\"2\":\"12.809943\"},{\"1\":\"CCBal\",\"2\":\"12.809943\"},{\"1\":\"CCPurc\",\"2\":\"12.809943\"},{\"1\":\"Inv\",\"2\":\"12.809943\"},{\"1\":\"InvBal\",\"2\":\"12.809943\"},{\"1\":\"AcctAge\",\"2\":\"6.415819\"},{\"1\":\"CRScore\",\"2\":\"2.191297\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\rWe can show this results in a fancy way with the following code:\n#Counting if columns have any NAs in them\rdata_NA \u0026lt;- data %\u0026gt;%\rmap_dfr(anyNA) %\u0026gt;%\rgather(key = \u0026quot;Variable\u0026quot;, value = \u0026quot;any_NA\u0026quot;) %\u0026gt;%\rfilter(any_NA == TRUE)\r#This hunk was run before to obtain the atual data types of every column\rdata_types \u0026lt;- data %\u0026gt;%\rmap_dfc(class) %\u0026gt;%\rgather(key = \u0026quot;Variable\u0026quot;, value = \u0026quot;Type\u0026quot;)\r#This Chunk counts the actual Number of NAs n_NA \u0026lt;- data %\u0026gt;%\rsummarize_all(funs(. %\u0026gt;%\ris.na %\u0026gt;%\rsum)) %\u0026gt;%\rgather(key = \u0026quot;Variable\u0026quot;, value = \u0026quot;n_NA\u0026quot;)\r# All the previous results are joined into a summary Table\rdata_NA %\u0026gt;%\rleft_join(data_types, by = \u0026quot;Variable\u0026quot;) %\u0026gt;%\rleft_join(n_NA, by = \u0026quot;Variable\u0026quot;) %\u0026gt;%\rarrange(Type)\r{\"columns\":[{\"label\":[\"Variable\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"any_NA\"],\"name\":[2],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"Type\"],\"name\":[3],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"n_NA\"],\"name\":[4],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"CC\",\"2\":\"TRUE\",\"3\":\"factor\",\"4\":\"4133\"},{\"1\":\"HMOwn\",\"2\":\"TRUE\",\"3\":\"factor\",\"4\":\"5533\"},{\"1\":\"Inv\",\"2\":\"TRUE\",\"3\":\"factor\",\"4\":\"4133\"},{\"1\":\"AcctAge\",\"2\":\"TRUE\",\"3\":\"numeric\",\"4\":\"2070\"},{\"1\":\"Phone\",\"2\":\"TRUE\",\"3\":\"numeric\",\"4\":\"4133\"},{\"1\":\"POS\",\"2\":\"TRUE\",\"3\":\"numeric\",\"4\":\"4133\"},{\"1\":\"POSAmt\",\"2\":\"TRUE\",\"3\":\"numeric\",\"4\":\"4133\"},{\"1\":\"CCBal\",\"2\":\"TRUE\",\"3\":\"numeric\",\"4\":\"4133\"},{\"1\":\"CCPurc\",\"2\":\"TRUE\",\"3\":\"numeric\",\"4\":\"4133\"},{\"1\":\"Income\",\"2\":\"TRUE\",\"3\":\"numeric\",\"4\":\"5782\"},{\"1\":\"LORes\",\"2\":\"TRUE\",\"3\":\"numeric\",\"4\":\"5782\"},{\"1\":\"HMVal\",\"2\":\"TRUE\",\"3\":\"numeric\",\"4\":\"5782\"},{\"1\":\"Age\",\"2\":\"TRUE\",\"3\":\"numeric\",\"4\":\"6357\"},{\"1\":\"CRScore\",\"2\":\"TRUE\",\"3\":\"numeric\",\"4\":\"707\"},{\"1\":\"InvBal\",\"2\":\"TRUE\",\"3\":\"numeric\",\"4\":\"4133\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\rIt can be seen that:\n\r15 Variables have Missing Values. T\rThe range of Missing values varies from 2.19 % to 19.7 %.\n\r3 out of 15 are Categorical Values whereas the rest are Numeric Variables.\r\rDuring the Diploma a 2% threshold for Missing Values was discussed. Imputation was not recommended if Missing Values are greater than that. So in order to simplify the problem we will just get rid of NAs. The tidyr package does this really easily.\n\rSidenote: I¬¥m not completely sure about this criterion. I will be asking about this during LatinR_2019.\n\r#Droping observations with missing Values\rdata \u0026lt;- data %\u0026gt;% drop_na()\r#Showing distribution of records of th Target Variable\rdata %\u0026gt;% count(Ins)\r{\"columns\":[{\"label\":[\"Ins\"],\"name\":[1],\"type\":[\"fctr\"],\"align\":[\"left\"]},{\"label\":[\"n\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"yes\",\"2\":\"7504\"},{\"1\":\"no\",\"2\":\"13373\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\r\r4 Conclusion\rSo far we have been able to import a SAS dataset and apply a high level cleansing to organize the data, discover factor variables, reorganize the event Variable correctly and get rid of NAs.\ndata %\u0026gt;% glimpse\r## Observations: 20,877\r## Variables: 48\r## $ AcctAge \u0026lt;dbl\u0026gt; 0.3, 0.7, 4.1, 0.5, 6.7, 12.3, 8.8, 9.3, 0.9, 3.0, 4.8...\r## $ DDA \u0026lt;fct\u0026gt; yes, yes, no, yes, yes, yes, yes, yes, yes, yes, yes, ...\r## $ DDABal \u0026lt;dbl\u0026gt; 419.27, 1986.81, 0.00, 1594.84, 2813.45, 1069.78, 1437...\r## $ CashBk \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\r## $ Checks \u0026lt;dbl\u0026gt; 0, 1, 0, 1, 2, 13, 12, 2, 4, 1, 0, 0, 5, 4, 9, 8, 0, 2...\r## $ DirDep \u0026lt;fct\u0026gt; no, yes, no, no, no, yes, yes, yes, no, yes, no, no, n...\r## $ NSF \u0026lt;fct\u0026gt; no, no, no, no, no, no, no, no, no, no, no, no, no, ye...\r## $ NSFAmt \u0026lt;dbl\u0026gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, ...\r## $ Phone \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, ...\r## $ Teller \u0026lt;dbl\u0026gt; 0, 0, 0, 1, 5, 9, 0, 0, 2, 1, 0, 0, 3, 2, 1, 1, 0, 2, ...\r## $ Sav \u0026lt;fct\u0026gt; yes, no, no, yes, yes, no, no, yes, yes, no, no, yes, ...\r## $ SavBal \u0026lt;dbl\u0026gt; 10233.72, 0.00, 0.00, 425.06, 2716.55, 0.00, 0.00, 967...\r## $ ATM \u0026lt;fct\u0026gt; yes, yes, no, yes, no, no, yes, yes, yes, no, no, no, ...\r## $ ATMAmt \u0026lt;dbl\u0026gt; 106.74, 268.88, 0.00, 278.07, 0.00, 0.00, 391.63, 276....\r## $ POS \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 5, 0, 0, 0, 0, ...\r## $ POSAmt \u0026lt;dbl\u0026gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 23.13,...\r## $ CD \u0026lt;fct\u0026gt; no, no, no, no, no, no, no, no, no, no, no, no, no, no...\r## $ CDBal \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\r## $ IRA \u0026lt;fct\u0026gt; no, no, no, no, no, no, no, no, no, no, no, no, no, no...\r## $ IRABal \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\r## $ LOC \u0026lt;fct\u0026gt; no, no, no, no, no, no, no, no, no, no, no, no, no, no...\r## $ LOCBal \u0026lt;dbl\u0026gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, ...\r## $ ILS \u0026lt;fct\u0026gt; no, no, no, no, no, no, no, no, no, no, no, no, no, no...\r## $ ILSBal \u0026lt;dbl\u0026gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, ...\r## $ MM \u0026lt;fct\u0026gt; no, no, no, no, no, no, no, no, no, no, no, no, no, no...\r## $ MMBal \u0026lt;dbl\u0026gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, ...\r## $ MMCred \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, ...\r## $ MTG \u0026lt;fct\u0026gt; no, no, no, no, no, no, yes, no, no, no, no, no, no, n...\r## $ MTGBal \u0026lt;dbl\u0026gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 94539.95, 0.00, 0....\r## $ CC \u0026lt;fct\u0026gt; yes, yes, yes, yes, no, yes, yes, yes, no, no, yes, ye...\r## $ CCBal \u0026lt;dbl\u0026gt; 483.65, 0.00, 0.00, 65.76, 0.00, 38.62, 85202.99, 0.00...\r## $ CCPurc \u0026lt;dbl\u0026gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...\r## $ SDB \u0026lt;fct\u0026gt; no, no, no, no, no, no, no, no, yes, no, no, no, no, n...\r## $ Income \u0026lt;dbl\u0026gt; 16, 4, 30, 125, 25, 19, 55, 13, 54, 25, 100, 13, 7, 9,...\r## $ HMOwn \u0026lt;fct\u0026gt; yes, yes, yes, yes, yes, no, yes, no, no, yes, yes, ye...\r## $ LORes \u0026lt;dbl\u0026gt; 11.0, 7.0, 8.5, 7.5, 6.0, 3.0, 3.5, 4.5, 4.0, 7.5, 13....\r## $ HMVal \u0026lt;dbl\u0026gt; 89, 87, 97, 145, 101, 107, 128, 99, 129, 95, 135, 77, ...\r## $ Age \u0026lt;dbl\u0026gt; 63, 51, 60, 44, 46, 55, 57, 58, 73, 29, 75, 51, 49, 39...\r## $ CRScore \u0026lt;dbl\u0026gt; 696, 674, 640, 672, 648, 662, 659, 675, 667, 612, 715,...\r## $ Moved \u0026lt;fct\u0026gt; no, no, no, no, no, no, no, no, no, no, no, no, no, no...\r## $ InArea \u0026lt;fct\u0026gt; yes, yes, yes, yes, yes, yes, yes, yes, yes, yes, yes,...\r## $ Ins \u0026lt;fct\u0026gt; yes, no, yes, no, yes, yes, no, yes, yes, no, no, no, ...\r## $ Branch \u0026lt;fct\u0026gt; B17, B2, B3, B1, B1, B7, B1, B5, B6, B4, B9, B7, B7, B...\r## $ Res \u0026lt;fct\u0026gt; rural, rural, suburb, suburb, suburb, urban, urban, ur...\r## $ Dep \u0026lt;dbl\u0026gt; 2, 1, 0, 1, 2, 5, 2, 3, 2, 2, 0, 1, 3, 5, 2, 4, 1, 1, ...\r## $ DepAmt \u0026lt;dbl\u0026gt; 1170.06, 446.93, 0.00, 1144.24, 1208.94, 6813.58, 2237...\r## $ Inv \u0026lt;fct\u0026gt; no, no, no, no, no, no, no, no, no, no, no, no, no, no...\r## $ InvBal \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\rMore to come on this problem. Stay Tuned!!!\n\r","date":1563494400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569387730,"objectID":"b464c47dc9ba5b22e55e25317c7cdd54","permalink":"/project/machine-learning-diploma/","publishdate":"2019-07-19T00:00:00Z","relpermalink":"/project/machine-learning-diploma/","section":"project","summary":"This is will be some kind of tutorial of the different Packages I used to perform a Machine Learning Project for my diploma. This first Part will be focused on Importing Data and a high level Data Harmonization.","tags":["Machine Learning","Data Import (haven)","Data Cleaning"],"title":"My Final Project at the ML Diploma (Part I)","type":"project"},{"authors":["Alfonso Tobar"],"categories":["Basics"],"content":"\r\rRedes Convolucionales\r¬øC√≥mo se relaciona esto con las Redes Convolucionales?\r¬øC√≥mo se va especializando la red para ser cada vez m√°s detallista?\r¬øC√≥mo se relaciona este problema con Estructuras?\r\rEl Paper de Finol\rSoluci√≥n\rPuntos Interesantes para la Investigaci√≥n.\r1era Sugerencia (Quiz√°s no para la Memoria, pero para seguir investigando)\r2da Sugerencia\r3era Sugerencia.\r4ta Sugerencia ‚ÄúCrear Im√°genes Estructurales‚Äù\r\r\r\r\rRedes Convolucionales\rLas redes convolucionales es un tipo de Red que esta especializada principalmente en extraer caracter√≠sticas de im√°genes. Si bien es cierto es posible utilizar Redes convolucionales para otras aplicaciones, el an√°lisis de im√°genes y la Visi√≥n Computacional son la especialidad de estas redes.\nLa Principal ventaja sobre una Red neural normal es que no est√° densamente conectada. Sino que selecciona cuidadosamente las neuronas a las que se conectar√° cambiando la operaci√≥n matricial de Producto Punto por una convoluci√≥n.\nLa definici√≥n formal de Convoluci√≥n es:\n\\[ (f * g)(x)= \\int_{-\\infty}^{\\infty}{f(\\eta)\\cdot g(x-\\eta) d\\eta} \\]\rLa interpretaci√≥n m√°s cl√°sica de esta integral corresponde al producto de funciones al desplazar una por sobre la otra.\n¬øC√≥mo se relaciona esto con las Redes Convolucionales?\rEl an√°lisis de Im√°genes es bastante complicado. La manera en la que un computador es capaz de ver una im√°gen es parseando e interpretandola por un Tensor de Pixeles. Dependiendo de la resoluci√≥n dela Imagen este Tensor ser√° cada vez m√°s grande, por lo tanto usar Redes Densas supone un gran costo computacional por todos los $ w_{i,j}$ que ser√° necesario calcular.\nNormalmente el an√°lisis de Im√°genes utiliza tensor de este estilo:\n\r\rFigure 1: Estructura de Tensores de Im√°genes\r\r\rLa Matriz de Pixeles de Dimensiones Alto x Ancho m√°s una Pofundidad de Canales RGB que dan el color.\nEs por esto que las redes convolucionales ocupan esta operaci√≥n para reconocer patrones espec√≠ficos dentro del Tensor de Imagen. La manera en que est√° convoluci√≥n se lleva a cabo es por medio de un Filtro. Este filtro es otro Tensor que posee generalmente dimensiones de 3x3 o 5x5 el cual se desplaza a trav√©s del tensor de im√°genes para construir un Mapa de Caracter√≠sticas.\n\r\r\r\rFigure 2: Aplicaci√≥n de un Filtro en redes Convolucionales\r\r\rNormalmente luego de la Aplicaci√≥n del Filtro el Mapa de Caracter√≠sticas es pasado por una ReLU lo que genera que se realcen ciertos atributos de la imagen lo que permite que la red aprenda a diferenciar caracter√≠sticas espec√≠ficas.\nDependiendo del filtro aplicado, se ver√°n ciertos atributos de la imagen o no. Cabe destacar que los filtros parten como n√∫meros aleatorios que se van entrenando en los distintos Pasos de la red (Forward and Backward).\nSi intentamos analizar la foto de un gato, luego de aplicar filtros, esto es lo que un computador comienza a ver:\n\r\rFigure 3: Feature Map\r\r\rFigure 4: Resultado del Entrenamiento de Filtros\r\r\r\r¬øC√≥mo se va especializando la red para ser cada vez m√°s detallista?\rLuego de la red Convolucional hay una reducci√≥n de Dimensionalidad, para llevarse acabo se utiliza una Capa de pooling, esta puede ser una Average Pooling o Max Pooling (generalmente √©sta √∫ltima es la m√°s utilizada) la cual reduce la dimensi√≥n del problema para luego aplicar una nueva Capa de Red convolucional pero con m√°s filtros que el paso anterior.\r\r\r\r\rFigure 5: Max Pooling\r\r\rUna vez que se ha llegado al nivel de espacializaci√≥n deseado, se llevan todas las caracter√≠cticas a un vector que es pasado por una capa densamente conectada para entregar los outputs correspondientes.\nUna Configuraci√≥n t√≠pica podr√≠a verse as√≠:\n\r\rFigure 6: Especializaci√≥n de la Red\r\r\r\r¬øC√≥mo se relaciona este problema con Estructuras?\rNo tiene relaci√≥n alguna. Porque este tipo de redes fue dise√±ada para el desarrollo del an√°lisis de im√°genes. Adem√°s es la manera m√°s intuitiva de analizar el funcionamiento de estas redes.\rLas dos principales caracer√≠sticas de las Redes Convolucionales son:\n\rLa extracci√≥n de Caracter√≠sticas\rTranslation/Shift Invariance\r\rEsto quiere decir que la red se especializa en detectar Patrones que no dependen de su posici√≥n ni que deben ser identicos para ser detectados. Por ejemplo una oreja de gato se detectara si es grande, si es peque√±a, si es de otro color si esta rotada o desplazada.\nSon estas caracter√≠sticas las que permitir√≠an desarrollar otro tipo de problemas si es que pueden ser planteadas como un tensor de im√°gen o similar.\n\r\rEl Paper de Finol\rEl Paper propone solucionar un problema cualquiera de estructuras usando redes Convolucionales. Lo que ellos primeramente plantean es la obtenci√≥n de Valores propios para un material de propiedades variables.\nPara ello se utiliza una barra dividida en 100 Elementos en las que se miden dos Propiedades, el M√≥dulo de Elasticidad E y la Densidad $ $ .\nEn simple lo que se est√° planteando es una red convolucional para analizar una Imagen de 1 Pixel de Alto por 100 de Ancho, donde los Canales que representan color, en este caso representan propiedades f√≠sicas del Elemento. Dada las propiedades de la imagen esto puede extrapolarse a que las Redes convolucionales pueden analizar Data Secuencial, en este caso cada Elemento de la Barra est√°n en secuencia debido a la continuidad del la barra. Aunque cabe destacar que la red Convolucional no se deja llevar por el orden ya que sus patrones no dependen de la posici√≥n.\nMi Supuesto:\nEsto podr√≠a traducirse en que los invesigadores pensaron en quiz√°s un elemento equivalente de un Modulo $ E_{eq} $ en serie dado que es una barra unidimensional y el orden de los $ E_{i} $ y $ _{i} $ no influyen en la propiedad equivalente final del elemento.\nAdem√°s el problema que se est√° resolviendo es de valores propios, no hay elementos externos que indiquen que el orden de las propiedades mec√°nicas influyan en el c√°lculo final.\nBajo estas caracter√≠sticas es que el uso de la Red Convolucional es v√°lida.\nSoluci√≥n\rPara resolver el problema de Valores propios para un Cristal dividido en 100 Elementos se utiliza la siguiente configuraci√≥n:\n\r\rFigure 7: Soluci√≥n de Finol\r\r\r\rSe utiliza una Red Secuencial aplicando un Filtro de Largo 3.\rAl no utilizar padding el vector se reduce a 98 x 1 x 275 donde 275 corresponde te√≥ricamente al n√∫mero de filtros que est√° utilizando. Esto no sale definido en el Paper y es algo que averigu√© investigando por mi parte.\rSe utiliza una segunda capa convolucional. La raz√≥n de esto no lo s√©. Normalmente no he visto casos de dos Capas convolucionales seguidas excepto en modelos muy avanzados.\rSe utiliza una capa Max Pooling para reducir de largo 2 que disminuye el tama√±o de la red a la mitad.\rLuego de esto se une con una Red Convencional densamente conectada. Lo que normalmente he visto es que se usa una flatten Layer que aplana la reducci√≥n de dimensiones que se ha dado hasta ahora. El valor esperado deber√≠a ser de 275 x 48, pero no s√© porque ocupa 500 nodos.\r*Se utilizan 3 capas ocultas para luego generar una capa de 20 unidades de salida por los 20 valores propios esperados para el problema.\r\r\rPuntos Interesantes para la Investigaci√≥n.\r\rEstoy tomando un curso en Datacamp y uno p√∫blico de Stanford acerca del uso de redes Convolucionales. Dentro del curso se dan varias recomendaciones:\n\rPara un problema de im√°genes de 64 x 64 x 3 para predecir si hay un gato en la im√°gen o no se requieren aproximadamente 10.000 im√°genes.\n\rSe recomienda usar la m√≠nima resoluci√≥n posible para distinguir a nivel humano de tal manera de bajar el poder computacional que requiere la red.\n\rA mayor resoluci√≥n m√°s im√°genes. Menor Resoluci√≥n menos Im√°genes.\n\r\rCualquier Matriz puede ser interpretada como una im√°gen. En la cual una Red Convolucional puede encontrar patrones. Por lo que perfectamente la matriz de rigidez puede ser tratado como una imagen.\n\r\r\r\rFigure 8: Transformaci√≥n de Matriz en Im√°gen\r\r\r\rLas redes convolucionales poseen propiedades de Memoria dado que su principal funci√≥n es la Extracci√≥n de Caracter√≠sticas. Existen modelos pre-entrenados de redes que Pueden usarse para perfeccionar otras tareas.\n\rUn ejemplo que le√≠ fue una red que fue pre-entrenada con im√°genes de muebles pero que luego se adapt√≥ para mejor la identificaci√≥n de animales. La red a pesar de no ser entrenada con animales, era capaz de reconocer bordes y caracter√≠sticas muy finas.\r\r\r1era Sugerencia (Quiz√°s no para la Memoria, pero para seguir investigando)\rUso de Redes convolucionales para calcular Inversas de la Matriz de Rigidez. Siendo este el proceso m√°s caro de la resoluci√≥n de estructuras quiz√°s podr√≠a realizarse el desarrollo de una Red Pre-entrenada que sea capaz de calcular Inversas de Muchas Matrices de tal Manera de agilizar el proceso m√°s caro de la resoluci√≥n de estructuras.\n\r2da Sugerencia\rSi bien las redes Convolucionales pueden dar buenos resultados para data secuencial, dada su caracteristica de ‚ÄúInvariante a la Traslaci√≥n‚Äù, puede encontrar patrones incorrectos cuando el orden s√≠ importa. Se desaconseja su uso en series de tiempo, ya que el orden de la informaci√≥n importa al momento de predecir.\rPara contrarestar esto, se agrega una capa de redes recurrentes (RNN) que ayuda a establecer la secuencia como un prerequisito del an√°lisis.\n\rEs posible realizar modelos paralelos en la que se ingrese data distinta como input de la Red. Por ejemplo la Matriz de Rigidez del problema como ‚Äúimagen‚Äù y los Propiedades Mec√°nicas como vector o hasta las Cargas a las que est√° sometida la estructura como vector, de esa manera todos los elementos son parte de los Input del problema para predecir de manera m√°s acertada los desplazamientos normales o umbrales del problema.\r\r\r3era Sugerencia.\rGenerar una red no secuencial considerando todos los inputs del problema.\n\rDado que el an√°lisis de las Im√°genes poseen una caracter√≠stica Espacial y secuencial me da la impresi√≥n que se puede trabajar en optimizaciones al proceso de Condensaci√≥n de Grados de Libertad. Ser√≠a posible que la red redujera problemas con gran cantidad de grados de libertad en Rigideces equivalentes que pueden ser planteadas como una im√°gen.\n\rAs√≠ mismo dado que tambi√©n posee propiedades de Profundidad podr√≠a ayudar al desarrollo de analisis de estructuras en 3 dimensiones que es algo que yo no recuerdo haber visto si no era usando algo como ETABS o SAP.\n\rMe da la impresi√≥n de que Redes Convolucionales pueden ayudar al desarrollo de Propiedades equivalentes usando su caracter√≠stica secuencial para propiedades en serie y su profundidad como propiedades en paralelo.\n\r\r\r4ta Sugerencia ‚ÄúCrear Im√°genes Estructurales‚Äù\rUtilizar la estructura de la matriz de Rigidez con 3 Canales simulando el RGB: Rigidez, Cargas en Eje X y Cargas en Eje Y.\rHabr√≠a que buscar una manera de modelar dentro de la matriz los apoyos, que normalmente no son considerados dentro de la Matriz a priori.\nPara un Enrejado de 9 Elementos esto ser√≠a una Im√°gen de 9 x 9 x 3, lo cual no supondr√≠a una gran cantidad de casos para entrenar.\n\r\r\r","date":1554940800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569362496,"objectID":"7465b3b7e0f8a8e6deade092f6b5568a","permalink":"/project/convolutional-nets-sp/memoria-1/","publishdate":"2019-04-11T00:00:00Z","relpermalink":"/project/convolutional-nets-sp/memoria-1/","section":"project","summary":"Propuestas para Uso de CNN en estructuras.","tags":["Deep Learning","Convolutional Networks"],"title":"Propuesta Redes Convolucionales","type":"project"},{"authors":["Alfonso Tobar"],"categories":["Basics"],"content":"\r\rActivation Functions\rIdentity\rStep Function\rLinear Function\rSigmoid Function\rHyperbolic Tangent\rReLU\rLeaky ReLU\rSoftmax\r\rHow to choose the perfect Activation function?\r\r\rActivation Functions\rActivation functions are one of the most important characteristic of ANN. They basically decide whether a neuron should be activated or not. When a particular threshold is reached the Neuron will fire, meaning they will transmit the input signal to the next layer of the Network. Another Important feature of Activation Functions is that some of them provide the non-linearity. This is particular important because Activation functions help expand the range of problems that the Neural Networks can address. Finally Activation functions will play a major role when optimizing the edges weights when Backpropagation Algorithm comes into play, depending of their derivatives values is how the Gradient will change helping to decrease the error associated the Network prediction.\nSome of these Activation Functions are:\nIdentity\rIt is the most basic Activation, basically, do not alter the Neuron at all. The problem with this type of activation function is that is linear, transforming the Network into a Linear Regression limiting its classification capabilities for non-linear phenomena.\n\rStep Function\rThe binary function is extremely simple. It returns 1 if certain threshold is reached or 0 otherwise. The main drawback of this function is that his derivative is 0, meaning it is not useful in the optimizing process.\nThe function is defined as follows:\n\\[ f(x)= \\left\\{ \\begin{array}{lcc}\r0 \u0026amp; if \u0026amp; x \u0026lt; 0 \\\\\r\\\\ 1 \u0026amp; if \u0026amp; x \\geq 0 \\end{array}\r\\right. \\]\n\r\rFigure 1: Step Function\r\r\r\rLinear Function\rThis is another option, being the main difference the existence of a slope. In this case the derivative will be constant, which can be problematic because when trying to decrease the error no matter how right or off you are the gradient will be the same.\nThe function goes as follows:\n\\[ f(x) = a x \\]\n\r\rFigure 2: Linear Function\r\r\r\rSigmoid Function\rThis is a very popular activation function. The main advantages of this function is that it is smooth, S-shaped, it is continuously differentiable and non-linear.\nThe function is defined as follows:\n\\[ f(x)= \\frac{1}{1+e^{-x}} \\]\n\r\rFigure 3: Sigmoid Function\r\r\rThe derivative of this function is always positive and greater than 0 and x-dependent so it is very helpful when optimizing.\nOne of the setbacks is that only ranges from 0 to 1, for one thing is very limiting with the output but for the other it is particularly useful when dealing with probabilities.\n\rHyperbolic Tangent\rHyperbolic Tangent or $ tanh(x) $ is just an scaled version of the Sigmoid function. It is defined as follows:\n\\[ tanh(x)= 2 \\cdot sigmoid(2x) - 1 = \\frac{2}{1+e^{-2x}} - 1 \\]\r\r\rFigure 4: Hyperbolic Tangent Function\r\r\r$ tanh $ works similarly to sigmoid but it is symmetric at the x - axis. Normally $ tanh $ and sigmoid can be used interchangeably depending on the requirements of the problem.\n\rReLU\rReLu stands for Rectified Linear unit and it is defined as follows:\n\\[ f(x)= max(0,x) \\]\r\r\rFigure 5: ReLU Function\r\r\rIt is the most used function nowadays in hidden layers. The main capability is that it doesn‚Äôt activate all of the functions creating sparsity in the network, allowing efficiency in computation.\nThis function is limited at the positive side so it is not suggested for Output Layers. Another drawback is that not activated neurons in the range $ x \u0026lt; 0 $ will not be optimized since derivative is zero.\n\rLeaky ReLU\rThis is an improved version of the ReLU function. It is not widely used yet and it has a subtle difference with ReLu: \\[ f(x)= \\left\\{ \\begin{array}{lcc}\rax \u0026amp; if \u0026amp; x \u0026lt; 0 \\\\\r\\\\ x \u0026amp; if \u0026amp; x \\geq 0 \\end{array}\r\\right. \\]\n\r\rFigure 6: Leaky ReLU Function\r\r\rThis solves the problem of dead neurons during Optimization process, since the derivative of $ x \u0026lt; 0 $ is not zero.\n\rSoftmax\rThis is a sigmoid kind-of function capable of handling more than 2 classes. The function is defined as follows:\n\\[ \\sigma(z)_{j}=\\frac{e^{z_{j}}}{\\sum_{k=1}^{K}e^{z_{k}}} \\]\nThe softmax functions are normally used in output layers when trying to solve classification problems with more than 2 classes..\n\r\rHow to choose the perfect Activation function?\rWell, there is not a clear answer to this, but definitely some guidelines we can follow:\n\rSigmoid functions generally work better in classification problems.\rSigmoids and tanh functions are sometimes avoided due to the vanishing gradient problem.\rReLU function is a general activation function and is used in most cases these days.\rIf we encounter a case of dead neurons in our networks the leaky ReLU function is the best choice.\rAlways keep in mind that ReLU function should only be used in the hidden layers.\rAs a rule of thumb, you can begin with using ReLU function and then move over to other activation functions in case ReLU doesn‚Äôt provide with optimum results.\r\rDeep Learning with R provides some other Guidelines to use Activation Functions in the Output Layer:\n\rBinary Classification: Sigmoid\rMulticlass Single-Label Classification: Softmax\rMulticlass Multi-Label Classification: Sigmoid\rRegression to Arbitrary Values: Identity or None\rRegression to Values between 0 to 1: Sigmoid\r\r\r","date":1554681600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554783338,"objectID":"7d22714745bd3ed36a07d7ead5033e85","permalink":"/project/activation-functions/activation-functions/","publishdate":"2019-04-08T00:00:00Z","relpermalink":"/project/activation-functions/activation-functions/","section":"project","summary":"Definitions of the Most Common ACtivations Functions.","tags":["Deep Learning"],"title":"Activation Functions","type":"project"},{"authors":["Alfonso Tobar"],"categories":["Basics"],"content":"\r\rWhat is an Artificial Neural Network?\rBasic Structure\rWhat Problems can Neural Networks solve?\rClassification\rRegression\r\rTypical Problems solved with Neural Networks\r\rConclusion\r\r\rWhat is an Artificial Neural Network?\rThe Picture you see up there is far a way from what a real Neural Network looks like:\nActually it is more similar to something like this:\n\r\rFigure 1: Multi-Layer Perceptron\r\r\rThe configuration showed above is nothing but a visual representation of serial Matrix Multiplications. The neural network (aka ANN that stands for Artificial Neural Networks) itself is not an algorithm, but rather a framework for many different machine learning algorithms to work together and process complex data inputs1.\nANN are inspired by Neurons but they work in a completely different fashion. The main advantage of this system is that organizes successive Matrix multiplications and provides a visual representation of the different steps in the Optimization algorithm.\nThere are different types of Networks, Densely Connected, Convolutional, Recurrent, Long Short Memory, Radial Biased, Autoencoders, and so on. All of them having their own area of specialization, strengths and shortcomings.\nBasic Structure\rEvery Network will contain Nodes/Units, emulating Neurons and Edges, emulating the connection between Units.\nNormally the Units are organized by Layers, every Network should contain a first layer for Inputs, a last Layer for Outputs and Intermediate Layers, also known as Hidden Layers.\n\r\rFigure 2: Basic Neural Network\r\r\r\rInput Nodes $ i_{j} $ will contain Input Values to train the Network.\n\rEdges will provide weights $ w_{j} $.\n\rHidden Layer Nodes $ h_{j} $ will contain the result of Sum Product between Input Nodes and weighted edges connected to them.\n\rFinally Output Nodes $ o_{j} $ will contain the result of Sum Product between hidden Nodes and the Edges Connected to them.\n\rOptionally, Networks can include a Bias $ b_{j} $ to control values of the network.\n\r\rThe Network then will calculate values in the following way:\n\\[ h_{1} = w_{1} i_{1} + w_{2} i_{2} + b_{1} = 0.15 \\cdot 0.05 + 0.25 \\cdot 0.10 + 0.35 = 0.3825 \\]\n\\[ h_{2} = w_{2} i_{1} + w_{4} i_{2} + b_{1} = 0.20 \\cdot 0.05 + 0.30 \\cdot 0.10 + 0.35 = 0.39 \\]\n\\[ o_{1} = w_{5} h_{1} + w_{7} h_{2} + b_{2} = 0.40 \\cdot 0.3825 + 0.50 \\cdot 0.39 + 0.60 = 0.948 \\]\n\\[ o_{2} = w_{6} h_{1} + w_{8} h_{2} + b_{2} = 0.45 \\cdot 0.3825 + 0.55 \\cdot 0.39 + 0.60 = 0.986625 \\]\nThis is called a forward pass. All the Input Values were able to move through the Network by the Edge Connections up to the Output. Normally, when the Network is trained, there are expected Output values that will be compared with the ones obtained with the Forward Pass in order to compute the Error. In this case 0.01 and 0.99 respectively.\n\rWhat Problems can Neural Networks solve?\rNormally there are two Problems that Neural Networks Solves, Classification and Regression.\nClassification\rThe Classification problem is the most common problem addressed by Neural Networks. It implies to classify based on a Probability. The output will calculate how likely is that an specific label corresponds to a class. Classification problems can be sub-divided into other sub-types:\n\rBinary Classification: As the name implies, it involves two classes: Spam or not Span, Positive or Negative, Man or Woman, etc.\rMulticlass Classification: In this case several labels can be applied: Is it a Dog, Cat, Horse? What Car Brand is that? etc.\r\r\rRegression\rThis kind of problems involved calculate a number associated to a Metric. Typical Problems are predicting House Values, Temperature, Balances, Displacements, etc.\n\r\rTypical Problems solved with Neural Networks\rNeural Networks are powerful and they are the most cutting-edge methodology to make computers do the most incredibly/creepy things.\nEven though we think computers can do anything like Analyzing Photos, Driving Cars, Recognizing Animals, Predicting Prices and so on, the scope of their work is completely limited to just one thing: Computing Tensors.\nTensors are the generalization to N dimensions of Matrices. Basically any problem that can be represented by Tensors is something that Neural Networks could potentially solve.\nDifferent kind of Tensors can solve specific problems. Here some examples taken from Deep Learning with R:\n\rVector data‚Äî2D tensors of shape (samples, features): This is the Most common Data Structure Data Scientist uses in a daily basis. Basically a Matrix, having Features as Columns and Samples as Rows.\n\rTimeseries data or sequence data‚Äî3D tensors of shape (samples, timesteps, features): This is something a little bit fancier, having several timeseries organized as a collection of matrices, this creates a 3D Tensor.\n\r\r\r\rFigure 3: Multiple Timeseries data\r\r\r\rImages‚Äî4D tensors of shape (samples, height, width, channels) or (samples, channels, height, width): Images are represented as Pixel Matrices, Every Pixel also has RGB Channels giving the color properties to it, thus a 3D Tensor. Adding several samples of Images to analize and you have a 4D Tensor.\r\r\r\rFigure 4: Image Data\r\r\r\rVideo‚Äî5D tensors of shape (samples, frames, height, width, channels) or (samples, frames, channels, height, width): Videos are nothing but a collection of Sequencial Images. So You‚Äôll have different Samples of Sequencial Images producing which is a 4D Tensor, since you analize several samples of Videos, you‚Äôll get a 5D Tensor.\r\r\r\rConclusion\rNeural Networks are simple visual representations of Tensor Calcultions that are capable of addressing different real life problems. So far we have covered how the Networks transmit Information from Input to Output also called Forward Pass, but there are some other concepts that are necessary to understan in order to fully unerstand how to properly train a Neural Network.\n\r\rWikipedia: https://en.wikipedia.org/wiki/Artificial_neural_network‚Ü©\n\r\r\r","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554783338,"objectID":"5ab0020c69035456ab833ae99bb5d554","permalink":"/project/neural-nets-101/intro-to-nn/","publishdate":"2019-04-07T00:00:00Z","relpermalink":"/project/neural-nets-101/intro-to-nn/","section":"project","summary":"Short Summary of What Neural Networks are","tags":["Deep Learning"],"title":"Intro to Neural Networks","type":"project"},{"authors":["Alfonso Tobar"],"categories":["Vampire Diaries"],"content":" Deep Learning Well, I got to know Deep Learning by chance. I remember to have had a College subject called Operational Investigation Fundamentals and they taugth Artifical Intelligence algorithms. Neural networks were mentioned but I never thought I could even understand what they were about. During my first real job (as a quasi-Engineer) I always thougth I never paid sufficient attention to that subject. The subject basically covered some optimization models and for some reason AI was there, just sky-high level mentions in some classes.\nThen I remember during my Data Science Program, we had 3 different Research Projects (this was at my Butterflies and Unicorns time, if you don\u0026rsquo;t know what I\u0026rsquo;m talking, go here). One was Data Analytics to Measure Cars Price (super-duper boring), the second was Using Sentiment Analysis to Understand whether Lyrics of Top 20 Billboard were related to Decade\u0026rsquo;s Most Important Facts (I did this, and it was super interesting) and there was a third one I really tried to sneak out: Sentiment Annalysis with Machine Learning. Machine Learning sounded really scary to be my very first Data Science Project, so I put it off.\nIt turns out that a colleague just joined my team at EVS and he had this awesome Max Kuhn book: Applied Predictive Modeling. And I thought: \u0026ldquo;I\u0026rsquo;m more experienced now, so probably I can take a look at this\u0026rdquo;. And well I discovered caret and completely blew my mind (But I still dislike its little to null compatibility with Tidyverse, but thanks Max Kuhn for creating parsnip).\nThen Nico, I think, was the first guy mentioning Deep Learning, because he created the XoR problem in VBA and also took Andrew Ng specialization Course, and I started to feel interested in the Field.\nWell, I dicovered Keras, the R API just came out (I think so) and I got this other super Book: Deep Learning with R and I just learned so much, but not enough. I think I learned a bit of Keras but I suddenly realized I had no idea about the inner black-box, so I just wanted to learn and understand what the hell is happening inside that box.\nWell here I am\u0026hellip; Trying to understand\u0026hellip; and I didn\u0026rsquo;t do very well. Internet has little to no information about theory. Codes are everywhere, but why that code is useful is just not important for users.\nSo my intent is creating content (R based obviously, because R is not popular for Deep Learning either) for me to learn, and hopefully spread the word about this.\nWell, the thing is I need to finish my Thesis, and my Professor told me: \u0026ldquo;Why don\u0026rsquo;t you combine what you know about Deep Learning (sincerely, not too much) with Structure Engineering? And you know what, this is something really unexplored in the field, so here I go.\nHopefully publishing my findings help to have a better understanding of what I\u0026rsquo;m doing, also i can keep track of it and help others on my way (I love teaching, so this is a good way to get started).\nHope to have news about my Thesis soon\u0026hellip;\n","date":1553647800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554521808,"objectID":"2046ae8bd00899bda7e50b0ef72f0071","permalink":"/post/deep-learning/","publishdate":"2019-03-27T00:50:00Z","relpermalink":"/post/deep-learning/","section":"post","summary":"How I ran into DL?","tags":["Thesis","Deep Learning"],"title":"Deep Learning","type":"post"},{"authors":["Alfonso Tobar"],"categories":["Blog"],"content":" Deep Learning Well, I got to know Deep Learning by chance. I remember to have had a College subject called Operation Investigation Fundamentals and they taugth Artifical Intelligence algorithms. Neural networks were mentioned but I never thought I could even understand what they were about. During my first real job (as an engineer) I always thougth I never paid sufficient attention to that subject. The subject basically covered some optimization models and for some reason AI was there, just sky-high level mentions in some classes.\nThen I remember during my Data Science Program, we had 3 different Research Projects (this was at my Butterflies and Unicorns time, if you don\u0026rsquo;t know what I\u0026rsquo;m talking, go here). One was Data Analytics to Measure Cars Price (super-duper boring), the second was Using Sentiment Analysis to Understand if the Topics of Top 20 Billboard were related to Decade\u0026rsquo;s Most Important Facts (I did this, and it was super interesting) and there was a third one I really tried to sneak out: Sentiment Annalysis with Machine Learning. Machine Learning sounded really scary to be my very first Data Science Project, so I put it off.\nIt turns out that a colleague just joined my team at EVS and he had this awesome Max Kuhn book: Applied Predictive Modeling. And I thought: \u0026ldquo;I\u0026rsquo;m more experienced now, so probably I can take a look at this\u0026rdquo;. And well I discovered caret and completely blew my mind (But I still dislike its little to null compatibility with Tidyverse, thanks Max Kuhn for creating Parsnip).\nThen Nico, I think, was the first guy mentioning Deep Learning, because he created the XoR problem in VBA and also took Andrew Ng specialization, and I started to feel interested in the Field.\nWell, I dicovered Keras, the R API just came out (I think so) and I got this other super Book: Deep Learning with R and I just learned so much, but not enough. I think I learned a bit of Keras but suddenly realized I had no idea about the inner black-box, so I just wanted to learn and understand what the hell is happening inside that box.\nWell here I am\u0026hellip; Trying to understand\u0026hellip; and I didn\u0026rsquo;t do very well, The Internet has little to no information about theory. Codes are everywhere, but why that code is useful is just not important for users.\nSo my intent is creating content (R based obviously, because R is not popular for Deep Learning either) for me to learn, and hopefully spread the word about this.\nWell, the thing is I need to finish my Thesis, and my Professor told me why don\u0026rsquo;t we combine what you know about Deep Learning (sincerely, not too much) with Structure Engineering, and you know what this is something really unexplored in the field, so here I go. Hopefully publishing my findings help to have a better understanding of what I\u0026rsquo;m doing, also i can keep track of it, and I can help others (I love teaching, so this is a good way to get started).\nHope to have news about my Thesis soon\u0026hellip;\n","date":1553647800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554521808,"objectID":"476c6249b9ba536df97e2f07196cc1e9","permalink":"/publication/deep-learning/","publishdate":"2019-03-27T00:50:00Z","relpermalink":"/publication/deep-learning/","section":"publication","summary":"How I ran into DL?","tags":["Thesis"],"title":"Deep Learning","type":"publication"},{"authors":["Alfonso Tobar"],"categories":["Vampire Diaries"],"content":" Why am I not graduated yet? Graduation is something we all are longing. Well, I recognize this is something I\u0026rsquo;ve been evading. The main reason is because I\u0026rsquo;m not working on what I studied. But, what the hell, I think I like Data Science.\nOk, Long Story Short\u0026hellip; I was about to study Mathematical Engineering but I asked myself, where can I possibly work as a Mathematical Engineer ? (In my current job, obviously üòÖ)\nSo I decide to go for Civil Engineering because it has a decent amount of advanced Maths that was something I was looking for and a High Employment Index.\nWell I started working as a Civil Engineer in INVAR as soon as I finished my internship. I just loved it, I learned so much, but the thingwas that they didn\u0026rsquo;t pay too much. So I decided to take a position at Aguas del Valle. Sincerely, and very respectfully, it has been the worst choice of my life, not only because it was 5 hours away of my Hometown, but also because they took my soul out of my body and make me spend awful moments doing absolutely nothing. There is nothing more frustrating than doing nothing all day long in a desk in a Company you don\u0026rsquo;t feel a part of.\nSo the Evalueserve Post showed up. A friend referred me to the Data Science Program (DSP) and then I fell in love with Data Science. During the program I learned SQL, VBA, SAS üëç, R, Tableau and definitely built up my English and Communication skills. Felipe, help me prepare my first interview in English and thanks to him I had the English I needed to learn all I know so far.\nUpskilling and moving through cities took all of my time, besides I got married (Best Decision so far) but a lot to do, hence, no time to finish my Thesis.\nWhat happened with my Thesis? Well in INVAR I worked as an Hidraulics Engineer so I started a Thesis back in 2013 about Water Hammer. Really interesting topic, that needed Finite Differences Methods to solve really complicated Partial Differential Equations. The only proffesor with this knowledge started working wih me, but after a year he got retired. So I had to find another Professor, so in order to avoid this issue to happen again, I decided to go with one of the youngest Professor, well he left me for his Phd.\nDamn, Suddenly working in INVAR I noticed the CEO was my professor of Planning so I ask him to mentor me. This happened just before leaving to Aguas del Valle in La Serena. 6 months later a Cancer was diagnosed and he passed away, so freaking fast. The Chief of Department took over all of the abandoned Thesis that my Professor\u0026rsquo;s dead left behind, but he couldn\u0026rsquo;t continue to mentor me, because he considered being in La Serena was too far away. In 2015 I came back to Vi√±a and I started to learn, upskill, teach, develop as a Data Scientist that I decided to put it on hold. On 2017 I started another Thesis. Another Professor wanted to mentor me but he was of the Structural Area. That meant I needed to move to that Area. We started investigating uncertainty on Structural Analysis. The thing is my work again didn\u0026rsquo;t give me any chance to continue. Until now. Why now? Probably I will explain it later, because it is also related with the creation of this site.\nWhat My Thesis is going to be about? Good news, I will be able to mix my previous work with one thing I discovered as I developed as Data Science: Deep Learning in R üëè.\nI will be posting some progress about that. Hang tight!!\n","date":1553647800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554521808,"objectID":"fc07de7a276a157101a10808a9efd24b","permalink":"/post/my-thesis/","publishdate":"2019-03-27T00:50:00Z","relpermalink":"/post/my-thesis/","section":"post","summary":"Where, What and Why is ~~Gamora~~ my Thesis About?","tags":["Thesis"],"title":"My Thesis Project","type":"post"},{"authors":["Alfonso Tobar"],"categories":["Vampire Diaries"],"content":" Why am I not graduated yet? Graduation is something we all are longing. Well, I recognize this is something I\u0026rsquo;ve been evading. The main reason is because I\u0026rsquo;m not working on what I studied. But, what the hell, I think I like Data Science.\nOk, Long Story Short\u0026hellip; I was about to study Mathematical Engineering but I asked myself, where can I possibly work as a Mathematical Engineer ? (In my current job, obviously üòÖ)\nSo I decide to go for Civil Engineering because it has a decent amount of advanced Maths that was something I was looking for and a High Employment Index.\nWell I started working as a Civil Engineer in INVAR as soon as I finished my internship. I just loved it, I learned so much, but the thingwas that they didn\u0026rsquo;t pay too much. So I decided to take a position at Aguas del Valle. Sincerely, and very respectfully, it has been the worst choice of my life, not only because it was 5 hours away of my Hometown, but also because they took my soul out of my body and make me spend awful moments doing absolutely nothing. There is nothing more frustrating than doing nothing all day long in a desk in a Company you don\u0026rsquo;t feel a part of.\nSo the Evalueserve Post showed up. A friend referred me to the Data Science Program (DSP) and then I fell in love with Data Science. During the program I learned SQL, VBA, SAS üëç, R, Tableau and definitely built up my English and Communication skills. Felipe, help me prepare my first interview in English and thanks to him I had the English I needed to learn all I know so far.\nUpskilling and moving through cities took all of my time, besides I got married (Best Decision so far) but a lot to do, hence, no time to finish my Thesis.\nWhat happened with my Thesis? Well in INVAR I worked as an Hidraulics Engineer so I started a Thesis back in 2013 about Water Hammer. Really interesting topic, that needed Finite Differences Methods to solve really complicated Partial Differential Equations. The only proffesor with this knowledge started working wih me, but after a year he got retired. So I had to find another Professor, so in order to avoid this issue to happen again, I decided to go with one of the youngest Professor, well he left me for his Phd.\nDamn, Suddenly working in INVAR I noticed the CEO was my professor of Planning so I ask him to mentor me. This happened just before leaving to Aguas del Valle in La Serena. 6 months later a Cancer was diagnosed and he passed away, so freaking fast. The Chief of Department took over all of the abandoned Thesis that my Professor\u0026rsquo;s dead left behind, but he couldn\u0026rsquo;t continue to mentor me, because he considered being in La Serena was too far away. In 2015 I came back to Vi√±a and I started to learn, upskill, teach, develop as a Data Scientist that I decided to put it on hold. On 2017 I started another Thesis. Another Professor wanted to mentor me but he was of the Structural Area. That meant I needed to move to that Area. We started investigating uncertainty on Structural Analysis. The thing is my work again didn\u0026rsquo;t give me any chance to continue. Until now. Why now? Probably I will explain it later, because it is also related with the creation of this site.\nWhat My Thesis is going to be about? Good news, I will be able to mix my previous work with one thing I discovered as I developed as Data Science: Deep Learning in R üëè.\nI will be posting some progress about that. Hang tight!!\n","date":1553647800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554521808,"objectID":"340c92f3fd37efeff818f6906d6a59a5","permalink":"/publication/my-thesis/","publishdate":"2019-03-27T00:50:00Z","relpermalink":"/publication/my-thesis/","section":"publication","summary":"Where, What and Why is ~~Gamora~~ my Thesis About?","tags":["Thesis"],"title":"My Thesis Project","type":"publication"},{"authors":null,"categories":null,"content":"","date":1551409200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553554159,"objectID":"d7c99b5f0ba2ef066af2121e72286554","permalink":"/bio/","publishdate":"2019-03-01T00:00:00-03:00","relpermalink":"/bio/","section":"","summary":"This is me!","tags":null,"title":"Bio","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1551409200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551409200,"objectID":"10b23fb9a98faf23942f651c808b6081","permalink":"/contact/","publishdate":"2019-03-01T00:00:00-03:00","relpermalink":"/contact/","section":"","summary":"This is me!","tags":null,"title":"Contact","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1551409200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551409200,"objectID":"6087c0ef875554f4409ac52928d79279","permalink":"/projects/","publishdate":"2019-03-01T00:00:00-03:00","relpermalink":"/projects/","section":"","summary":"Some Projets","tags":null,"title":"Projects","type":"widget_page"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\ngetwd()  ## [1] \u0026quot;D:/Alfonso/website/content/tutorial\u0026quot;  Tip 1 \u0026hellip;\nTip 2 \u0026hellip;\nbla bla ","date":1536462000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553562400,"objectID":"2e59fda17a04e7a8318c6191daa0102b","permalink":"/tutorial/example/","publishdate":"2018-09-09T00:00:00-03:00","relpermalink":"/tutorial/example/","section":"tutorial","summary":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\ngetwd()  ## [1] \u0026quot;D:/Alfonso/website/content/tutorial\u0026quot;  Tip 1 \u0026hellip;\nTip 2 \u0026hellip;\nbla bla ","tags":null,"title":"Example Page","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\ngetwd()  ## [1] \u0026quot;E:/Alfonso/website/content/tutorial\u0026quot;  Tip 1 \u0026hellip;\nTip 2 \u0026hellip;\nbla bla ","date":1536462000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536462000,"objectID":"c40d38224c2b476cbfc1f032fbb6a78e","permalink":"/tutorial/copyofexample/","publishdate":"2018-09-09T00:00:00-03:00","relpermalink":"/tutorial/copyofexample/","section":"tutorial","summary":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\ngetwd()  ## [1] \u0026quot;E:/Alfonso/website/content/tutorial\u0026quot;  Tip 1 \u0026hellip;\nTip 2 \u0026hellip;\nbla bla ","tags":null,"title":"Example Page","type":"docs"},{"authors":null,"categories":null,"content":"My blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License.\n  \n","date":1530140400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553404091,"objectID":"18d05a63a1c8d7ed973cc51838494e41","permalink":"/privacy/","publishdate":"2018-06-28T00:00:00+01:00","relpermalink":"/privacy/","section":"","summary":"My blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License.","tags":null,"title":"Licensed under Creative Commons Attribution-ShareAlike 4.0 International","type":"page"},{"authors":["GA Cushen"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1441076400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553404091,"objectID":"d77fa4a74076ffcd7ca6c21cfc27a4b2","permalink":"/publication/person-re-id/","publishdate":"2015-09-01T00:00:00-03:00","relpermalink":"/publication/person-re-id/","section":"publication","summary":"Person re-identification is a critical security task for recognizing a person across spatially disjoint sensors. Previous work can be computationally intensive and is mainly based on low-level cues extracted from RGB data and implemented on a PC for a fixed sensor network (such as traditional CCTV). We present a practical and efficient framework for mobile devices (such as smart phones and robots) where high-level semantic soft biometrics are extracted from RGB and depth data. By combining these cues, our approach attempts to provide robustness to noise, illumination, and minor variations in clothing. This mobile approach may be particularly useful for the identification of persons in areas ill-served by fixed sensors or for tasks where the sensor position and direction need to dynamically adapt to a target. Results on the BIWI dataset are preliminary but encouraging. Further evaluation and demonstration of the system will be available on our website.","tags":[],"title":"A Person Re-Identification System For Mobile Devices","type":"publication"},{"authors":["GA Cushen","MS Nixon"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1372651200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553661585,"objectID":"2b4d919e3cf73dfcd0063c88fe01cb00","permalink":"/publication/clothing-search/","publishdate":"2013-07-01T00:00:00-04:00","relpermalink":"/publication/clothing-search/","section":"publication","summary":"A mobile visual clothing search system is presented whereby a smart phone user can either choose a social networking image or capture a new photo of a person wearing clothing of interest and search for similar clothing in a large cloud-based ecommerce database. The phone's GPS location is used to re-rank results by retail store location, to inform the user of local stores where similar clothing items can be tried on.","tags":[],"title":"Mobile visual clothing search","type":"publication"},{"authors":null,"categories":null,"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553404091,"objectID":"c2915ec5da95791851caafdcba9664af","permalink":"/slides/example-slides/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example-slides/","section":"slides","summary":"Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$","tags":null,"title":"Slides","type":"slides"}]