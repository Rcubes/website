[{"authors":["admin"],"categories":null,"content":"I\u0026rsquo;m almost a Civil Engineer that for well-unknown reasons ended up working as a Data Scientist prior to finish my Thesis Project (more to come on this) and it seems I enjoy it.\nI love solving the Rubiks\u0026rsquo;s Cube when analizing data in R, that should be enough insight to figure out why the site name.\nI worked at Evalueserve during 4 years on some interesting and a lot of not-so-interesting Data Science projects. I started getting bored since R started to be banned from Production Processes (probably I\u0026rsquo;ll write about this later) so I decided to come up with this site to showcase the Power of R and some use-cases in my Thesis Project\nThe purpose of this site is to honor Yihui Xie\u0026rsquo;s quote: \u0026ldquo;I web, therefore I am Spiderman\u0026ldquo; (but I have to say I feel more identified with Tony Stark, not because I\u0026rsquo;m a Genius, Billionaire, Playboy nor Philanthropist, but I like his creativity\u0026hellip;and his Armor) and of course learn together about how R can achieve really cool Data Science Stuffs. I feel I\u0026rsquo;m a pretty advanced useR, I\u0026rsquo;ve been using R since I joined EVS and the Tidyverse just changed my life.\nIn my spare time I like to solve the Rubik\u0026rsquo;s Cube (of course!! 😛), I average around 13 Seconds (by 2019-03) and play some Table Tennis . I\u0026rsquo;m a Big Fan of the Marvel Cinematic Universe (I haven\u0026rsquo;t read any comics, I just like the Movies) and I really enjoyed Avengers: Endgame.\nMy native Language is Spanish, so please forgive me for any Language aberration I can posibly write in the near future (I\u0026rsquo;m planning to have a Spanish Version too, but I\u0026rsquo;m still learning Blogdown). I\u0026rsquo;m happily married with Valentina since 2015 and J.A.R.V.I.S Jesus is my copilot.\n","date":1553647277,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1553647277,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":"I\u0026rsquo;m almost a Civil Engineer that for well-unknown reasons ended up working as a Data Scientist prior to finish my Thesis Project (more to come on this) and it seems I enjoy it.\nI love solving the Rubiks\u0026rsquo;s Cube when analizing data in R, that should be enough insight to figure out why the site name.\nI worked at Evalueserve during 4 years on some interesting and a lot of not-so-interesting Data Science projects.","tags":null,"title":"Alfonso Tobar","type":"author"},{"authors":null,"categories":null,"content":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.main]] menu links to it in the config.toml.\n","date":1536462000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1553404091,"objectID":"c3224f3a64174f08aaf31e1f1d16ffd3","permalink":"/tutorial/","publishdate":"2018-09-09T00:00:00-03:00","relpermalink":"/tutorial/","section":"tutorial","summary":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.","tags":null,"title":"Overview","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.\n  Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using url_slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906563600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553404091,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00-03:00","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":["Alfonso Tobar"],"categories":["Vapire Diaries","Rambling"],"content":"\rLatin R\rLatin R is a conference organize by RLadies Latam and is the oportunity we have here to know the last breakthrough and how is being using in Research, companies, etc.\rThey also provide a full day of Tutorials and of course important keynotes are invited to come over. This year I think is huge 3 Main R users are coming:\nMine Çetinkaya-Rundel\rShe is one of the most important persons involved into R Teaching. I personally had the chance to take some Coursera and Datacamp Courses with her and I learned a lot. Having her now here in hile to meet her in person is suh a great Honor. I just can´t wait to hear about what she has to say.\rShe will be conducting a Tutorial on how to Teach R and of Course a main Speech.\n\rErin Ledell\rI have to say I don’t know a lot about her but she is the Chief Machine Learning Scientist at H2O and that is more than enough. She will be conducting a Tutorial on Machine Learning and Deep Learning that of course I´ll be taking so I´ll give more details on later post.\n\rHadley Wickham\rThis Guy needs no Introduction. He is the Chief Scientist at RStudio and he will be giving the Main Speech of the Conference plus a Package Development Tutorial. Of ourse I will be in first row of this Tutorial and I expect to learn a lot and have a lot of questions prepared beforehand.\nThis guys is by far my most important inspiration when it comes to program in R. Definitely this guys has made my life easier beause of all of the pakages that he has created and I use almost everyday.\nI’ll be the next 3 days in the Conference and I expect to share some pictures and experiences about the things I will learned.\nStay tuned!!!\n\r\r","date":1569283200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569283200,"objectID":"8e89b01640dd805b3b195ff53f61bdb0","permalink":"/post/latin-r/","publishdate":"2019-09-24T00:00:00Z","relpermalink":"/post/latin-r/","section":"post","summary":"Hadley is oming to Chile and I´ll be taking some Package Development Classes with him","tags":["R","Conference"],"title":"Latin R","type":"post"},{"authors":["Alfonso Tobar"],"categories":["Tutorial"],"content":"\r\rThe Method\rThe Problem\r\r\r\rThe Method\rThe Rayleigh Ritz Method is nothing but applying Finite Elements to Structural problems. Basically you split your structure into smaller structures that can easily be solved By solving, I mean, Calculate the specific stifness of the Structure in order to determine how the loads affects the structure. Once the individual mini-strutures are solved they are ensembled into a Merged Matrix equivalent to the total Stiffness of the Structure.\nThe purpose of this Document is not get into deep details about the Method. If you want to learn about this you can go to this paper to learn the Maths behind this. The idea is to show how to implement this in R. Since this is a computational expensive method I’ll be using library(Rcpp).\nThe Problem\r\r\rFigure 1: Problem Structure\r\r\r\r\r","date":1569283200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569283200,"objectID":"b464c47dc9ba5b22e55e25317c7cdd54","permalink":"/project/machine-learning-diploma/","publishdate":"2019-09-24T00:00:00Z","relpermalink":"/project/machine-learning-diploma/","section":"project","summary":"This is will be some kind of tutorial of the different Packages I used to perform a Machine Learning Project","tags":["Logistic Regression","Machine Learning"],"title":"My Final Project at the ML Diploma","type":"project"},{"authors":["Alfonso Tobar"],"categories":null,"content":"Coming up with an Interesting Thesis Project is not easy at all. Actually I had 3 different projets and 5 different professors. None of them were really interested in my propositions. Thank God I found Dr. Marcos Valdebenito. He is really interested in Reliability Analysis in Structures and Study the Response of Random Field Variables into Strutures, you can learn more about his work on his website. Once I talked to him about I was doing in my former job he was really interested in applying Machine Learning techniques to solve this kind of problems.\nNormally to study the effect of Ramdom fields in Structures a Montecarlo Simulation is run several times to determine how the Structure response is affected. This is done by analyzing the mean and the Covariance of the simulations. This process is omputationally expensive since normally 10,000 to 1,000,000 simulations are needed. Every one of those simulations solves the following problem, also called the Rayleigh - Ritz Method:\n$$ [K] {u} = {f}$$\nWhere $ [K] $ is the Finite Element Matrix representing the Equivalent Stiffness of the different Degrees of Fredom of the Structure. $ {f} $ is the Equivalent Load Vector representing the forces affecting the Structure. In order to solve this problem $ [K]^{-1} $ needs to be pre-multiplied with $ {f}$ to obtain the Struture Response $ {u} $ representing the Structure displacement at every Degree of freedom. Normally $ [K] $ is a fairly large Matrix and the Inversion process costly so alternative methods to Montecarlo Simulation are deeply appreciated.\nSo that is how we came up with a Project. What about considering $ [K] $ as a black and white image (1 channel), representing the stiffness of a Truss. Therefore, Convolutional Networks could be a good alternative to analyze the Matrix and train a Network capable of determinimg in a first instance the displacement of the Structure (${u}$) and afterwards the failure of the Structure, transforming the problem into a Clasification Binary Problem (Failing - not Failing).\nI\u0026rsquo;ll be posting more technical content about how I\u0026rsquo;ve been tackling the problem. Stay tuned!!!\n","date":1569207600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553662289,"objectID":"ddc06374e9bc1835608820cda1843546","permalink":"/project/my-thesis/","publishdate":"2019-09-23T00:00:00-03:00","relpermalink":"/project/my-thesis/","section":"project","summary":"This is What my Thesis Project will be about.","tags":["Deep Learning","Finite Elements","Rayleigh - Ritz"],"title":"My Thesis","type":"project"},{"authors":["Alfonso Tobar"],"categories":["Basics"],"content":"\r\rThe Method\rThe Problem\rRcpp Basics\rCreating an Rcpp file\rStiffness Method\rStiff Matrix by Element\rActive DoF Assembly\rConnectivity Array\rStiffness Matrix Assembly\rLoad Vector Assembly\rSolving the Problem\r\rConclusions\r\r\r\rThe Method\rThe Rayleigh Ritz Method is nothing but applying Finite Elements to Structural problems. Basically you split your structure into smaller structures that can easily be solved By solving, I mean, Calculate the specific stifness of the Structure in order to determine how the loads affects the structure. Once the individual mini-strutures are solved they are ensembled into a Merged Matrix equivalent to the total Stiffness of the Structure.\nThe purpose of this Document is not get into deep details about the Method. If you want to learn about this you can go to this paper to learn the Maths behind this. The idea is to show how to implement this in R. Since this is a computational expensive method I’ll be using library(Rcpp).\nThe Problem\r\r\rFigure 1: Problem Structure\r\r\rThis is a simple problem and useful to understand the different steps of the Method.\rThis is implementation is for a Truss with 3 Nodes and 3 Elements where:\n#Number of Nodes by Element\rNN_e \u0026lt;- 2\r#Number of Degrees of Freedom (DoF) by Node\rNgl_N \u0026lt;- 2\rL \u0026lt;- 1 #Value of L\rE \u0026lt;- 2 * 10 ^ 11 # Young Module / Elasticity Metric\rA \u0026lt;- 0.0001 # Cross Sectional Area\rP \u0026lt;- 1000 # Load\rThe First and most simple Step is to organize the Input Data. All of the Data will be input in tibble form.\nRow i of the Nodes Matrix will store the X and Y Coordinates for Every Node.\n(Nodes \u0026lt;-\rtibble::tribble(~ Xi, ~ Yi,\r0, 0,\rsqrt(2) / 2 * L, sqrt(2) / 2 * L,\rsqrt(2) * L, 0))\r## # A tibble: 3 x 2\r## Xi Yi\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 0 0 ## 2 0.707 0.707\r## 3 1.41 0\rThe Row j of the Elements Matrix will contain the Initial Node ni, the ending Node nf and the corresponding E and A properties for Element j. In this case all the Elements share the same properties.\n(Elements \u0026lt;-\rtibble::tribble(~ ni, ~ nf, ~ E, ~ A,\r1, 2, E, A,\r2, 3, E, A,\r3, 1, E, A))\r## # A tibble: 3 x 4\r## ni nf E A\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1 2 200000000000 0.0001\r## 2 2 3 200000000000 0.0001\r## 3 3 1 200000000000 0.0001\rThe Row i of the Loads Matrix contains the x and y vectorial component of the Loads for Node i.\n(Loads \u0026lt;-\rtibble::tribble(~ Px, ~ Py,\r0, 0,\r0, P,\r0, 0))\r## # A tibble: 3 x 2\r## Px Py\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 0 0\r## 2 0 1000\r## 3 0 0\rThe Row i correspond to the freedom of the X and Y Component of the Node i. 1 meaning no Movement and 0 meaning free movement.\n(Supports \u0026lt;-\rtibble::tribble(~ Rx, ~ Ry,\r1, 1,\r0, 0,\r0, 1))\r## # A tibble: 3 x 2\r## Rx Ry\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1 1\r## 2 0 0\r## 3 0 1\r\rRcpp Basics\rRcpp is the R API package to access to the huge benefits that C++ offers. I´m not an expert in C++ actually I just learned a bit of C++ because Rcpp offers easy sintax to access to C++ Elements but always showing equivalents in the R Environment.\nC++ is far for being an adequate language for Data Science, but once you want to optimize code or algorithms is definitely the way to go. In these case I´ll be showing the algorithm to the different steps of the Stiffness Method and how can be implemented in Rcpp.\nMy main sources to learn Rcpp were this excellent Rcpp for Everyone and of course Hadley´s Help. With these two resources you should have more than enough to create your first Rcpp functions.\n\rCreating an Rcpp file\r\r\rFigure 2: Create a C++ File\r\r\rIf you work with RStudio you can go to File \u0026gt; New File \u0026gt; C++ File and will open a C++ Template like this:\n\r\rFigure 3: C++ Template\r\r\rThe main thing you need to be aware of is loading the required libraries from C++. In this case we will use the following:\nAll C++ code chunks will be combined to the chunk below:\n// [[Rcpp::depends(RcppEigen)]]\r#include \u0026lt;Rcpp.h\u0026gt;\r#include \u0026lt;RcppEigen.h\u0026gt;\r#include \u0026lt;Eigen/LU\u0026gt; #include \u0026lt;Eigen/Eigenvalues\u0026gt; using namespace Rcpp;\rusing namespace Eigen;\rAs you may know C++ is a compiled language. Compilation means, in really simple words, to optimize and speed up the code making it available in R through functions. If you want functions to be available in the R environment they need to be preceeded by this special comment. Otherwise they can be called from within the C++ environment as intermediate functions but they won´t work in R.\n\rStiffness Method\rStiff Matrix by Element\rThis Step calculates Stiff for the mini-structures, meaning every single bar.\nEvery Element Matrix has the following form that needs to be created according to its properties.\n\\[\r[K]_j=\\begin{bmatrix}\rc^2 \u0026amp; \u0026amp; \u0026amp; sim\\\\\rcs \u0026amp; s^2 \u0026amp; \u0026amp; \\\\\r-c^2 \u0026amp; -cs \u0026amp; c^2 \u0026amp; \\\\\r-cs \u0026amp; -s^2 \u0026amp; cs \u0026amp; s^2 \\\\\r\\end{bmatrix}\r\\]\rThe pseudo code is as follows:\n\\[ Ne \\leftarrow \\text{Number of Rows in the Element Matrix} \\\\\rc \\leftarrow \\text{ Sparse Matrix for Director Cosines, Dimension Ne x 1 } \\\\\rs \\leftarrow \\text{ Sparse Matrix for Director Sinus, Dimension Ne x 1 } \\\\\rL \\leftarrow \\text{ Sparse Matrix for Element Length, Dimension Ne x 1 } \\\\\r\\text{for j = 1 to Ne do}\r\\left\\{ \\begin{array}{lcc}\rNi=Elements(j,1) \\\\ Nf=Elements(j,2) \\\\\r\\Delta x = Nodes(Nf,1) - Nodes(Ni,1) \\\\\r\\Delta y = Nodes(Nf,2) - Nodes(Ni,2) \\\\\rL(j)=\\sqrt{\\Delta x^2 + \\Delta y^2} \\\\\rc(j) = {\\Delta x\\over L(j)} \\\\\rs(j) = {\\Delta y\\over L(j)}\r\\end{array}\r\\right.\r\\]\nNow translating this into Rcpp looks like this:\n\rYou need define every object to use preceeded by its type.\rThe output will be an R List since I want object storing the different Element Matrix.\rAll of the Function arguments are Mandatory by default and need to go in the same order that will be used. If an Optional Argument is needed the default value needs to be defined as in NN_e.\r\r// [[Rcpp::export]]\r// First you define the Output Type. In this case an R List.\rList K_Element(NumericMatrix Nodes, NumericMatrix Elements, int NN_e = 2){\r// Ne is defined by using the nrow method to calculate number of rows.\rint Num_Elements = Elements.nrow();\r// c, s and L are defined Vectors since the second Dimension is 1.\rNumericVector c (Num_Elements);\rNumericVector s (Num_Elements);\rNumericVector L (Num_Elements);\rint j,Ni,Nf;\r// dx and dy are defined as doubles since they can contain decimals\rdouble dx,dy;\rList K_list (Num_Elements);\r// C++ is defined from 0 as the first element. So the pseudo code needs to be adjusted accordingly.\r// Notice the for syntax, from 0 to NE-1 defined as j\u0026lt;Num_Elementos and the ++j iterator\rfor(j=0;j\u0026lt;Num_Elements;++j){\rNi=Elements(j,0) -1;\rNf=Elements(j,1) - 1;\rdx=Nodes(Nf,0)-Nodes(Ni,0);\rdy=Nodes(Nf,1)-Nodes(Ni,1);\r//pow is the C++ operator for ^\rL[j]=sqrt(pow(dx,2)+pow(dy,2));\rc(j)=dx/L(j);\rs(j)=dy/L(j);\r// This is a special way to define a Matrix by Element coming from library(RcppEigen)\rMatrix4f ke;\rke \u0026lt;\u0026lt; pow(c[j],2),c[j]*s[j],-pow(c[j],2),-c[j]*s[j],\rc[j]*s[j],pow(s[j],2),-c[j]*s[j], -pow(s[j],2),\r-pow(c[j],2),-c[j]*s[j],pow(c[j],2),c[j]*s[j],\r-c[j]*s[j],-pow(s[j],2),c[j]*s[j],pow(s[j],2);\r//Here you populate every List Element with the corresponding Element Matrix\rK_list[j]= Elements(j,NN_e)*Elements(j,NN_e + 1)/L[j]*ke; }\rreturn K_list;\r}\r/*** R\r(K_E \u0026lt;- K_Element(Nodes,Elements))\r*/\r\rActive DoF Assembly\rThe Stiffness Method needs to determine what Dof are actually active, meaning that are free to move, hence are unknowns of the equation of the problem.\rIn order to do that it is necessary to determine which ones are free to move depending on the support Matrix and a Position Number is assigned to them.\nPseudocode as follows:\n\\[ Nn \\leftarrow \\text{Number of Rows in the Node Matrix} \\\\\rGl_act \\leftarrow \\text{ Sparse Matrix Dimension (NN \\cdot Ngl_N) x 1 } \\\\\rcont = 0 \\\\\r\\begin{aligned}\r\u0026amp; \\text{for i = 1 to Nn do } \\\\\r\u0026amp; \\text{for k = 1 to Ngl_N do} \\\\\r\\end{aligned} \\\\\r\\left\\{ \\begin{array}{lcc}\r\\text{if Apoyos(i,k) = 0 then} \\\\\rcont= cont +1 \\\\\rpos=Ngl_N \\cdot (i-1) + k \\\\\rGl_act(pos)=cont \\\\\r\\end{array}\r\\right.\r\\]\rRcpp Code:\n// [[Rcpp::export]] // Sparse Vector that uses Support Matrix as Input NumericVector Gr_Active(NumericMatrix Support, int Ngl_N = 2){\rint Num_Nodes = Support.nrow();\rint cont=0, i, k;\r//Defining Dimension of Gl Vector\rNumericVector Gl (Num_Nodes*Ngl_N);\rfor(i = 0; i \u0026lt; Num_Nodes; ++i){\rfor(k = 0; k \u0026lt; Ngl_N; ++k){\rif(Apoyos(i,k)==0){\r//Counter needs to be adapted since C++ starts off at Zero\rGl[Ngl_N*i+k] = ++cont;\r}\r} }\rreturn Gl;\r}\r\r\rConnectivity Array\rThe Method determines an array to identify how the different elements are connected each other. This way it is possible to create an equivalent Matrix representing the Equivalent Stiffness of the ensembled elements.\n\\[ Ngle = Ngl_N \\cdot NN_e \\\\\rconect \\leftarrow \\text{ Sparse Matrix Dimension Ne x Ngle } \\\\\r\\begin{aligned}\r\u0026amp; \\text{for j = 1 to Ne do } \\\\\r\u0026amp; \\text{for k = 1 to NN_e do} \\\\\r\u0026amp; N_k=Elementos(j,k) \\\\\r\u0026amp; pos1= (N_k - 1) \\cdot Ngl_N \\\\\r\\end{aligned} \\\\\r\\text{ for l= 1 to Ngl_N do } \\\\\r\\left\\{ \\begin{array}{lcc}\rpos2=pos1+l \\\\\rpos3= (k-1) \\cdot Ngl_N + l \\\\\rconect(j,pos3) = Gl_act(pos2) \\\\\r\\end{array}\r\\right.\r\\]\rRcpp Code:\n// [[Rcpp::export]]\r// This is a Numeric Matrix using Elements Matrix and Gl Vector as Input\rNumericMatrix Arr_Connect(NumericMatrix Elements, NumericVector Gl, int NN_e = 2, int Ngl_N = 2){\rint Num_Elements = Elements.nrow();\r// Several counters an be defined simultaneously if sharing the same properties.\rint j, k, l, pos1, pos2, pos3;\rNumericMatrix conect(Num_Elements, NN_e * Ngl_N);\rfor(j=0; j \u0026lt; Num_Elements; ++j){\rfor(k=0; k \u0026lt; NN_e; ++k){\rpos1 = (Elements(j,k) - 1) * Ngl_N;\rfor(l=0; l \u0026lt; Ngl_N; ++l){\rpos2 = pos1 + l;\r// pos3 had to be adjusted because C++ index starting at 0 pos3 = k * Ngl_N + l;\rconect(j,pos3) = Gl[pos2];\r}\r}\r}\rreturn conect;\r}\r\rStiffness Matrix Assembly\rOnce the Connectivity Array and the Active DoFs are determined the Global Stiffness Matrix can be assembled. This matrix contains the Contribution of every element to an specific Node. Less Elements joined to a specific Node will end up adding less stiffness than a lot of elements being part of a Node.\nPseudocode as follows:\n\\[ N_R \\leftarrow \\text{ sum of all of the entries of the support Matrix } \\\\\rNGl_total = Ngl_N \\cdot Nn - N_R \\\\\rK \\leftarrow \\text{ Sparse Matrix Ngl_total x Ngl_total } \\\\\r\\begin{aligned}\r\u0026amp; \\text{for j = 1 to Ne do } \\\\\r\u0026amp; \\text{for k = 1 to Ngle do} \\\\\r\\end{aligned} \\\\\r\\text{ for l= 1 to Ngl_e do } \\\\\r\\left\\{ \\begin{array}{lcc}\rpos1=conect(j,k) \\\\\rpos2=conect(j,l) \\\\\rtext{ if conect(j,k) \\neq 0 and conect(j,l) \\neq 0 then } \\\\\rK(pos1,pos2)=K_E{j}(k,l) + K(pos1,pos2) \\\\\r\\end{array}\r\\right.\r\\]\rRcpp Code:\n// [[Rcpp::export]]\r//Numeric Matrix using Support, Gl and Conect Matrix and K_E List as Inputs\rNumericMatrix K_Total(List K_E, NumericMatrix Support, NumericVector Gl, NumericMatrix conect, int NN_e = 2, int Ngl_N =2 ){\rint Num_Elements = K_E.length();\rint Num_Nodes = Support.nrow();\rint Nr=sum(Support), j, k, l, pos1, pos2;\rNumericMatrix K( Ngl_N * Num_Nodos- Nr );\rint Ngl_E = NN_e * Ngl_N;\rfor(j=0; j\u0026lt;Num_Elements; ++j){\rfor(k=0; k\u0026lt;Ngl_E; ++k){\rfor(l=0; l\u0026lt;Ngl_E; ++l){\rpos1 = conect(j,k);\rpos2 = conect(j,l);\r//Notice that List Elements need to be pulled using brakets\rNumericMatrix Ke = K_E[j];\r// and operator uses double ampersand and inequality syntax follow same rules than R\rif(pos1 != 0 \u0026amp;\u0026amp; pos2 !=0){\r// += is the C++ operator to sum the new value to the current one.\rK(pos1 - 1, pos2 - 1) += Ke(k,l);\r}\r}\r}\r}\rreturn K;\r}\r\r\rLoad Vector Assembly\rThis is the equivalent load Vector considering only Loads for active DoFs that are participating in the solution of the problem.\n\\[ F \\leftarrow \\text{ Sparse Matrix dimension Ngl_total x 1 } \\\\\r\\begin{aligned}\r\u0026amp; \\text{for i = 1 to Nn do } \\\\\r\\end{aligned} \\\\\r\\text{ for k= 1 to Ngl_n do } \\\\\r\\left\\{ \\begin{array}{lcc}\rpos1=Ngl_n \\cdot (i-1) + k \\\\\rpos2=Gl_act(pos1) \\\\\r\\text{ if pos2 Loads(i,k) } \\\\\rF(pos2)=Cargas(i,k)\\\\\r\\end{array}\r\\right.\r\\]\rRcpp Code:\n// [[Rcpp::export]]\rNumericVector f_Total(NumericMatrix Loads, NumericVector Gl, int Nr, int Ngl_N = 2 ){\rint Num_Nodos = Loads.nrow();\rint N_t = Ngl_N * Num_Nodos - Nr;\rNumericVector F (N_t);\rint i,k,pos1,pos2;\rfor(i=0; i \u0026lt; Num_Nodos; ++i){\rfor(k=0; k \u0026lt; Ngl_N; ++k){\rpos1 = Ngl_N * i + k;\rpos2 = Gl[pos1];\rif(pos2 != 0){\rF[pos2 - 1] = Cargas(i,k);\r}\r}\r}\rreturn F;\r}\r\rSolving the Problem\rAll this Steps allows to pose the following problem:\n\\[ [K] \\cdot \\{u\\} = \\{F\\} \\]\nIn order to get the desired displacements it is just necessary to inverse $ [K] $.\n\\[ \\{u\\} = [K]^{-1} \\cdot \\{F\\}\\]\nFor this case I´ll be using RcppEigen, a Rcpp Linear Algebra Library that allows some extra Matrix operations that are useful for, in this case, Matrix inversion:\n// I have defined a new object type called MapMatd whih is a Matrix with no specific size of doubles\rtypedef Map\u0026lt;MatrixXd\u0026gt; MapMatd;\r// Defined a Vector with same characteristics as before\rtypedef Map\u0026lt;VectorXd\u0026gt; MapVecd;\r// [[Rcpp::export]]\r// I use a VectorXd non defined size X with double data type d\rVectorXd u_vect(NumericMatrix K_Total, NumericVector f_Total){\r//I need to cast R Objects coming from Inputs into Eigen Objects. In this case i would just say trust me.\rconst MapMatd K(as\u0026lt;MapMatd\u0026gt;(K_Total));\rconst MapVecd f(as\u0026lt;MapVecd\u0026gt;(f_Total));\r//Applying Inverse Method, this is only available because K and f are already Eigen objets\rVectorXd result = K.inverse()*f;\rreturn result;\r}\r\r\rConclusions\r\rR and Rcpp share a very similar syntax.\rAll R objects are compatible with Rcpp, even Lists\rThe Main advantage of using Rcpp is that is way too faster than Regular R. This makes it especially suitable for Algorithms and Matrix manipulation.\rNotice that Matrices use () for indexing whereas Vectors and Lists use [].\rRcpp starts at 0, make the proper adjustments when dealing with indices.\r\rI´ll be posting another Entry using the recently reated functions to show how fast they are. Stay tuned!!\n\r\r","date":1569196800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569196800,"objectID":"aa94fc09c8d96ba39ca46ac3f1b7b171","permalink":"/project/stiffness-method/","publishdate":"2019-09-23T00:00:00Z","relpermalink":"/project/stiffness-method/","section":"project","summary":"How to Implement the Stiffness Method using Rcpp","tags":["Deep Learning","Convolutional Networks"],"title":"The Stiffness Method","type":"project"},{"authors":["Alfonso Tobar"],"categories":["Basics"],"content":"\r\rRedes Convolucionales\r¿Cómo se relaciona esto con las Redes Convolucionales?\r¿Cómo se va especializando la red para ser cada vez más detallista?\r¿Cómo se relaciona este problema con Estructuras?\r\rEl Paper de Finol\rSolución\rPuntos Interesantes para la Investigación.\r1era Sugerencia (Quizás no para la Memoria, pero para seguir investigando)\r2da Sugerencia\r3era Sugerencia.\r4ta Sugerencia “Crear Imágenes Estructurales”\r\r\r\r\rRedes Convolucionales\rLas redes convolucionales es un tipo de Red que esta especializada principalmente en extraer características de imágenes. Si bien es cierto es posible utilizar Redes convolucionales para otras aplicaciones, el análisis de imágenes y la Visión Computacional son la especialidad de estas redes.\nLa Principal ventaja sobre una Red neural normal es que no está densamente conectada. Sino que selecciona cuidadosamente las neuronas a las que se conectará cambiando la operación matricial de Producto Punto por una convolución.\nLa definición formal de Convolución es:\n\\[ (f * g)(x)= \\int_{-\\infty}^{\\infty}{f(\\eta)\\cdot g(x-\\eta) d\\eta} \\]\rLa interpretación más clásica de esta integral corresponde al producto de funciones al desplazar una por sobre la otra.\n¿Cómo se relaciona esto con las Redes Convolucionales?\rEl análisis de Imágenes es bastante complicado. La manera en la que un computador es capaz de ver una imágen es parseando e interpretandola por un Tensor de Pixeles. Dependiendo de la resolución dela Imagen este Tensor será cada vez más grande, por lo tanto usar Redes Densas supone un gran costo computacional por todos los $ w_{i,j}$ que será necesario calcular.\nNormalmente el análisis de Imágenes utiliza tensor de este estilo:\n\r\rFigure 1: Estructura de Tensores de Imágenes\r\r\rLa Matriz de Pixeles de Dimensiones Alto x Ancho más una Pofundidad de Canales RGB que dan el color.\nEs por esto que las redes convolucionales ocupan esta operación para reconocer patrones específicos dentro del Tensor de Imagen. La manera en que está convolución se lleva a cabo es por medio de un Filtro. Este filtro es otro Tensor que posee generalmente dimensiones de 3x3 o 5x5 el cual se desplaza a través del tensor de imágenes para construir un Mapa de Características.\n\r\r\r\rFigure 2: Aplicación de un Filtro en redes Convolucionales\r\r\rNormalmente luego de la Aplicación del Filtro el Mapa de Características es pasado por una ReLU lo que genera que se realcen ciertos atributos de la imagen lo que permite que la red aprenda a diferenciar características específicas.\nDependiendo del filtro aplicado, se verán ciertos atributos de la imagen o no. Cabe destacar que los filtros parten como números aleatorios que se van entrenando en los distintos Pasos de la red (Forward and Backward).\nSi intentamos analizar la foto de un gato, luego de aplicar filtros, esto es lo que un computador comienza a ver:\n\r\rFigure 3: Feature Map\r\r\rFigure 4: Resultado del Entrenamiento de Filtros\r\r\r\r¿Cómo se va especializando la red para ser cada vez más detallista?\rLuego de la red Convolucional hay una reducción de Dimensionalidad, para llevarse acabo se utiliza una Capa de pooling, esta puede ser una Average Pooling o Max Pooling (generalmente ésta última es la más utilizada) la cual reduce la dimensión del problema para luego aplicar una nueva Capa de Red convolucional pero con más filtros que el paso anterior.\r\r\r\r\rFigure 5: Max Pooling\r\r\rUna vez que se ha llegado al nivel de espacialización deseado, se llevan todas las caracterícticas a un vector que es pasado por una capa densamente conectada para entregar los outputs correspondientes.\nUna Configuración típica podría verse así:\n\r\rFigure 6: Especialización de la Red\r\r\r\r¿Cómo se relaciona este problema con Estructuras?\rNo tiene relación alguna. Porque este tipo de redes fue diseñada para el desarrollo del análisis de imágenes. Además es la manera más intuitiva de analizar el funcionamiento de estas redes.\rLas dos principales caracerísticas de las Redes Convolucionales son:\n\rLa extracción de Características\rTranslation/Shift Invariance\r\rEsto quiere decir que la red se especializa en detectar Patrones que no dependen de su posición ni que deben ser identicos para ser detectados. Por ejemplo una oreja de gato se detectara si es grande, si es pequeña, si es de otro color si esta rotada o desplazada.\nSon estas características las que permitirían desarrollar otro tipo de problemas si es que pueden ser planteadas como un tensor de imágen o similar.\n\r\rEl Paper de Finol\rEl Paper propone solucionar un problema cualquiera de estructuras usando redes Convolucionales. Lo que ellos primeramente plantean es la obtención de Valores propios para un material de propiedades variables.\nPara ello se utiliza una barra dividida en 100 Elementos en las que se miden dos Propiedades, el Módulo de Elasticidad E y la Densidad $ $ .\nEn simple lo que se está planteando es una red convolucional para analizar una Imagen de 1 Pixel de Alto por 100 de Ancho, donde los Canales que representan color, en este caso representan propiedades físicas del Elemento. Dada las propiedades de la imagen esto puede extrapolarse a que las Redes convolucionales pueden analizar Data Secuencial, en este caso cada Elemento de la Barra están en secuencia debido a la continuidad del la barra. Aunque cabe destacar que la red Convolucional no se deja llevar por el orden ya que sus patrones no dependen de la posición.\nMi Supuesto:\nEsto podría traducirse en que los invesigadores pensaron en quizás un elemento equivalente de un Modulo $ E_{eq} $ en serie dado que es una barra unidimensional y el orden de los $ E_{i} $ y $ _{i} $ no influyen en la propiedad equivalente final del elemento.\nAdemás el problema que se está resolviendo es de valores propios, no hay elementos externos que indiquen que el orden de las propiedades mecánicas influyan en el cálculo final.\nBajo estas características es que el uso de la Red Convolucional es válida.\nSolución\rPara resolver el problema de Valores propios para un Cristal dividido en 100 Elementos se utiliza la siguiente configuración:\n\r\rFigure 7: Solución de Finol\r\r\r\rSe utiliza una Red Secuencial aplicando un Filtro de Largo 3.\rAl no utilizar padding el vector se reduce a 98 x 1 x 275 donde 275 corresponde teóricamente al número de filtros que está utilizando. Esto no sale definido en el Paper y es algo que averigué investigando por mi parte.\rSe utiliza una segunda capa convolucional. La razón de esto no lo sé. Normalmente no he visto casos de dos Capas convolucionales seguidas excepto en modelos muy avanzados.\rSe utiliza una capa Max Pooling para reducir de largo 2 que disminuye el tamaño de la red a la mitad.\rLuego de esto se une con una Red Convencional densamente conectada. Lo que normalmente he visto es que se usa una flatten Layer que aplana la reducción de dimensiones que se ha dado hasta ahora. El valor esperado debería ser de 275 x 48, pero no sé porque ocupa 500 nodos.\r*Se utilizan 3 capas ocultas para luego generar una capa de 20 unidades de salida por los 20 valores propios esperados para el problema.\r\r\rPuntos Interesantes para la Investigación.\r\rEstoy tomando un curso en Datacamp y uno público de Stanford acerca del uso de redes Convolucionales. Dentro del curso se dan varias recomendaciones:\n\rPara un problema de imágenes de 64 x 64 x 3 para predecir si hay un gato en la imágen o no se requieren aproximadamente 10.000 imágenes.\n\rSe recomienda usar la mínima resolución posible para distinguir a nivel humano de tal manera de bajar el poder computacional que requiere la red.\n\rA mayor resolución más imágenes. Menor Resolución menos Imágenes.\n\r\rCualquier Matriz puede ser interpretada como una imágen. En la cual una Red Convolucional puede encontrar patrones. Por lo que perfectamente la matriz de rigidez puede ser tratado como una imagen.\n\r\r\r\rFigure 8: Transformación de Matriz en Imágen\r\r\r\rLas redes convolucionales poseen propiedades de Memoria dado que su principal función es la Extracción de Características. Existen modelos pre-entrenados de redes que Pueden usarse para perfeccionar otras tareas.\n\rUn ejemplo que leí fue una red que fue pre-entrenada con imágenes de muebles pero que luego se adaptó para mejor la identificación de animales. La red a pesar de no ser entrenada con animales, era capaz de reconocer bordes y características muy finas.\r\r\r1era Sugerencia (Quizás no para la Memoria, pero para seguir investigando)\rUso de Redes convolucionales para calcular Inversas de la Matriz de Rigidez. Siendo este el proceso más caro de la resolución de estructuras quizás podría realizarse el desarrollo de una Red Pre-entrenada que sea capaz de calcular Inversas de Muchas Matrices de tal Manera de agilizar el proceso más caro de la resolución de estructuras.\n\r2da Sugerencia\rSi bien las redes Convolucionales pueden dar buenos resultados para data secuencial, dada su caracteristica de “Invariante a la Traslación”, puede encontrar patrones incorrectos cuando el orden sí importa. Se desaconseja su uso en series de tiempo, ya que el orden de la información importa al momento de predecir.\rPara contrarestar esto, se agrega una capa de redes recurrentes (RNN) que ayuda a establecer la secuencia como un prerequisito del análisis.\n\rEs posible realizar modelos paralelos en la que se ingrese data distinta como input de la Red. Por ejemplo la Matriz de Rigidez del problema como “imagen” y los Propiedades Mecánicas como vector o hasta las Cargas a las que está sometida la estructura como vector, de esa manera todos los elementos son parte de los Input del problema para predecir de manera más acertada los desplazamientos normales o umbrales del problema.\r\r\r3era Sugerencia.\rGenerar una red no secuencial considerando todos los inputs del problema.\n\rDado que el análisis de las Imágenes poseen una característica Espacial y secuencial me da la impresión que se puede trabajar en optimizaciones al proceso de Condensación de Grados de Libertad. Sería posible que la red redujera problemas con gran cantidad de grados de libertad en Rigideces equivalentes que pueden ser planteadas como una imágen.\n\rAsí mismo dado que también posee propiedades de Profundidad podría ayudar al desarrollo de analisis de estructuras en 3 dimensiones que es algo que yo no recuerdo haber visto si no era usando algo como ETABS o SAP.\n\rMe da la impresión de que Redes Convolucionales pueden ayudar al desarrollo de Propiedades equivalentes usando su característica secuencial para propiedades en serie y su profundidad como propiedades en paralelo.\n\r\r\r4ta Sugerencia “Crear Imágenes Estructurales”\rUtilizar la estructura de la matriz de Rigidez con 3 Canales simulando el RGB: Rigidez, Cargas en Eje X y Cargas en Eje Y.\rHabría que buscar una manera de modelar dentro de la matriz los apoyos, que normalmente no son considerados dentro de la Matriz a priori.\nPara un Enrejado de 9 Elementos esto sería una Imágen de 9 x 9 x 3, lo cual no supondría una gran cantidad de casos para entrenar.\n\r\r\r","date":1554940800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554962037,"objectID":"7465b3b7e0f8a8e6deade092f6b5568a","permalink":"/project/convolutional-nets-sp/memoria-1/","publishdate":"2019-04-11T00:00:00Z","relpermalink":"/project/convolutional-nets-sp/memoria-1/","section":"project","summary":"Propuestas para Uso de CNN en estructuras.","tags":["Deep Learning","Convolutional Networks"],"title":"Propuesta Redes Convolucionales","type":"project"},{"authors":["Alfonso Tobar"],"categories":["Basics"],"content":"\r\rActivation Functions\rIdentity\rStep Function\rLinear Function\rSigmoid Function\rHyperbolic Tangent\rReLU\rLeaky ReLU\rSoftmax\r\rHow to choose the perfect Activation function?\r\r\rActivation Functions\rActivation functions are one of the most important characteristic of ANN. They basically decide whether a neuron should be activated or not. When a particular threshold is reached the Neuron will fire, meaning they will transmit the input signal to the next layer of the Network. Another Important feature of Activation Functions is that some of them provide the non-linearity. This is particular important because Activation functions help expand the range of problems that the Neural Networks can address. Finally Activation functions will play a major role when optimizing the edges weights when Backpropagation Algorithm comes into play, depending of their derivatives values is how the Gradient will change helping to decrease the error associated the Network prediction.\nSome of these Activation Functions are:\nIdentity\rIt is the most basic Activation, basically, do not alter the Neuron at all. The problem with this type of activation function is that is linear, transforming the Network into a Linear Regression limiting its classification capabilities for non-linear phenomena.\n\rStep Function\rThe binary function is extremely simple. It returns 1 if certain threshold is reached or 0 otherwise. The main drawback of this function is that his derivative is 0, meaning it is not useful in the optimizing process.\nThe function is defined as follows:\n\\[ f(x)= \\left\\{ \\begin{array}{lcc}\r0 \u0026amp; if \u0026amp; x \u0026lt; 0 \\\\\r\\\\ 1 \u0026amp; if \u0026amp; x \\geq 0 \\end{array}\r\\right. \\]\n\r\rFigure 1: Step Function\r\r\r\rLinear Function\rThis is another option, being the main difference the existence of a slope. In this case the derivative will be constant, which can be problematic because when trying to decrease the error no matter how right or off you are the gradient will be the same.\nThe function goes as follows:\n\\[ f(x) = a x \\]\n\r\rFigure 2: Linear Function\r\r\r\rSigmoid Function\rThis is a very popular activation function. The main advantages of this function is that it is smooth, S-shaped, it is continuously differentiable and non-linear.\nThe function is defined as follows:\n\\[ f(x)= \\frac{1}{1+e^{-x}} \\]\n\r\rFigure 3: Sigmoid Function\r\r\rThe derivative of this function is always positive and greater than 0 and x-dependent so it is very helpful when optimizing.\nOne of the setbacks is that only ranges from 0 to 1, for one thing is very limiting with the output but for the other it is particularly useful when dealing with probabilities.\n\rHyperbolic Tangent\rHyperbolic Tangent or $ tanh(x) $ is just an scaled version of the Sigmoid function. It is defined as follows:\n\\[ tanh(x)= 2 \\cdot sigmoid(2x) - 1 = \\frac{2}{1+e^{-2x}} - 1 \\]\r\r\rFigure 4: Hyperbolic Tangent Function\r\r\r$ tanh $ works similarly to sigmoid but it is symmetric at the x - axis. Normally $ tanh $ and sigmoid can be used interchangeably depending on the requirements of the problem.\n\rReLU\rReLu stands for Rectified Linear unit and it is defined as follows:\n\\[ f(x)= max(0,x) \\]\r\r\rFigure 5: ReLU Function\r\r\rIt is the most used function nowadays in hidden layers. The main capability is that it doesn’t activate all of the functions creating sparsity in the network, allowing efficiency in computation.\nThis function is limited at the positive side so it is not suggested for Output Layers. Another drawback is that not activated neurons in the range $ x \u0026lt; 0 $ will not be optimized since derivative is zero.\n\rLeaky ReLU\rThis is an improved version of the ReLU function. It is not widely used yet and it has a subtle difference with ReLu: \\[ f(x)= \\left\\{ \\begin{array}{lcc}\rax \u0026amp; if \u0026amp; x \u0026lt; 0 \\\\\r\\\\ x \u0026amp; if \u0026amp; x \\geq 0 \\end{array}\r\\right. \\]\n\r\rFigure 6: Leaky ReLU Function\r\r\rThis solves the problem of dead neurons during Optimization process, since the derivative of $ x \u0026lt; 0 $ is not zero.\n\rSoftmax\rThis is a sigmoid kind-of function capable of handling more than 2 classes. The function is defined as follows:\n\\[ \\sigma(z)_{j}=\\frac{e^{z_{j}}}{\\sum_{k=1}^{K}e^{z_{k}}} \\]\nThe softmax functions are normally used in output layers when trying to solve classification problems with more than 2 classes..\n\r\rHow to choose the perfect Activation function?\rWell, there is not a clear answer to this, but definitely some guidelines we can follow:\n\rSigmoid functions generally work better in classification problems.\rSigmoids and tanh functions are sometimes avoided due to the vanishing gradient problem.\rReLU function is a general activation function and is used in most cases these days.\rIf we encounter a case of dead neurons in our networks the leaky ReLU function is the best choice.\rAlways keep in mind that ReLU function should only be used in the hidden layers.\rAs a rule of thumb, you can begin with using ReLU function and then move over to other activation functions in case ReLU doesn’t provide with optimum results.\r\rDeep Learning with R provides some other Guidelines to use Activation Functions in the Output Layer:\n\rBinary Classification: Sigmoid\rMulticlass Single-Label Classification: Softmax\rMulticlass Multi-Label Classification: Sigmoid\rRegression to Arbitrary Values: Identity or None\rRegression to Values between 0 to 1: Sigmoid\r\r\r","date":1554681600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554783338,"objectID":"7d22714745bd3ed36a07d7ead5033e85","permalink":"/project/activation-functions/activation-functions/","publishdate":"2019-04-08T00:00:00Z","relpermalink":"/project/activation-functions/activation-functions/","section":"project","summary":"Definitions of the Most Common ACtivations Functions.","tags":["Deep Learning"],"title":"Activation Functions","type":"project"},{"authors":["Alfonso Tobar"],"categories":["Basics"],"content":"\r\rWhat is an Artificial Neural Network?\rBasic Structure\rWhat Problems can Neural Networks solve?\rClassification\rRegression\r\rTypical Problems solved with Neural Networks\r\rConclusion\r\r\rWhat is an Artificial Neural Network?\rThe Picture you see up there is far a way from what a real Neural Network looks like:\nActually it is more similar to something like this:\n\r\rFigure 1: Multi-Layer Perceptron\r\r\rThe configuration showed above is nothing but a visual representation of serial Matrix Multiplications. The neural network (aka ANN that stands for Artificial Neural Networks) itself is not an algorithm, but rather a framework for many different machine learning algorithms to work together and process complex data inputs1.\nANN are inspired by Neurons but they work in a completely different fashion. The main advantage of this system is that organizes successive Matrix multiplications and provides a visual representation of the different steps in the Optimization algorithm.\nThere are different types of Networks, Densely Connected, Convolutional, Recurrent, Long Short Memory, Radial Biased, Autoencoders, and so on. All of them having their own area of specialization, strengths and shortcomings.\nBasic Structure\rEvery Network will contain Nodes/Units, emulating Neurons and Edges, emulating the connection between Units.\nNormally the Units are organized by Layers, every Network should contain a first layer for Inputs, a last Layer for Outputs and Intermediate Layers, also known as Hidden Layers.\n\r\rFigure 2: Basic Neural Network\r\r\r\rInput Nodes $ i_{j} $ will contain Input Values to train the Network.\n\rEdges will provide weights $ w_{j} $.\n\rHidden Layer Nodes $ h_{j} $ will contain the result of Sum Product between Input Nodes and weighted edges connected to them.\n\rFinally Output Nodes $ o_{j} $ will contain the result of Sum Product between hidden Nodes and the Edges Connected to them.\n\rOptionally, Networks can include a Bias $ b_{j} $ to control values of the network.\n\r\rThe Network then will calculate values in the following way:\n\\[ h_{1} = w_{1} i_{1} + w_{2} i_{2} + b_{1} = 0.15 \\cdot 0.05 + 0.25 \\cdot 0.10 + 0.35 = 0.3825 \\]\n\\[ h_{2} = w_{2} i_{1} + w_{4} i_{2} + b_{1} = 0.20 \\cdot 0.05 + 0.30 \\cdot 0.10 + 0.35 = 0.39 \\]\n\\[ o_{1} = w_{5} h_{1} + w_{7} h_{2} + b_{2} = 0.40 \\cdot 0.3825 + 0.50 \\cdot 0.39 + 0.60 = 0.948 \\]\n\\[ o_{2} = w_{6} h_{1} + w_{8} h_{2} + b_{2} = 0.45 \\cdot 0.3825 + 0.55 \\cdot 0.39 + 0.60 = 0.986625 \\]\nThis is called a forward pass. All the Input Values were able to move through the Network by the Edge Connections up to the Output. Normally, when the Network is trained, there are expected Output values that will be compared with the ones obtained with the Forward Pass in order to compute the Error. In this case 0.01 and 0.99 respectively.\n\rWhat Problems can Neural Networks solve?\rNormally there are two Problems that Neural Networks Solves, Classification and Regression.\nClassification\rThe Classification problem is the most common problem addressed by Neural Networks. It implies to classify based on a Probability. The output will calculate how likely is that an specific label corresponds to a class. Classification problems can be sub-divided into other sub-types:\n\rBinary Classification: As the name implies, it involves two classes: Spam or not Span, Positive or Negative, Man or Woman, etc.\rMulticlass Classification: In this case several labels can be applied: Is it a Dog, Cat, Horse? What Car Brand is that? etc.\r\r\rRegression\rThis kind of problems involved calculate a number associated to a Metric. Typical Problems are predicting House Values, Temperature, Balances, Displacements, etc.\n\r\rTypical Problems solved with Neural Networks\rNeural Networks are powerful and they are the most cutting-edge methodology to make computers do the most incredibly/creepy things.\nEven though we think computers can do anything like Analyzing Photos, Driving Cars, Recognizing Animals, Predicting Prices and so on, the scope of their work is completely limited to just one thing: Computing Tensors.\nTensors are the generalization to N dimensions of Matrices. Basically any problem that can be represented by Tensors is something that Neural Networks could potentially solve.\nDifferent kind of Tensors can solve specific problems. Here some examples taken from Deep Learning with R:\n\rVector data—2D tensors of shape (samples, features): This is the Most common Data Structure Data Scientist uses in a daily basis. Basically a Matrix, having Features as Columns and Samples as Rows.\n\rTimeseries data or sequence data—3D tensors of shape (samples, timesteps, features): This is something a little bit fancier, having several timeseries organized as a collection of matrices, this creates a 3D Tensor.\n\r\r\r\rFigure 3: Multiple Timeseries data\r\r\r\rImages—4D tensors of shape (samples, height, width, channels) or (samples, channels, height, width): Images are represented as Pixel Matrices, Every Pixel also has RGB Channels giving the color properties to it, thus a 3D Tensor. Adding several samples of Images to analize and you have a 4D Tensor.\r\r\r\rFigure 4: Image Data\r\r\r\rVideo—5D tensors of shape (samples, frames, height, width, channels) or (samples, frames, channels, height, width): Videos are nothing but a collection of Sequencial Images. So You’ll have different Samples of Sequencial Images producing which is a 4D Tensor, since you analize several samples of Videos, you’ll get a 5D Tensor.\r\r\r\rConclusion\rNeural Networks are simple visual representations of Tensor Calcultions that are capable of addressing different real life problems. So far we have covered how the Networks transmit Information from Input to Output also called Forward Pass, but there are some other concepts that are necessary to understan in order to fully unerstand how to properly train a Neural Network.\n\r\rWikipedia: https://en.wikipedia.org/wiki/Artificial_neural_network↩\n\r\r\r","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554783338,"objectID":"5ab0020c69035456ab833ae99bb5d554","permalink":"/project/neural-nets-101/intro-to-nn/","publishdate":"2019-04-07T00:00:00Z","relpermalink":"/project/neural-nets-101/intro-to-nn/","section":"project","summary":"Short Summary of What Neural Networks are","tags":["Deep Learning"],"title":"Intro to Neural Networks","type":"project"},{"authors":["Alfonso Tobar"],"categories":["Vampire Diaries"],"content":" Deep Learning Well, I got to know Deep Learning by chance. I remember to have had a College subject called Operational Investigation Fundamentals and they taugth Artifical Intelligence algorithms. Neural networks were mentioned but I never thought I could even understand what they were about. During my first real job (as a quasi-Engineer) I always thougth I never paid sufficient attention to that subject. The subject basically covered some optimization models and for some reason AI was there, just sky-high level mentions in some classes.\nThen I remember during my Data Science Program, we had 3 different Research Projects (this was at my Butterflies and Unicorns time, if you don\u0026rsquo;t know what I\u0026rsquo;m talking, go here). One was Data Analytics to Measure Cars Price (super-duper boring), the second was Using Sentiment Analysis to Understand whether Lyrics of Top 20 Billboard were related to Decade\u0026rsquo;s Most Important Facts (I did this, and it was super interesting) and there was a third one I really tried to sneak out: Sentiment Annalysis with Machine Learning. Machine Learning sounded really scary to be my very first Data Science Project, so I put it off.\nIt turns out that a colleague just joined my team at EVS and he had this awesome Max Kuhn book: Applied Predictive Modeling. And I thought: \u0026ldquo;I\u0026rsquo;m more experienced now, so probably I can take a look at this\u0026rdquo;. And well I discovered caret and completely blew my mind (But I still dislike its little to null compatibility with Tidyverse, but thanks Max Kuhn for creating parsnip).\nThen Nico, I think, was the first guy mentioning Deep Learning, because he created the XoR problem in VBA and also took Andrew Ng specialization Course, and I started to feel interested in the Field.\nWell, I dicovered Keras, the R API just came out (I think so) and I got this other super Book: Deep Learning with R and I just learned so much, but not enough. I think I learned a bit of Keras but I suddenly realized I had no idea about the inner black-box, so I just wanted to learn and understand what the hell is happening inside that box.\nWell here I am\u0026hellip; Trying to understand\u0026hellip; and I didn\u0026rsquo;t do very well. Internet has little to no information about theory. Codes are everywhere, but why that code is useful is just not important for users.\nSo my intent is creating content (R based obviously, because R is not popular for Deep Learning either) for me to learn, and hopefully spread the word about this.\nWell, the thing is I need to finish my Thesis, and my Professor told me: \u0026ldquo;Why don\u0026rsquo;t you combine what you know about Deep Learning (sincerely, not too much) with Structure Engineering? And you know what, this is something really unexplored in the field, so here I go.\nHopefully publishing my findings help to have a better understanding of what I\u0026rsquo;m doing, also i can keep track of it and help others on my way (I love teaching, so this is a good way to get started).\nHope to have news about my Thesis soon\u0026hellip;\n","date":1553647800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554521808,"objectID":"2046ae8bd00899bda7e50b0ef72f0071","permalink":"/post/deep-learning/","publishdate":"2019-03-27T00:50:00Z","relpermalink":"/post/deep-learning/","section":"post","summary":"How I ran into DL?","tags":["Thesis","Deep Learning"],"title":"Deep Learning","type":"post"},{"authors":["Alfonso Tobar"],"categories":["Blog"],"content":" Deep Learning Well, I got to know Deep Learning by chance. I remember to have had a College subject called Operation Investigation Fundamentals and they taugth Artifical Intelligence algorithms. Neural networks were mentioned but I never thought I could even understand what they were about. During my first real job (as an engineer) I always thougth I never paid sufficient attention to that subject. The subject basically covered some optimization models and for some reason AI was there, just sky-high level mentions in some classes.\nThen I remember during my Data Science Program, we had 3 different Research Projects (this was at my Butterflies and Unicorns time, if you don\u0026rsquo;t know what I\u0026rsquo;m talking, go here). One was Data Analytics to Measure Cars Price (super-duper boring), the second was Using Sentiment Analysis to Understand if the Topics of Top 20 Billboard were related to Decade\u0026rsquo;s Most Important Facts (I did this, and it was super interesting) and there was a third one I really tried to sneak out: Sentiment Annalysis with Machine Learning. Machine Learning sounded really scary to be my very first Data Science Project, so I put it off.\nIt turns out that a colleague just joined my team at EVS and he had this awesome Max Kuhn book: Applied Predictive Modeling. And I thought: \u0026ldquo;I\u0026rsquo;m more experienced now, so probably I can take a look at this\u0026rdquo;. And well I discovered caret and completely blew my mind (But I still dislike its little to null compatibility with Tidyverse, thanks Max Kuhn for creating Parsnip).\nThen Nico, I think, was the first guy mentioning Deep Learning, because he created the XoR problem in VBA and also took Andrew Ng specialization, and I started to feel interested in the Field.\nWell, I dicovered Keras, the R API just came out (I think so) and I got this other super Book: Deep Learning with R and I just learned so much, but not enough. I think I learned a bit of Keras but suddenly realized I had no idea about the inner black-box, so I just wanted to learn and understand what the hell is happening inside that box.\nWell here I am\u0026hellip; Trying to understand\u0026hellip; and I didn\u0026rsquo;t do very well, The Internet has little to no information about theory. Codes are everywhere, but why that code is useful is just not important for users.\nSo my intent is creating content (R based obviously, because R is not popular for Deep Learning either) for me to learn, and hopefully spread the word about this.\nWell, the thing is I need to finish my Thesis, and my Professor told me why don\u0026rsquo;t we combine what you know about Deep Learning (sincerely, not too much) with Structure Engineering, and you know what this is something really unexplored in the field, so here I go. Hopefully publishing my findings help to have a better understanding of what I\u0026rsquo;m doing, also i can keep track of it, and I can help others (I love teaching, so this is a good way to get started).\nHope to have news about my Thesis soon\u0026hellip;\n","date":1553647800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554521808,"objectID":"476c6249b9ba536df97e2f07196cc1e9","permalink":"/publication/deep-learning/","publishdate":"2019-03-27T00:50:00Z","relpermalink":"/publication/deep-learning/","section":"publication","summary":"How I ran into DL?","tags":["Thesis"],"title":"Deep Learning","type":"publication"},{"authors":["Alfonso Tobar"],"categories":["Vampire Diaries"],"content":" Why am I not graduated yet? Graduation is something we all are longing. Well, I recognize this is something I\u0026rsquo;ve been evading. The main reason is because I\u0026rsquo;m not working on what I studied. But, what the hell, I think I like Data Science.\nOk, Long Story Short\u0026hellip; I was about to study Mathematical Engineering but I asked myself, where can I possibly work as a Mathematical Engineer ? (In my current job, obviously 😅)\nSo I decide to go for Civil Engineering because it has a decent amount of advanced Maths that was something I was looking for and a High Employment Index.\nWell I started working as a Civil Engineer in INVAR as soon as I finished my internship. I just loved it, I learned so much, but the thingwas that they didn\u0026rsquo;t pay too much. So I decided to take a position at Aguas del Valle. Sincerely, and very respectfully, it has been the worst choice of my life, not only because it was 5 hours away of my Hometown, but also because they took my soul out of my body and make me spend awful moments doing absolutely nothing. There is nothing more frustrating than doing nothing all day long in a desk in a Company you don\u0026rsquo;t feel a part of.\nSo the Evalueserve Post showed up. A friend referred me to the Data Science Program (DSP) and then I fell in love with Data Science. During the program I learned SQL, VBA, SAS 👍, R, Tableau and definitely built up my English and Communication skills. Felipe, help me prepare my first interview in English and thanks to him I had the English I needed to learn all I know so far.\nUpskilling and moving through cities took all of my time, besides I got married (Best Decision so far) but a lot to do, hence, no time to finish my Thesis.\nWhat happened with my Thesis? Well in INVAR I worked as an Hidraulics Engineer so I started a Thesis back in 2013 about Water Hammer. Really interesting topic, that needed Finite Differences Methods to solve really complicated Partial Differential Equations. The only proffesor with this knowledge started working wih me, but after a year he got retired. So I had to find another Professor, so in order to avoid this issue to happen again, I decided to go with one of the youngest Professor, well he left me for his Phd.\nDamn, Suddenly working in INVAR I noticed the CEO was my professor of Planning so I ask him to mentor me. This happened just before leaving to Aguas del Valle in La Serena. 6 months later a Cancer was diagnosed and he passed away, so freaking fast. The Chief of Department took over all of the abandoned Thesis that my Professor\u0026rsquo;s dead left behind, but he couldn\u0026rsquo;t continue to mentor me, because he considered being in La Serena was too far away. In 2015 I came back to Viña and I started to learn, upskill, teach, develop as a Data Scientist that I decided to put it on hold. On 2017 I started another Thesis. Another Professor wanted to mentor me but he was of the Structural Area. That meant I needed to move to that Area. We started investigating uncertainty on Structural Analysis. The thing is my work again didn\u0026rsquo;t give me any chance to continue. Until now. Why now? Probably I will explain it later, because it is also related with the creation of this site.\nWhat My Thesis is going to be about? Good news, I will be able to mix my previous work with one thing I discovered as I developed as Data Science: Deep Learning in R 👏.\nI will be posting some progress about that. Hang tight!!\n","date":1553647800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554521808,"objectID":"fc07de7a276a157101a10808a9efd24b","permalink":"/post/my-thesis/","publishdate":"2019-03-27T00:50:00Z","relpermalink":"/post/my-thesis/","section":"post","summary":"Where, What and Why is ~~Gamora~~ my Thesis About?","tags":["Thesis"],"title":"My Thesis Project","type":"post"},{"authors":["Alfonso Tobar"],"categories":["Vampire Diaries"],"content":" Why am I not graduated yet? Graduation is something we all are longing. Well, I recognize this is something I\u0026rsquo;ve been evading. The main reason is because I\u0026rsquo;m not working on what I studied. But, what the hell, I think I like Data Science.\nOk, Long Story Short\u0026hellip; I was about to study Mathematical Engineering but I asked myself, where can I possibly work as a Mathematical Engineer ? (In my current job, obviously 😅)\nSo I decide to go for Civil Engineering because it has a decent amount of advanced Maths that was something I was looking for and a High Employment Index.\nWell I started working as a Civil Engineer in INVAR as soon as I finished my internship. I just loved it, I learned so much, but the thingwas that they didn\u0026rsquo;t pay too much. So I decided to take a position at Aguas del Valle. Sincerely, and very respectfully, it has been the worst choice of my life, not only because it was 5 hours away of my Hometown, but also because they took my soul out of my body and make me spend awful moments doing absolutely nothing. There is nothing more frustrating than doing nothing all day long in a desk in a Company you don\u0026rsquo;t feel a part of.\nSo the Evalueserve Post showed up. A friend referred me to the Data Science Program (DSP) and then I fell in love with Data Science. During the program I learned SQL, VBA, SAS 👍, R, Tableau and definitely built up my English and Communication skills. Felipe, help me prepare my first interview in English and thanks to him I had the English I needed to learn all I know so far.\nUpskilling and moving through cities took all of my time, besides I got married (Best Decision so far) but a lot to do, hence, no time to finish my Thesis.\nWhat happened with my Thesis? Well in INVAR I worked as an Hidraulics Engineer so I started a Thesis back in 2013 about Water Hammer. Really interesting topic, that needed Finite Differences Methods to solve really complicated Partial Differential Equations. The only proffesor with this knowledge started working wih me, but after a year he got retired. So I had to find another Professor, so in order to avoid this issue to happen again, I decided to go with one of the youngest Professor, well he left me for his Phd.\nDamn, Suddenly working in INVAR I noticed the CEO was my professor of Planning so I ask him to mentor me. This happened just before leaving to Aguas del Valle in La Serena. 6 months later a Cancer was diagnosed and he passed away, so freaking fast. The Chief of Department took over all of the abandoned Thesis that my Professor\u0026rsquo;s dead left behind, but he couldn\u0026rsquo;t continue to mentor me, because he considered being in La Serena was too far away. In 2015 I came back to Viña and I started to learn, upskill, teach, develop as a Data Scientist that I decided to put it on hold. On 2017 I started another Thesis. Another Professor wanted to mentor me but he was of the Structural Area. That meant I needed to move to that Area. We started investigating uncertainty on Structural Analysis. The thing is my work again didn\u0026rsquo;t give me any chance to continue. Until now. Why now? Probably I will explain it later, because it is also related with the creation of this site.\nWhat My Thesis is going to be about? Good news, I will be able to mix my previous work with one thing I discovered as I developed as Data Science: Deep Learning in R 👏.\nI will be posting some progress about that. Hang tight!!\n","date":1553647800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554521808,"objectID":"340c92f3fd37efeff818f6906d6a59a5","permalink":"/publication/my-thesis/","publishdate":"2019-03-27T00:50:00Z","relpermalink":"/publication/my-thesis/","section":"publication","summary":"Where, What and Why is ~~Gamora~~ my Thesis About?","tags":["Thesis"],"title":"My Thesis Project","type":"publication"},{"authors":null,"categories":null,"content":"","date":1551409200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553554159,"objectID":"d7c99b5f0ba2ef066af2121e72286554","permalink":"/bio/","publishdate":"2019-03-01T00:00:00-03:00","relpermalink":"/bio/","section":"","summary":"This is me!","tags":null,"title":"Bio","type":"widget_page"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\ngetwd()  ## [1] \u0026quot;D:/Alfonso/website/content/tutorial\u0026quot;  Tip 1 \u0026hellip;\nTip 2 \u0026hellip;\nbla bla ","date":1536462000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553562400,"objectID":"2e59fda17a04e7a8318c6191daa0102b","permalink":"/tutorial/example/","publishdate":"2018-09-09T00:00:00-03:00","relpermalink":"/tutorial/example/","section":"tutorial","summary":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\ngetwd()  ## [1] \u0026quot;D:/Alfonso/website/content/tutorial\u0026quot;  Tip 1 \u0026hellip;\nTip 2 \u0026hellip;\nbla bla ","tags":null,"title":"Example Page","type":"docs"},{"authors":["GA Cushen"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1441076400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553404091,"objectID":"d77fa4a74076ffcd7ca6c21cfc27a4b2","permalink":"/publication/person-re-id/","publishdate":"2015-09-01T00:00:00-03:00","relpermalink":"/publication/person-re-id/","section":"publication","summary":"Person re-identification is a critical security task for recognizing a person across spatially disjoint sensors. Previous work can be computationally intensive and is mainly based on low-level cues extracted from RGB data and implemented on a PC for a fixed sensor network (such as traditional CCTV). We present a practical and efficient framework for mobile devices (such as smart phones and robots) where high-level semantic soft biometrics are extracted from RGB and depth data. By combining these cues, our approach attempts to provide robustness to noise, illumination, and minor variations in clothing. This mobile approach may be particularly useful for the identification of persons in areas ill-served by fixed sensors or for tasks where the sensor position and direction need to dynamically adapt to a target. Results on the BIWI dataset are preliminary but encouraging. Further evaluation and demonstration of the system will be available on our website.","tags":[],"title":"A Person Re-Identification System For Mobile Devices","type":"publication"},{"authors":["GA Cushen","MS Nixon"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1372651200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553661585,"objectID":"2b4d919e3cf73dfcd0063c88fe01cb00","permalink":"/publication/clothing-search/","publishdate":"2013-07-01T00:00:00-04:00","relpermalink":"/publication/clothing-search/","section":"publication","summary":"A mobile visual clothing search system is presented whereby a smart phone user can either choose a social networking image or capture a new photo of a person wearing clothing of interest and search for similar clothing in a large cloud-based ecommerce database. The phone's GPS location is used to re-rank results by retail store location, to inform the user of local stores where similar clothing items can be tried on.","tags":[],"title":"Mobile visual clothing search","type":"publication"},{"authors":null,"categories":null,"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553404091,"objectID":"c2915ec5da95791851caafdcba9664af","permalink":"/slides/example-slides/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example-slides/","section":"slides","summary":"Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$","tags":null,"title":"Slides","type":"slides"}]